{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iakioh/MusiCAN/blob/main/models/musiCAN_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ATBcagDnDpM"
      },
      "source": [
        "### Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjvf82O3S2Yz",
        "outputId": "11cd1920-8926-40fe-919d-8260a3aefdc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCaQQJb6TXgV",
        "outputId": "cedd4cad-1955-4727-9fab-a31cd54c0f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MusiCAN/models\n"
          ]
        }
      ],
      "source": [
        "# Go to this notebook's directory\n",
        "repo_path = \"/content/drive/MyDrive/MusiCAN/\"\n",
        "%cd {repo_path}/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGZ0ptpZQNH",
        "outputId": "49c9e402-2d84-44e9-c8c4-555030a227cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 12 21:58:39 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    36W / 250W |   3459MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU connection\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7_ny_A1Z6L9",
        "outputId": "cd836680-3885-467d-88df-59fabb46a24f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "# Check RAM access\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnLdyhmuzL2Z",
        "outputId": "73a136f9-884f-4935-fa59-78af291fa7f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: muspy in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: pypianoroll>=1.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (1.0.4)\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.7/dist-packages (from muspy) (1.1.0)\n",
            "Requirement already satisfied: mido>=1.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (1.2.10)\n",
            "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.7/dist-packages (from muspy) (3.2.2)\n",
            "Requirement already satisfied: music21>=6.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (7.3.3)\n",
            "Requirement already satisfied: bidict>=0.21 in /usr/local/lib/python3.7/dist-packages (from muspy) (0.22.0)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (6.0)\n",
            "Requirement already satisfied: pretty-midi>=0.2 in /usr/local/lib/python3.7/dist-packages (from muspy) (0.2.9)\n",
            "Requirement already satisfied: miditoolkit>=0.1 in /usr/local/lib/python3.7/dist-packages (from muspy) (0.1.16)\n",
            "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5->muspy) (4.1.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.7/dist-packages (from music21>=6.0->muspy) (2.2.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from music21>=6.0->muspy) (8.14.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from music21>=6.0->muspy) (3.0.4)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.7/dist-packages (from music21>=6.0->muspy) (1.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty-midi>=0.2->muspy) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pypianoroll>=1.0->muspy) (1.7.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->muspy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->muspy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->muspy) (2022.6.15)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonpickle->music21>=6.0->muspy) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle->music21>=6.0->muspy) (3.8.1)\n",
            "Skip downloading as the MuseScore General soundfont is found.\n",
            "Skip downloading as the Bravura font is found.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fluidsynth is already the newest version (1.1.9-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyfluidsynth in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyfluidsynth) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "# Install muspy related code\n",
        "!pip install muspy\n",
        "import muspy\n",
        "muspy.download_musescore_soundfont() \n",
        "muspy.download_bravura_font() \n",
        "\n",
        "# Install fluidsynth related code\n",
        "!apt install fluidsynth\n",
        "!pip install pyfluidsynth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgRRhU6SSfmz"
      },
      "source": [
        "# musiGAN\n",
        "\n",
        "**Description:** 1-Track MuseGAN architecture build on MiniGAN.\\\n",
        "**Purpose:** implement a composing GAN.\\\n",
        "**Results:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "3Zy36G-CGamF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tqdm import notebook\n",
        "from datetime import datetime\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('tableau-colorblind10')\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import ndimage\n",
        "\n",
        "import muspy\n",
        "import fluidsynth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trwcbyaoSfm2"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ePkslq2wSfm4"
      },
      "outputs": [],
      "source": [
        "class Pianoroll :\n",
        "    def __init__ (self, filepath) :\n",
        "        assert  type(filepath) == str\n",
        "\n",
        "        # Creating the dataset from a file\n",
        "        stored_data = np.load(filepath)\n",
        "        data_array  = stored_data[\"data\"]\n",
        "        labels_array = stored_data[\"labels\"]\n",
        "        self.data   = torch.as_tensor(data_array, dtype = torch.float32)\n",
        "        self.labels = torch.as_tensor(labels_array, dtype = torch.int64)\n",
        "\n",
        "        self.dataset = torch.utils.data.TensorDataset(self.data, self.labels)\n",
        "\n",
        "        # Storing additional info about it\n",
        "        self.shape  = tuple(self.data.shape[1:])   # shape of one pianoroll image\n",
        "        self.size   = self.shape[0] * self.shape[1]\n",
        "        self.height       = self.data.shape[2]\n",
        "        self.width        = self.data.shape[1]\n",
        "        self.dataset_size = self.data.shape[0]\n",
        "\n",
        "    \n",
        "    def show (self, number = None) :\n",
        "        if number == None :\n",
        "            number = np.random.randint(self.dataset_size)\n",
        "        else :\n",
        "            assert  type(number) == int\n",
        "            assert  number >= 0 and number < self.dataset_size\n",
        "\n",
        "        plt.figure(figsize = (12, 6))\n",
        "        plt.title(f\"pianoroll #{number}\")\n",
        "        plt.imshow(self.data[number].T)\n",
        "        plt.show()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5pWodkypE8U"
      },
      "source": [
        "### LPD5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "KCEcB4f4pE8V"
      },
      "outputs": [],
      "source": [
        "lpd5 = Pianoroll(\"../experiments/lpd5_full_4bars/prepared_arrays.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "wRH2fKd1pE8W",
        "outputId": "01e2b6bd-100c-495b-f645-97ef8b0fbdac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 703,
              "height": 339
            },
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lpd5.dataset_size 102483\n",
            "lpd5.shape (192, 84)\n"
          ]
        }
      ],
      "source": [
        "lpd5.show()\n",
        "print(\"lpd5.dataset_size\", lpd5.dataset_size)\n",
        "print(\"lpd5.shape\", lpd5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_kByFrepE8X",
        "outputId": "f736ceca-65f5-4c3a-8dc0-3b3e6f08ba61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lpd5.blips_per_bar 48\n",
            "lpd5.pitches 84\n"
          ]
        }
      ],
      "source": [
        "lpd5.bars           = 4\n",
        "lpd5.lowest_pitch   = 24\n",
        "lpd5.blips_per_bar  = lpd5.width // lpd5.bars\n",
        "lpd5.blips_per_beat = lpd5.blips_per_bar // 4\n",
        "lpd5.pitches        = lpd5.height\n",
        "lpd5.octaves        = lpd5.pitches // 12\n",
        "lpd5.genre_list     = ['Rap', 'Latin', 'International', 'Electronic', \n",
        "                       'Country', 'Folk', 'Blues', 'Reggae', 'Jazz',\n",
        "                       'Vocal', 'New-Age', 'RnB', 'Pop_Rock']\n",
        "\n",
        "print(\"lpd5.blips_per_bar\", lpd5.blips_per_bar)\n",
        "print(\"lpd5.pitches\", lpd5.pitches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tnu9HElSfm5"
      },
      "source": [
        "## Architecture classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdGO-KlvSfm5"
      },
      "source": [
        "### Support classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "acpAO9NSSfm5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    These two classes serves as torch layers to binarize the output of the Generator while keeping the layer still \"backpropagatable\" (via a hardtanh).\n",
        "    This is not our own code. For source, see:\n",
        "    https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html#:~:text=A%20straight%2Dthrough%20estimator%20is,function%20was%20an%20identity%20function.\n",
        "\"\"\"\n",
        "\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0.5).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return torch.nn.functional.hardtanh(grad_output)\n",
        "\n",
        "class StraightThroughEstimator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StraightThroughEstimator, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # only binarize in eval() mode, not in training\n",
        "        x = x  if self.training  else  STEFunction.apply(x)\n",
        "        #x = STEFunction.apply(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "-_Tgofn5pfRU"
      },
      "outputs": [],
      "source": [
        "class GeneratorBlock(torch.nn.Module):\n",
        "    \"\"\" 2d transconv layer, batch normalization & ReLU \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gen_block = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(in_dim, out_dim, kernel, stride),\n",
        "            torch.nn.BatchNorm2d(out_dim),\n",
        "            torch.nn.ReLU()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen_block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "n38oto6cpfRW"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorBlock(torch.nn.Module):\n",
        "    \"\"\"3d conv layer & Leaky ReLU\"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
        "        super().__init__()\n",
        "        self.dis_block = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_dim, out_dim, kernel, stride),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2)   # MuseGAN Hyperparameter\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dis_block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU1bKYfdSfm6"
      },
      "source": [
        "### Main neural network classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "rOkq0b4VpfRV"
      },
      "outputs": [],
      "source": [
        "class MusiGen (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 64\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(64, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 3, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 128, 1024, (2, 1), (2, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 256,  256, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 256,  128, (3, 1), (3, 1)),\n",
        "            GeneratorBlock( 128,   64, (1, self.octaves), (1, self.octaves)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (1, 12), (1, 12)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward_custom (self, seed) :\n",
        "        assert  type(seed) == torch.Tensor\n",
        "        assert  len(seed.shape) == 2\n",
        "        assert  seed.shape[0] >= 1\n",
        "        assert  seed.shape[1] == (1 + self.bars) * self.seedlength\n",
        "\n",
        "        batchsize = seed.shape[0]\n",
        "        return self.forward(batchsize, seed)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        bar_seed_1 = seeds[0]\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "\n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            #print(f\"temporal seed: {temporal_seed.size()}\")\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 64)\n",
        "            #print(f\"bar seed 2: {bar_seed_2.size()}\")\n",
        "\n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "            \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 128 x 1 x 1)\n",
        "            #print(f\"bar seed: {bar_seed d.size()}\")\n",
        "\n",
        "            ## generate one bar \n",
        "            \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 24 x 84)\n",
        "            #print(f\"generated_bar: {generated_bar.size()}\")\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 24 x 84) \n",
        "        #print(f\"gen output: {pianoroll.size()}\")\n",
        "\n",
        "        return pianoroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "I5vGYaCfpfRW"
      },
      "outputs": [],
      "source": [
        "class MusiDis (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, n_labels = 13, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 64\n",
        "        self.n_labels   = n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 12), (1, 1, 12)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 1))\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, self.n_labels))\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swF_vr8kSfm9"
      },
      "source": [
        "## Training & evaluation classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8eX-HGSfm-"
      },
      "source": [
        "### Training support"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training metrics"
      ],
      "metadata": {
        "id": "L_lrgdWMSrIk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "D0p-ZDYGSfm-"
      },
      "outputs": [],
      "source": [
        "def abs_mean_diff (generated_batch, real_batch) :\n",
        "    \"\"\"\n",
        "        compare two batches of data by calculating the absolute mean difference\n",
        "    \"\"\"\n",
        "    \n",
        "    # equalize shapes\n",
        "    real_shape = real_batch.shape[-2:]\n",
        "    real_batch = real_batch.view(-1, *real_shape)\n",
        "    generated_batch = generated_batch.view(-1, *real_shape)\n",
        "    assert  generated_batch.shape == real_batch.shape\n",
        "\n",
        "    # averaged over batches \n",
        "    generated_mean = torch.mean(generated_batch, dim = 0)\n",
        "    real_mean      = torch.mean(real_batch, dim = 0)\n",
        "\n",
        "    # take differnece & absolut value, average over features lastly\n",
        "    absolute_mean_difference = torch.mean(torch.abs(real_mean - generated_mean))\n",
        "\n",
        "    return absolute_mean_difference.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def abs_std_diff (generated_batch, real_batch) :\n",
        "    \"\"\"\n",
        "        compare two batches of data by calculating the absolute standard deviation difference\n",
        "    \"\"\"\n",
        "    \n",
        "    # equalize shapes\n",
        "    real_shape = real_batch.shape[-2:]\n",
        "    real_batch = real_batch.view(-1, *real_shape)\n",
        "    generated_batch = generated_batch.view(-1, *real_shape)\n",
        "    assert  generated_batch.shape == real_batch.shape\n",
        "\n",
        "    # averaged over batches \n",
        "    generated_std = torch.std(generated_batch, dim = 0, unbiased = True)\n",
        "    real_std      = torch.std(real_batch, dim = 0, unbiased = True)\n",
        "    \n",
        "    # take differnece & absolut value, average over features lastly\n",
        "    absolute_std_difference = torch.mean(torch.abs(real_std - generated_std))\n",
        "\n",
        "    return absolute_std_difference.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "1uWF5EdvzU8Y"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inter_bar_var (generated_batch) :\n",
        "    \"\"\"\n",
        "        computes the inter-bar standard deviation\n",
        "    \"\"\"\n",
        "\n",
        "    inter_bar_std_dev = torch.mean(torch.std(generated_batch, dim = 1, \n",
        "                                             unbiased = True)) # std over bars\n",
        "    \n",
        "    return inter_bar_std_dev.cpu().detach().numpy()\n"
      ],
      "metadata": {
        "id": "cvYGRG9z2dR-"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inter_track_var (generated_batch) :\n",
        "    \"\"\"\n",
        "        computes the inter-track standard deviation\n",
        "    \"\"\"\n",
        "\n",
        "    inter_track_std_dev = torch.mean(torch.std(generated_batch, dim = 0, \n",
        "                                               unbiased = True)) # std over tracks\n",
        "    \n",
        "    return inter_track_std_dev.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "3NjqpcV34U3t"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss function support"
      ],
      "metadata": {
        "id": "xS0kN-LMSzV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "eYO2ATSm3oRb"
      },
      "outputs": [],
      "source": [
        "def unif_cross_entropy(probabilities, weight):\n",
        "    return(torch.mean(weight * torch.log(probabilities)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(probabilities, safe_normalization = True, eps = 0.000001):\n",
        "  \n",
        "    if safe_normalization == \"safe\":\n",
        "        exp_probs = torch.exp(probabilities)\n",
        "        normalization = torch.maximum(torch.sum(exp_probs, dim = 1), eps)\n",
        "  \n",
        "        if normalization > 0: \n",
        "            return(exp_probs / normalization)\n",
        "    \n",
        "        else:\n",
        "            return(exp_probs)\n",
        "  \n",
        "    else:\n",
        "        return(torch.nn.functional.softmax(probabilities, dim = 1))"
      ],
      "metadata": {
        "id": "TWKiWR-hYbJG"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: this function comes directly from the museGAN tutorial [1].\n",
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
        "    \"\"\"Compute the gradient penalty for regularization. Intuitively, the\n",
        "    gradient penalty help stablize the magnitude of the gradients that the\n",
        "    discriminator provides to the generator, and thus help stablize the training\n",
        "    of the generator.\"\"\"\n",
        "    # Get random interpolations between real and fake samples\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).to(device)\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n",
        "    interpolates = interpolates.requires_grad_(True)\n",
        "    \n",
        "    # Get the discriminator output for the interpolations\n",
        "    d_interpolates, _ = discriminator(interpolates)\n",
        "    # Get gradients w.r.t. the interpolations\n",
        "    fake = torch.ones(real_samples.size(0)).to(device)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    # Compute gradient penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Sources:\n",
        "# [1] https://github.com/salu133445/ismir2019tutorial/blob/main/musegan.ipynb"
      ],
      "metadata": {
        "id": "h0LYqJUeUJne"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logging"
      ],
      "metadata": {
        "id": "6ejVE_JNS2VE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Jxg1fBDoSfm_"
      },
      "outputs": [],
      "source": [
        "class Log :\n",
        "    \"\"\"\n",
        "        container class for GANTraining logs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self, rounds, dis_rounds, n_labels) :\n",
        "        self.losses        = np.zeros((5, rounds)) \n",
        "        self.music_probs   = np.zeros((2, rounds))\n",
        "        self.genre_probs   = np.zeros((1 + n_labels, rounds))\n",
        "        self.abs_diff      = np.zeros((2, rounds))  # abs_mean_diff(), abs_std_diff()\n",
        "        self.gen_var       = np.zeros((2, rounds))  # inter_bar_var(), inter_track_var()\n",
        "        \n",
        "        self._dis_losses   = torch.zeros((4, dis_rounds)).cpu()\n",
        "        self._music_probs  = torch.zeros((2, dis_rounds)).cpu()\n",
        "        self._genre_probs  = torch.zeros((1 + n_labels, rounds)).cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogLoaded :\n",
        "    \"\"\"\n",
        "        A class to load stored Log data from an .npz file\n",
        "        and to use it exactly like Log.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log_dictionary) :\n",
        "        for keyword, value in log_dictionary.items() :\n",
        "            setattr(self, keyword, value)\n",
        "        "
      ],
      "metadata": {
        "id": "jMlCiiOwBybk"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WstRQBd2SfnA"
      },
      "source": [
        "### GANTraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "fg1SZJl7SfnA"
      },
      "outputs": [],
      "source": [
        "class GANTraining :\n",
        "    \"\"\"\n",
        "        general GAN training class\n",
        "        How To Use:\n",
        "        * `MyTrain = GANTraining(<Generator>, <Discriminator>, <torch_dataset>)`\n",
        "        * `MyTrain.setup(<int_rounds>, batchsize = 1, discriminator_rounds = 1,     \n",
        "                        loss_function = [\"WGAN\", \"GAN\"])`\n",
        "        * `MyTrain.train()`\n",
        "      \n",
        "        After That:\n",
        "        * `MyTrain.gen` contains trained Generator\n",
        "        * `MyTrain.dis` contains trained Discriminator\n",
        "        * `MyTrain.log` contains metrics from each round (see class Log)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__ (self, Gen, Dis, dataset) :\n",
        "        assert  type(dataset) == torch.utils.data.dataset.TensorDataset\n",
        "        \n",
        "        self.device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "\n",
        "        # GAN classes and dataset\n",
        "        self.n_labels = 13     # number of labels in dataset, automate maybe\n",
        "        self.GenClass = Gen\n",
        "        self.DisClass = Dis\n",
        "        self.dataset  = dataset\n",
        "        \n",
        "\n",
        "    def setup (self, rounds, batch_size = 1, discriminator_rounds = 1, \n",
        "               loss_function = \"CAN\") :\n",
        "        assert  type(rounds) == int\n",
        "        assert  rounds >= 1\n",
        "        assert  type(batch_size) == int\n",
        "        assert  batch_size >= 1\n",
        "        assert  type(discriminator_rounds) == int\n",
        "        assert  discriminator_rounds >= 1\n",
        "        assert  loss_function in [\"GAN\", \"WGAN\", \"WGAN-GP\", \"CAN\", \"WCAN-GP\"]\n",
        "\n",
        "        # Training parameters\n",
        "        self.rounds     = rounds\n",
        "        self.batch_size = batch_size\n",
        "        self.dis_rounds = discriminator_rounds\n",
        "        self.loss       = loss_function\n",
        "\n",
        "        # Dataloader\n",
        "        self.data_loader = torch.utils.data.DataLoader(self.dataset,\n",
        "                                batch_size = self.batch_size, \n",
        "                                drop_last = True,\n",
        "                                shuffle = True)\n",
        "        self.dataset_size = self.dataset.tensors[0].shape[0]  # number of instances in dataset\n",
        "        self.batch_count = self.dataset_size // self.batch_size\n",
        "        self._batch_idx  = self.batch_count \n",
        "        \n",
        "        # Logs\n",
        "        self.log    = Log(self.rounds, self.dis_rounds, self.n_labels)\n",
        "        self.backup = False   # only on if self.set_backup() is run\n",
        "        \n",
        "        # Initialize GAN\n",
        "        self.gen = self.GenClass().to(self.device)\n",
        "        self.dis = self.DisClass().to(self.device)\n",
        "        self.optimizer_gen = torch.optim.Adam(self.gen.parameters(), \n",
        "                                              lr = 0.001,\n",
        "                                              betas = (0.5, 0.9))\n",
        "        self.optimizer_dis = torch.optim.Adam(self.dis.parameters(), \n",
        "                                              lr = 0.001,\n",
        "                                              betas = (0.5, 0.9))\n",
        "        # Note: ADAM parameters from GAN tutorial [1].\n",
        "       \n",
        " \n",
        "    def set_backups (self, training_name, checkpoints) :\n",
        "        assert  type(training_name) == str\n",
        "        for element in checkpoints :\n",
        "            assert  type(element) == int\n",
        "            assert  element > 0  and  element <= self.rounds\n",
        "        \n",
        "        self.training_name   = training_name\n",
        "        self.training_folder = \"\"   # 'timestamp+training_name', gets set at first backup\n",
        "        self.checkpoints     = checkpoints\n",
        "        self.backup          = True  # Flag for rest of code\n",
        "\n",
        "\n",
        "\n",
        "    def _get_batch (self) :\n",
        "        \"\"\"\n",
        "            samples one batch of data from self.data_loader without replacement.\n",
        "            When the self.data_set is depleted of fresh batches, \n",
        "            self.data_loader will shuffle a list of new batches.\n",
        "        \"\"\"\n",
        "        if self._batch_idx >= self.batch_count :\n",
        "            self._data_iter = iter(self.data_loader)\n",
        "            self._batch_idx = 0\n",
        "        batch_data, batch_labels = self._data_iter.next()\n",
        "        batch_data = batch_data.view(-1, lpd5.bars, \n",
        "                                     lpd5.blips_per_bar, lpd5.pitches)\n",
        "        self._batch_idx += 1\n",
        "\n",
        "        return batch_data.to(self.device), batch_labels.to(self.device)\n",
        "\n",
        "\n",
        "    def train (self) :\n",
        "        assert  hasattr(self, \"data_loader\")  # If test fails, you haven't run set_params()\n",
        "\n",
        "        print(f\"Training\")\n",
        "        arranged_tensor = torch.arange(self.batch_size) # used each round\n",
        "  \n",
        "        for round in notebook.tqdm(range(self.rounds)) :\n",
        "            for dis_round in range(self.dis_rounds) :\n",
        "                # Forward propagation\n",
        "                batch_real, labels_real        = self._get_batch()\n",
        "                music_dis_real, genre_dis_real = self.dis.forward(batch_real)\n",
        "                self.music_prob_real = torch.sigmoid(music_dis_real)\n",
        "                genre_probs_real     = softmax(genre_dis_real)\n",
        "                self.genre_prob_real = genre_probs_real[arranged_tensor, labels_real] # get prob of real genre\n",
        "                true_genre_dis_real = genre_dis_real[arranged_tensor, labels_real]\n",
        "                \n",
        "                batch_gen  = self.gen.forward(batch_size = self.batch_size)\n",
        "                music_dis_gen, genre_dis_gen = self.dis.forward(batch_gen)\n",
        "                self.music_prob_gen  = torch.sigmoid(music_dis_gen)\n",
        "                self.genre_probs_gen = softmax(genre_dis_gen)\n",
        "                \n",
        "                # Calculating the Discriminator loss function\n",
        "                if self.loss == \"GAN\" :\n",
        "                    self.loss_real = - torch.mean(torch.log(self.music_prob_real))\n",
        "                    self.loss_gen  = - torch.mean(torch.log(1 - self.music_prob_gen))\n",
        "                    self.loss_reg  = torch.tensor(0.)\n",
        "                \n",
        "                elif self.loss == \"WGAN\" :\n",
        "                    var_gen   = torch.var(music_dis_gen)\n",
        "                    var_real  = torch.var(music_dis_real)\n",
        "                    self.loss_reg  = torch.where(var_gen > 1, \n",
        "                                                 (var_gen - 1)**2, 0) \\\n",
        "                                     + torch.where(var_real > 1, \n",
        "                                                   (var_real - 1)**2, 0)\n",
        "                    self.loss_real = - torch.mean(music_dis_real)\n",
        "                    self.loss_gen  = torch.mean(music_dis_gen)\n",
        "                \n",
        "                elif self.loss == \"WGAN-GP\" :    \n",
        "                    var_gen   = torch.var(music_dis_gen)\n",
        "                    var_real  = torch.var(music_dis_real)\n",
        "                    self.loss_reg  = 10.0 * compute_gradient_penalty(\n",
        "                                     self.dis, batch_real, batch_gen, self.device)\n",
        "                    self.loss_real = - torch.mean(music_dis_real)\n",
        "                    self.loss_gen  = torch.mean(music_dis_gen)\n",
        "\n",
        "                elif self.loss == \"CAN\" :\n",
        "                    loss_real_music = - torch.mean(torch.log(self.music_prob_real))\n",
        "                    loss_real_genre = - torch.mean(torch.log(self.genre_prob_real))\n",
        "                    self.loss_real = loss_real_music + loss_real_genre\n",
        "                    self.loss_gen = - torch.mean(torch.log(1 - self.music_prob_gen))\n",
        "                    self.loss_reg  = torch.tensor(0.)\n",
        "\n",
        "                elif self.loss == \"WCAN-GP\" : \n",
        "                    var_gen   = torch.var(music_dis_gen)\n",
        "                    var_real  = torch.var(music_dis_real)\n",
        "                    self.loss_reg  = 10.0 * compute_gradient_penalty(\n",
        "                                     self.dis, batch_real, batch_gen, self.device)\n",
        "                    loss_real_music = - torch.mean(music_dis_real)\n",
        "                    loss_real_genre = - torch.mean(true_genre_dis_real)\n",
        "                    self.loss_real = loss_real_music + loss_real_genre \n",
        "                    self.loss_gen  = torch.mean(music_dis_gen)\n",
        "\n",
        "                self.loss_dis = self.loss_real + self.loss_gen + self.loss_reg\n",
        "                self._log_all(round, k = dis_round)\n",
        "                \n",
        "                # Discriminator update\n",
        "                self.optimizer_dis.zero_grad()\n",
        "                self.loss_dis.backward()\n",
        "                self.optimizer_dis.step()\n",
        "                \n",
        "\n",
        "            # Calculating the Generator loss function\n",
        "            batch_new = self.gen.forward(batch_size = self.batch_size)\n",
        "            music_dis_new, genre_dis_new = self.dis.forward(batch_new)\n",
        "                \n",
        "            if self.loss == \"GAN\" :\n",
        "                music_prob_new = torch.sigmoid(music_dis_new)\n",
        "                self.loss_gen = -torch.mean(torch.log(music_prob_new)) \n",
        "            \n",
        "            elif self.loss == \"WGAN\" :\n",
        "                self.loss_gen = -torch.mean(music_dis_new)\n",
        "\n",
        "            elif self.loss == \"WGAN-GP\" :\n",
        "                self.loss_gen = -torch.mean(music_dis_new)\n",
        "            \n",
        "            elif self.loss == \"CAN\" :\n",
        "                music_prob_new = torch.sigmoid(music_dis_new)\n",
        "                genre_probs_new = softmax(genre_dis_new)\n",
        "\n",
        "                loss_gen_music = - torch.mean(music_prob_new)\n",
        "                loss_gen_genre = - torch.mean( \\\n",
        "                    unif_cross_entropy(genre_probs_new, 1 / self.n_labels) + \\\n",
        "                    unif_cross_entropy(1 - genre_probs_new, 1 - 1 / self.n_labels))\n",
        "                \n",
        "                self.loss_gen = loss_gen_music + loss_gen_genre\n",
        "\n",
        "            elif self.loss == \"WCAN-GP\" :\n",
        "                self.loss_gen = -torch.mean(music_dis_new) \n",
        "                loss_gen_genre = torch.mean( \\\n",
        "                    unif_cross_entropy(1 + torch.exp(genre_dis_new), 1 / self.n_labels) + \\\n",
        "                    unif_cross_entropy(1 + torch.exp(-genre_dis_new), 1 - 1 / self.n_labels))\n",
        "\n",
        "\n",
        "            self._log_all(round)\n",
        "                \n",
        "            \n",
        "            # Generator update\n",
        "            self.optimizer_gen.zero_grad()\n",
        "            self.loss_gen.backward()\n",
        "            self.optimizer_gen.step()\n",
        "\n",
        "\n",
        "            # Make a backup\n",
        "            if self.backup  and  (round + 1) in self.checkpoints :\n",
        "                self._save_checkpoint(round + 1)\n",
        "                                \n",
        "            \n",
        "        # Put GAN in eval mode\n",
        "        self.gen.eval()\n",
        "        self.dis.eval()\n",
        "        print(\"Training complete. GAN now in eval() mode.\")\n",
        "\n",
        "\n",
        "    def _save_checkpoint (self, round) :\n",
        "        if round == min(self.checkpoints) :\n",
        "            self.training_folder = save_training(self.training_name, \n",
        "                                        self, checkpoint = round)\n",
        "        else :\n",
        "            save_training(self.training_folder, self, \n",
        "                            checkpoint = round, new_folder = False)\n",
        "        \n",
        "\n",
        "    def _log_all (self, round, k = -1) :\n",
        "        if k >= 0 : # before each Discriminator update\n",
        "            self.log._dis_losses[0, k] = self.loss_dis.cpu().detach()\n",
        "            self.log._dis_losses[1, k] = self.loss_real.cpu().detach()\n",
        "            self.log._dis_losses[2, k] = self.loss_gen.cpu().detach()\n",
        "            self.log._dis_losses[3, k] = self.loss_reg.cpu().detach()\n",
        "            self.log._music_probs[0, k] = self.music_prob_real.mean().cpu().detach()\n",
        "            self.log._music_probs[1, k] = self.music_prob_gen.mean().cpu().detach()\n",
        "            self.log._genre_probs[0, k]  = self.genre_prob_real.mean().cpu().detach() # prob of right label of real batch\n",
        "            self.log._genre_probs[1:, k] = self.genre_probs_gen.mean().cpu().detach() # prob of genres of generated batch\n",
        "        \n",
        "        if k == -1 : # before each Generator update\n",
        "            # Losses\n",
        "            dis_losses = self.log._dis_losses.detach().cpu().numpy()\n",
        "            self.log.losses[0:4, round] = dis_losses.mean(axis = 1)\n",
        "            self.log.losses[4, round]   = self.loss_gen.detach().cpu().numpy()\n",
        "            \n",
        "            # Discriminator Probabilities\n",
        "            music_probs                 = self.log._music_probs.cpu().detach().numpy()\n",
        "            genre_probs                 = self.log._genre_probs.cpu().detach().numpy()\n",
        "            self.log.music_probs[:, round] = music_probs.mean(axis = 1)\n",
        "            self.log.genre_probs[:, round] = genre_probs.mean(axis = 1)\n",
        "\n",
        "            # Generator metrics\n",
        "            batch_real, _ = self._get_batch()\n",
        "            batch_gen     = self.gen.forward(batch_size = self.batch_size)\n",
        "            self.log.abs_diff[0, round] = abs_mean_diff(batch_gen, batch_real)\n",
        "            self.log.abs_diff[1, round] = abs_std_diff(batch_gen, batch_real)\n",
        "            self.log.gen_var[0, round]  = inter_bar_var(batch_gen)\n",
        "            self.log.gen_var[1, round]  = inter_track_var(batch_gen)\n",
        "            \n",
        "\n",
        "\n",
        "# Sources:\n",
        "# [1] https://github.com/salu133445/ismir2019tutorial/blob/main/gan.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UjwRjHiook6"
      },
      "source": [
        "### Evaluation support"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default_training_path = \"../experiments\"\n",
        "default_dataset       = \"lpd5_full_4bars\""
      ],
      "metadata": {
        "id": "NJhnF4DsE0P4"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation metrics"
      ],
      "metadata": {
        "id": "ixzSjpbOS64f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def empty_bar_ratio (data) :\n",
        "    \"\"\"\n",
        "        ratio of bars devoid of notes\n",
        "        \n",
        "        also called:\n",
        "            EB = \"empty bar ratio\"\n",
        "    \"\"\"\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches)) # split into bars\n",
        "    data_reduced = np.mean(data, axis = (2, 3)).flatten() # mean over bar pixels\n",
        "    data_mask    = np.array(data_reduced == 0)  # bool of which bars are empty\n",
        "    empty_bar_fraction = np.mean(data_mask)  # mean over all bars\n",
        "\n",
        "    return empty_bar_fraction"
      ],
      "metadata": {
        "id": "3STzYDPRS9HD"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pitch_classes_per_bar (data) :\n",
        "    \"\"\"\n",
        "        number of pitch classes used per bar (from 0 to 12)\n",
        "        \n",
        "        also called:\n",
        "            UPC = \"used pitch classes per bar\"\n",
        "    \"\"\"\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches)) # split into bars\n",
        "    data = data.reshape((-1, lpd5.blips_per_bar, lpd5.pitches))  # array of bars\n",
        "    data = data.reshape((-1, lpd5.blips_per_bar, lpd5.octaves, 12)) # split into octaves\n",
        "    \n",
        "    pitches_used = np.any(data, axis = (1, 2))  # OR over timesteps and octaves\n",
        "    number_pitches = np.sum(pitches_used, axis = 1) # sum over pitches\n",
        "    mean_pitch_classes_per_bar = np.mean(number_pitches) # mean over all bars\n",
        "    \n",
        "    return mean_pitch_classes_per_bar"
      ],
      "metadata": {
        "id": "GclWeQ3TU7hU"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qualified_note_ratio (data) :\n",
        "    \"\"\"\n",
        "        ratio of \"qualified\" notes,\n",
        "        defined as a 3 blips/timesteps or longer. \n",
        "        In the current lpd5 dataset with 48-blip bars that is a 1/16 note.\n",
        "        ! Not like in museGAN (used 96-blip bars and thus a 1/32 note threshold)\n",
        "        \n",
        "        also called:\n",
        "            QN = \"qualified note ratio\"\n",
        "    \"\"\"\n",
        "    minimum_length = 3 # blips\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.width, lpd5.height)) # whole tracks\n",
        "    conv = np.array([-1, 1]) # used to measure note start and ends\n",
        "\n",
        "    total_notes       = 0\n",
        "    total_quali_notes = 0\n",
        "    for track in data :\n",
        "        for pitch_line in track.T :\n",
        "            note_starts = np.convolve(pitch_line, conv)\n",
        "            note_stops  = np.convolve(pitch_line, -conv)\n",
        "            start_indices = np.where(note_starts == -1)[0]\n",
        "            stop_indices  = np.where(note_stops == -1)[0]\n",
        "            \n",
        "            note_lengths     = stop_indices - start_indices\n",
        "            note_count       = note_lengths.shape[0]\n",
        "            quali_note_count = np.sum(note_lengths >= minimum_length)\n",
        "            total_notes       += note_count\n",
        "            total_quali_notes += quali_note_count\n",
        "\n",
        "    quali_note_ratio = total_quali_notes / total_notes\n",
        "\n",
        "    return quali_note_ratio"
      ],
      "metadata": {
        "id": "10tasJPahEvQ"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def muspy_metrics (data) :\n",
        "    \"\"\"\n",
        "    computes 4 muspy metrics from a batch of pianoroll data\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    averaged_metrics : np.array, size = (4), dtype = float\n",
        "        all values taken from whole pianoroll tracks and\n",
        "        are averaged over all tracks\n",
        "        1. muspy.pitch_range()\n",
        "            pitch range from lowest to highest pitch\n",
        "        2. muspy.polyphony()\n",
        "            average number of pitches being played concurrently\n",
        "        3. muspy.scale_consistency()\n",
        "            how many of the notes are in the track’s main scale \n",
        "            (max of notes in any scale)\n",
        "        4. muspy.empty_measure_rate()\n",
        "            ratio of 1/4 note beats where no note is played\n",
        "            \"measure\" is here defined as 1/4 notes by us.\n",
        "\n",
        "        For more details, see [1]\n",
        "\n",
        "    [1] https://muspy.readthedocs.io/en/stable/metrics.html?highlight=measures#other-metrics\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.width, lpd5.height)) # whole tracks\n",
        "    data.dtype = bool\n",
        "    \n",
        "    pianorolls = np.pad(data, \n",
        "                        ((0, 0), (0, 0), \n",
        "                         (lpd5.lowest_pitch, \n",
        "                          128 - lpd5.lowest_pitch - lpd5.height))\n",
        "                 )   # complete the pitch range\n",
        "    \n",
        "    muspy_stats = np.zeros((4, data.shape[0]))\n",
        "    for i, track in enumerate(pianorolls):\n",
        "        piano_music = muspy.from_pianoroll_representation(\n",
        "                        track,\n",
        "                        resolution = lpd5.blips_per_beat, \n",
        "                        encode_velocity = False\n",
        "                    )   # convert to muspy.music_object\n",
        "                  \n",
        "        muspy_stats[0, i] = muspy.pitch_range(piano_music)\n",
        "        muspy_stats[1, i] = muspy.polyphony(piano_music)\n",
        "        muspy_stats[2, i] = muspy.scale_consistency(piano_music)\n",
        "        muspy_stats[3, i] = muspy.empty_measure_rate(piano_music, \n",
        "                                                     lpd5.blips_per_beat)\n",
        "        \n",
        "    averaged_metrics = np.nanmean(muspy_stats, axis = 1)\n",
        "    \n",
        "    return averaged_metrics"
      ],
      "metadata": {
        "id": "jEpHEch60X02"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate key metrics of dataset for evaluation\n",
        "\n",
        "lpd5_metrics_file = f\"{default_training_path}/{default_dataset}/lpd5_metrics.json\"\n",
        "\n",
        "if not os.path.exists(lpd5_metrics_file):\n",
        "    # Calculating these metrics takes several minutes for lpd5.\n",
        "    # Therefore, they are calculated once and then stored in a file.\n",
        "    metrics = {}\n",
        "    metrics[\"abs_mean_diff\"]   = 0   # difference of dataset to itself\n",
        "    metrics[\"abs_std_diff\"]    = 0\n",
        "    metrics[\"inter_bar_var\"]   = inter_bar_var(lpd5.data.view(-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches))\n",
        "    metrics[\"inter_track_var\"] = inter_track_var(lpd5.data)\n",
        "    metrics[\"empty_bar_ratio\"]        = empty_bar_ratio(lpd5.data)\n",
        "    metrics[\"pitch_classses_per_bar\"] = pitch_classes_per_bar(lpd5.data)\n",
        "    metrics[\"qualified_note_ratio\"]   = qualified_note_ratio(lpd5.data)\n",
        "    metrics[\"muspy_metrics\"] = muspy_metrics(lpd5.data)\n",
        "    with open(lpd5_metrics_file, 'wb') as file :\n",
        "        pickle.dump(metrics, file)\n",
        "\n",
        "\n",
        "with open(lpd5_metrics_file, 'rb') as file :\n",
        "    # Loading all metrics is much quicker than recalculating them\n",
        "    metrics = pickle.load(file)\n",
        "    lpd5.abs_mean_diff   = metrics[\"abs_mean_diff\"]\n",
        "    lpd5.abs_std_diff    = metrics[\"abs_std_diff\"]\n",
        "    lpd5.inter_bar_var   = metrics[\"inter_bar_var\"]\n",
        "    lpd5.inter_track_var = metrics[\"inter_track_var\"]\n",
        "    lpd5.empty_bar_ratio        = metrics[\"empty_bar_ratio\"]\n",
        "    lpd5.pitch_classses_per_bar = metrics[\"pitch_classses_per_bar\"]\n",
        "    lpd5.qualified_note_ratio   = metrics[\"qualified_note_ratio\"]\n",
        "    lpd5.muspy_metrics = metrics[\"muspy_metrics\"]\n"
      ],
      "metadata": {
        "id": "XqBIvah5RK7k"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Show results"
      ],
      "metadata": {
        "id": "LyJRhxx8L_4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training (log, dataset = lpd5, CAN = False) :\n",
        "    training_rounds = log.losses.shape[1]\n",
        "    rounds          = np.arange(training_rounds) + 1\n",
        "    filter_size     = math.ceil(training_rounds / 100)\n",
        "    med_filter      = lambda x: ndimage.median_filter(x, size = filter_size)\n",
        "    \n",
        "    \n",
        "\n",
        "    # Training metrics\n",
        "\n",
        "    plt.figure(figsize = (16, 8))\n",
        "    plt.suptitle(\"Training metrics\", size=18)\n",
        "    \n",
        "    ## Losses\n",
        "    plt.title(\"Losses\")\n",
        "    dis_loss = log.losses[0]\n",
        "    gen_loss = log.losses[4]\n",
        "    plt.plot(rounds, dis_loss, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, gen_loss, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(dis_loss), label=\"Discriminator Loss\", \n",
        "             c=\"b\") #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(gen_loss), label=\"Generator Loss\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale('symlog', linthreshy = 10)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    ## Probabilities\n",
        "    plt.figure(figsize=(16,4))\n",
        "    prob_real = log.music_probs[0]\n",
        "    prob_gen  = log.music_probs[1]\n",
        "    prob_diff = prob_real - prob_gen\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(r\"$p_{Dis}(data_{real} = real)$\")\n",
        "    plt.plot(rounds, np.ones_like(prob_real), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, np.zeros_like(prob_real), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, prob_real, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(prob_real), c=\"b\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(r\"$p_{Dis}(data_{real} = real) - p_{Dis}(data_{gen} = real)$\")\n",
        "    plt.plot(rounds, np.ones_like(prob_diff), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, np.zeros_like(prob_diff), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, prob_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(prob_diff), c=\"b\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "    ## CAN metrics\n",
        "    if CAN :\n",
        "        genre_probs = log.genre_probs[1:]\n",
        "        \n",
        "        plt.title(\"Batch-averaged Generator Probabilities during Training\")\n",
        "        plt.plot(rounds, np.ones(genre_probs.shape[1]), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        plt.plot(rounds, np.zeros(genre_probs.shape[1]), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        plt.plot(rounds, genre_probs.T)\n",
        "        plt.xlabel(\"round\")\n",
        "        plt.legend(dataset.genre_list)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # Generator metrics\n",
        "\n",
        "    plt.figure(figsize=(16,6))\n",
        "    plt.suptitle(\"Generator metrics\", size=18)\n",
        "    abs_mean_diff   = log.abs_diff[0]\n",
        "    abs_std_diff    = log.abs_diff[1]\n",
        "    inter_bar_var   = log.gen_var[0]\n",
        "    inter_track_var = log.gen_var[1]\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Absolute mean and std difference to dataset\")\n",
        "    plt.plot(rounds, abs_mean_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, abs_std_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(abs_mean_diff), label = \"abs_mean_diff\", \n",
        "             c='b') #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(abs_std_diff), label = \"abs_std_diff\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Generator variation\")\n",
        "    plt.plot(rounds, np.ones_like(inter_bar_var) * lpd5.inter_bar_var, \n",
        "             linestyle=\"--\", lw=0.5, color='b', label=\"dataset bar-wise std.\")\n",
        "    plt.plot(rounds, np.ones_like(inter_bar_var) * lpd5.inter_track_var, \n",
        "             linestyle=\"--\", lw=0.5, color='r', label=\"dataset track-wise std.\")\n",
        "    plt.plot(rounds, inter_bar_var, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, inter_track_var, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(inter_bar_var), label = \"bar-wise std dev.\", \n",
        "             c=\"b\") #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(inter_track_var), label = \"track-wise std dev.\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_VBC5mONqw-T"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "mWBpY1DkSfm_"
      },
      "outputs": [],
      "source": [
        "def long_test (generator, discriminator, data, test_size = 1000, \n",
        "               dataset = default_dataset): \n",
        "    \"\"\"\n",
        "        runs a detailed evaluation of generator performance\n",
        "        and compares it to the training data set in a pandas table\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    with torch.inference_mode() :  # saves gpu memory\n",
        "        device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "        \n",
        "        # Test on generated data\n",
        "        generator.eval().to(device)\n",
        "        discriminator.eval().to(device)\n",
        "        data_real, labels_real = iter(torch.utils.data.DataLoader(data.dataset,\n",
        "                        batch_size = test_size, \n",
        "                        shuffle = True, drop_last = True)\n",
        "                    ).next()  # make one batch of test_size\n",
        "        data_real   = data_real.to(device)\n",
        "        labels_real = labels_real.to(device)\n",
        "\n",
        "        ## Generator\n",
        "        data_generated = generator.forward(batch_size = test_size)\n",
        "        \n",
        "        gen_abs_mean_diff   = abs_mean_diff(data_generated, data_real)\n",
        "        gen_abs_std_diff    = abs_std_diff(data_generated, data_real)\n",
        "        gen_inter_bar_var   = inter_bar_var(data_generated)\n",
        "        gen_inter_track_var = inter_track_var(data_generated)\n",
        "\n",
        "        gen_empty_bar_ratio        = empty_bar_ratio(data_generated)\n",
        "        gen_pitch_classses_per_bar = pitch_classes_per_bar(data_generated)\n",
        "        gen_qualified_note_ratio   = qualified_note_ratio(data_generated)\n",
        "\n",
        "        gen_muspy_metrics = muspy_metrics(data_generated)\n",
        "\n",
        "    # Create comparison table: generated data vs. real data\n",
        "    \n",
        "    real_music_metrics = np.array([\n",
        "        lpd5.abs_mean_diff,\n",
        "        lpd5.abs_std_diff,\n",
        "        lpd5.inter_bar_var,\n",
        "        lpd5.inter_track_var,\n",
        "        lpd5.empty_bar_ratio,\n",
        "        lpd5.pitch_classses_per_bar,\n",
        "        lpd5.qualified_note_ratio,\n",
        "        lpd5.muspy_metrics[0],\n",
        "        lpd5.muspy_metrics[1],\n",
        "        lpd5.muspy_metrics[2],\n",
        "        lpd5.muspy_metrics[3],\n",
        "    ], dtype = float).round(2)\n",
        "    gen_music_metrics = np.array([\n",
        "        gen_abs_mean_diff,\n",
        "        gen_abs_std_diff,\n",
        "        gen_inter_bar_var,\n",
        "        gen_inter_track_var,\n",
        "        gen_empty_bar_ratio,\n",
        "        gen_pitch_classses_per_bar,\n",
        "        gen_qualified_note_ratio,\n",
        "        gen_muspy_metrics[0],\n",
        "        gen_muspy_metrics[1],\n",
        "        gen_muspy_metrics[2],\n",
        "        gen_muspy_metrics[3],\n",
        "    ], dtype = float).round(2)\n",
        "\n",
        "    table_dict = {\n",
        "        \"Metrics\":[\n",
        "            \"Absoluted mean difference\", \n",
        "            \"Absoluted standard deviation difference\", \n",
        "            \"Inter-bar standard deviation\",\n",
        "            \"Inter-track standard deviation\",\n",
        "            \"Empty bar ratio\",\n",
        "            \"Used pitch classes per bar\",\n",
        "            \"Qualified note ratio\",\n",
        "            \"Pitch range\",\n",
        "            \"Polyphony\",\n",
        "            \"Scale consistency\",\n",
        "            \"Empty 1/4 note ratio\",\n",
        "        ],\n",
        "        \"real music\":real_music_metrics,\n",
        "        \"generated music\":gen_music_metrics,\n",
        "        \"Abbreviation\":[\"AMD\", \"ASD\", \"IBS\", \"ITS\", \"EB\", \"UBS\", \"QN\", \n",
        "                        \"PR\", \"PL\", \"SC\", \"EN\",],\n",
        "        \"metric source\":[\"own\", \"own\", \"own\", \"own\", \n",
        "                        \"museGAN\", \"museGAN\", \"museGAN\",\n",
        "                        \"muspy\", \"muspy\", \"muspy\", \"muspy\",],        \n",
        "    }\n",
        "    \n",
        "    table_panda = pd.DataFrame(table_dict)\n",
        "    \n",
        "    print(\"Comparison between the real and the generated music\\n\")\n",
        "    display(table_panda)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_test (generator, discriminator, data, test_size = 100, num_images = 1, \n",
        "                dataset = default_dataset, save_to = None) : \n",
        "\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "\n",
        "    # Calculating Discriminator predictions\n",
        "    with torch.inference_mode() :    \n",
        "        # Loading real data and models\n",
        "        generator.eval().to(device)\n",
        "        discriminator.eval().to(device)\n",
        "        data_real, labels_real = iter(torch.utils.data.DataLoader(data.dataset,\n",
        "                        batch_size = test_size, \n",
        "                        shuffle = True, drop_last = True)\n",
        "                    ).next()  # make one batch of test_size\n",
        "        data_real   = data_real.to(device)\n",
        "        labels_real = labels_real.to(device)\n",
        "\n",
        "        # Generator\n",
        "        data_generated = generator.forward(batch_size = test_size)\n",
        "        \n",
        "\n",
        "        # Discriminator\n",
        "        music_dis_gen,  genre_dis_gen  = discriminator.forward(data_generated)\n",
        "        music_prob_gen  = torch.sigmoid(music_dis_gen)\n",
        "        std_prob_gen    = torch.std_mean(music_prob_gen, unbiased=True)\n",
        "        \n",
        "        music_dis_real, genre_dis_real = discriminator.forward(data_real)\n",
        "        music_prob_real = torch.sigmoid(music_dis_real)\n",
        "        std_prob_real   = torch.std_mean(music_prob_real, unbiased=True)\n",
        "        \n",
        "        # Converting some generated data to pianorolls\n",
        "        data_generated = data_generated.cpu().detach().numpy()\n",
        "        images         = data_generated[:num_images]\n",
        "        images         = images.reshape(-1, data.width, data.height)\n",
        "        pianorolls     = np.pad(images, ((0, 0), (0, 0), \n",
        "                                        (data.lowest_pitch, \n",
        "                                        128 - data.lowest_pitch - data.height)))   \n",
        "                            # complete the pitch range\n",
        "\n",
        "    # Create audio save folder\n",
        "    default_path = f\"{default_training_path}/{dataset}\"\n",
        "    if save_to == None :\n",
        "        audio_folder = f\"{default_path}/temp_audio\"\n",
        "    else :\n",
        "        audio_folder = f\"{default_path}/{save_to}/audio\"\n",
        "    try:   # make new folder\n",
        "        os.makedirs(audio_folder)\n",
        "    except OSError:   # it already exists\n",
        "        pass\n",
        "\n",
        "\n",
        "    # Discriminator Results\n",
        "    print(f\"Discriminator p(x_real = real) = \" +\n",
        "        f\"{std_prob_real[1]*100:.0f}±{std_prob_real[0]*100:.0f}%\")\n",
        "    print(f\"Discriminator p(x_gen = real)  = \" +\n",
        "        f\"{std_prob_gen[1]*100:.0f}±{std_prob_gen[0]*100:.0f}%\")\n",
        "    print(\"\\n\\n\")\n",
        "    \n",
        "    # Generator examples\n",
        "    print(\"Example of the generated music\")\n",
        "    for i in range(num_images) :\n",
        "        piano_music = muspy.from_pianoroll_representation(pianorolls[i] > 0,\n",
        "                        resolution = data.blips_per_beat, \n",
        "                        encode_velocity = False)   # convert to muspy.music_object\n",
        "        \n",
        "        # save audio tracks\n",
        "        timestamp   = datetime.now()\n",
        "        filepath    = f\"{audio_folder}/{timestamp}.wav\"\n",
        "        muspy.write_audio(path = filepath, music = piano_music)\n",
        "\n",
        "        # Display example pianorolls with audio\n",
        "        muspy.visualization.show_pianoroll(piano_music)\n",
        "        display(Audio(filename = filepath))\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "ntUausIx9L9q"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save and load trained models and logs"
      ],
      "metadata": {
        "id": "k4rFGcgSMO7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_training (training_name, trainer, info_txt = None, \n",
        "                   dataset = default_dataset, new_folder = True, \n",
        "                   checkpoint = 0) :\n",
        "    assert  type(training_name) == str               \n",
        "    assert  info_txt == None  or  type(info_txt) == str\n",
        "    \n",
        "    # If trainer has already saved a checkpoint, no new folder is needed.\n",
        "    if hasattr(trainer, \"training_name\") :\n",
        "        if (training_name == trainer.training_name  and\n",
        "            trainer.training_folder != \"\") :\n",
        "            \n",
        "            training_name = trainer.training_folder\n",
        "            new_folder    = False\n",
        "    \n",
        "    # Name the save folder\n",
        "    if new_folder:\n",
        "        now             = datetime.now()\n",
        "        date            = f\"{now.year}-{now.month:02d}-{now.day:02d}\"\n",
        "        time            = f\"{now.hour:02d}-{now.minute:02d}\"\n",
        "        timestamp       = f\"{date}_{time}\"\n",
        "        training_folder = f\"{timestamp}_{training_name}\"\n",
        "    else :\n",
        "        training_folder = training_name\n",
        "    save_folder = f\"{default_training_path}/{dataset}/{training_folder}\"\n",
        "    \n",
        "    model_folder = f\"{save_folder}/model\"\n",
        "    try:   # make new folder\n",
        "        os.makedirs(model_folder)\n",
        "    except OSError:   # it already exists\n",
        "        pass\n",
        "    \n",
        "\n",
        "    # save models\n",
        "    gen = trainer.gen\n",
        "    dis = trainer.dis\n",
        "    \n",
        "    if checkpoint == 0 :\n",
        "        torch.save(gen.state_dict(), f\"{model_folder}/gen.pt\")\n",
        "        torch.save(dis.state_dict(), f\"{model_folder}/dis.pt\")\n",
        "    else : \n",
        "        # here, checkpoint is an int: the current training round number\n",
        "        torch.save(gen.state_dict(), f\"{model_folder}/gen{checkpoint}.pt\")\n",
        "        torch.save(dis.state_dict(), f\"{model_folder}/dis{checkpoint}.pt\")\n",
        "\n",
        "    # save logs\n",
        "    if checkpoint == 0 :\n",
        "        log_file = f\"{save_folder}/logs.npz\"\n",
        "        log_dict = trainer.log.__dict__\n",
        "    else :\n",
        "        # here, checkpoint is an int: the current training round number\n",
        "        total_rounds = checkpoint\n",
        "        log_file = f\"{save_folder}/logs{checkpoint}.npz\"\n",
        "        log_dict = trainer.log.__dict__.copy()\n",
        "        # shorten the log arrays to current checkpoint \n",
        "        for key, value in log_dict.items() :\n",
        "            if type(value) == np.ndarray :\n",
        "                log_dict[key] = value[:, :total_rounds]\n",
        "    \n",
        "    np.savez(log_file, **log_dict)     \n",
        "    \n",
        "\n",
        "    # save additional info about training\n",
        "    info_path  = f\"{save_folder}/info.txt\"\n",
        "    and_info   = \"\"\n",
        "    if info_txt != None :\n",
        "        with open(info_path, \"w+\") as f :\n",
        "            f.writelines(info_txt)\n",
        "        and_info = \"and info text \"  \n",
        "    \n",
        "    if checkpoint == 0:\n",
        "        print(f\"Saved models {and_info}under:\\n\",\n",
        "            f\"'{default_training_path}/{dataset}/\\n\",\n",
        "            f\" {training_folder}'\")\n",
        "        \n",
        "    return training_folder "
      ],
      "metadata": {
        "id": "m92AAeQbMMl1"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_training (training_folder, model = (MusiGen, MusiDis), \n",
        "                   print_info = False, dataset = default_dataset, \n",
        "                   checkpoint = 0) :\n",
        "    save_folder = f\"{default_training_path}/{dataset}/{training_folder}\"\n",
        "    assert  os.path.exists(save_folder)\n",
        "\n",
        "    # load models\n",
        "    model_folder       = f\"{save_folder}/model\"\n",
        "    GenClass, DisClass = model\n",
        "    gen, dis           = GenClass(), DisClass()\n",
        "    if checkpoint == 0 :\n",
        "        gen.load_state_dict(torch.load(f\"{model_folder}/gen.pt\"))\n",
        "        dis.load_state_dict(torch.load(f\"{model_folder}/dis.pt\"))\n",
        "    else :\n",
        "        # here, checkpoint is an int: the current training round number\n",
        "        gen.load_state_dict(torch.load(f\"{model_folder}/gen{checkpoint}.pt\"))\n",
        "        dis.load_state_dict(torch.load(f\"{model_folder}/dis{checkpoint}.pt\"))\n",
        "    \n",
        "\n",
        "    # Prepare models for evaluation\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "    gen    = gen.to(device)\n",
        "    dis    = dis.to(device)\n",
        "    gen.eval()\n",
        "    dis.eval()\n",
        "\n",
        "    # load logs\n",
        "    if checkpoint == 0 :\n",
        "        log_file = f\"{save_folder}/logs.npz\"\n",
        "    else :\n",
        "        log_file = f\"{save_folder}/logs{checkpoint}.npz\"\n",
        "\n",
        "    logs = None\n",
        "    with np.load(log_file) as log_dict :\n",
        "        logs = LogLoaded(log_dict)\n",
        "    \n",
        "    # load info\n",
        "    info_path  = f\"{save_folder}/info.txt\"\n",
        "    if print_info :\n",
        "        with open(info_path, \"r\") as f :\n",
        "            print(f.read())\n",
        "    \n",
        "    return gen, dis, logs"
      ],
      "metadata": {
        "id": "FK6Iwny8CvsL"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7J679rwSfnC"
      },
      "source": [
        "## Network training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Training"
      ],
      "metadata": {
        "id": "Q6uuHHR6fQOe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gasGW3KpSfnM",
        "outputId": "3173f558-a50f-409a-a1a8-a0381dc86c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "3bb8c3e984c147e492ca011058c69931",
            "1a10505b49e9423482a240b7220edd4e",
            "11f9aa4a8bae4af1bbdb88f0e8fc128f",
            "50ab53eb79af4e06a7ed6b6ecd979989",
            "08c11f4ce77c46cfb945b72e401e09a2",
            "579789436aac4cad80ce263e4ab07c7c",
            "a0562a20fc944ea6ad3576aea3a22fd8",
            "89533421e32d4c5fa573a88afb419127",
            "822763735f1f4fb8a8b0cac329d2a0a5",
            "1ad52e8660ac470d842dd0b575ca300a",
            "d017fb1525d748b79a28f2dc547d2833"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bb8c3e984c147e492ca011058c69931"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_name = \"10k-vanilla-MusiCAN\"\n",
        "lpd5Train     = GANTraining(MusiGen, MusiDis, lpd5.dataset)\n",
        "lpd5Train.setup(10000, batch_size = 25, discriminator_rounds = 5, \n",
        "                loss_function = \"WCAN-GP\")\n",
        "#lpd5Train.set_backups(training_name, checkpoints = [1, 2, 5, 10])\n",
        "lpd5Train.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eBKbfRJbh6D"
      },
      "outputs": [],
      "source": [
        "info_text     = \\\n",
        "f\"\"\"\n",
        "Training info: {training_name}\n",
        "=======================\n",
        "\n",
        "models: MusiGen, MusiDis (note [1])\n",
        "dataset: lpd5_full_4bars\n",
        "\n",
        "rounds = 10000\n",
        "batch_size = 25\n",
        "discriminator_rounds = 5\n",
        "loss_function = WCAN-GP\n",
        "checkpoints   = []\n",
        "\n",
        "adam_optimizer_params:\n",
        "    gen: (lr = 0.001, betas = (0.5, 0.9))\n",
        "    dis: (lr = 0.001, betas = (0.5, 0.9))\n",
        "\n",
        "additional comments:\n",
        "    Try naive WCAN loss function: the discriminator term is analogous to the \n",
        "    GAN -> WGAN transformation; the generator term is just the original CAN one\n",
        "    for an typical WGAN discriminator output\n",
        "    \n",
        "    [1] Generator output layer is set to be only binarized if in testing mode\n",
        "        (museGAN setting)\n",
        "    \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47lzP9FrGRaI"
      },
      "outputs": [],
      "source": [
        "training_folder_name = save_training(training_name, lpd5Train, info_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXwYSkWt3oRg"
      },
      "outputs": [],
      "source": [
        "plot_training(lpd5Train.log, CAN = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd-LOltQSfnN"
      },
      "outputs": [],
      "source": [
        "long_test(lpd5Train.gen, lpd5Train.dis, lpd5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quick_test(lpd5Train.gen, lpd5Train.dis, lpd5, num_images = 3, \n",
        "           save_to = training_folder_name)"
      ],
      "metadata": {
        "id": "D85pAG48E0Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and investigating old trained models"
      ],
      "metadata": {
        "id": "zb-NJGprfWGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen, dis, logs = load_training(training_folder_name, print_info = True)"
      ],
      "metadata": {
        "id": "xI_lD4kCfbl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(logs)"
      ],
      "metadata": {
        "id": "j7D5VmVLffr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_test(gen, dis, lpd5)"
      ],
      "metadata": {
        "id": "MUYxjanafl3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_test(gen, dis, lpd5, num_images = 1)"
      ],
      "metadata": {
        "id": "ajOJ3t7ngFgr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8ATBcagDnDpM",
        "trwcbyaoSfm2",
        "E5pWodkypE8U",
        "3tnu9HElSfm5",
        "KdGO-KlvSfm5",
        "kU1bKYfdSfm6",
        "swF_vr8kSfm9",
        "vG8eX-HGSfm-",
        "L_lrgdWMSrIk",
        "xS0kN-LMSzV0",
        "6ejVE_JNS2VE",
        "WstRQBd2SfnA",
        "9UjwRjHiook6",
        "ixzSjpbOS64f",
        "LyJRhxx8L_4y",
        "k4rFGcgSMO7j",
        "Q6uuHHR6fQOe",
        "zb-NJGprfWGK"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('aml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e9db57278ae8397bc5fe3bec6b9ba53c33a2aa76e79d386f678fc754e34f9547"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3bb8c3e984c147e492ca011058c69931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a10505b49e9423482a240b7220edd4e",
              "IPY_MODEL_11f9aa4a8bae4af1bbdb88f0e8fc128f",
              "IPY_MODEL_50ab53eb79af4e06a7ed6b6ecd979989"
            ],
            "layout": "IPY_MODEL_08c11f4ce77c46cfb945b72e401e09a2"
          }
        },
        "1a10505b49e9423482a240b7220edd4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_579789436aac4cad80ce263e4ab07c7c",
            "placeholder": "​",
            "style": "IPY_MODEL_a0562a20fc944ea6ad3576aea3a22fd8",
            "value": "  1%"
          }
        },
        "11f9aa4a8bae4af1bbdb88f0e8fc128f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89533421e32d4c5fa573a88afb419127",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_822763735f1f4fb8a8b0cac329d2a0a5",
            "value": 56
          }
        },
        "50ab53eb79af4e06a7ed6b6ecd979989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ad52e8660ac470d842dd0b575ca300a",
            "placeholder": "​",
            "style": "IPY_MODEL_d017fb1525d748b79a28f2dc547d2833",
            "value": " 56/10000 [01:22&lt;4:04:58,  1.48s/it]"
          }
        },
        "08c11f4ce77c46cfb945b72e401e09a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "579789436aac4cad80ce263e4ab07c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0562a20fc944ea6ad3576aea3a22fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89533421e32d4c5fa573a88afb419127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822763735f1f4fb8a8b0cac329d2a0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ad52e8660ac470d842dd0b575ca300a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d017fb1525d748b79a28f2dc547d2833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}