{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# musiGAN\n",
    "\n",
    "**Description:** 1-Track MuseGAN architecture build on MiniGAN.\\\n",
    "**Purpose:** implement a composing GAN.\\\n",
    "**Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "number_pitches = 4\n",
    "number_blips = 4\n",
    "pianoroll_size = number_pitches * number_blips \n",
    "print(pianoroll_size)\n",
    "\n",
    "# Architecture\n",
    "seed_length = 4\n",
    "\n",
    "# Storage\n",
    "training_filepath = \"../execution/trained_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimock = torch.tensor([[0, 0, 0, 0], \n",
    "                          [0, 1, 1, 0], \n",
    "                          [0, 1, 1, 0], \n",
    "                          [0, 0, 0, 0]], dtype = torch.float)\n",
    "\n",
    "plt.title(\"minimock\")\n",
    "plt.imshow(minimock);\n",
    "\n",
    "# make a dataset by stacking the same image a number of times\n",
    "number_images     = 100\n",
    "minimock_vector   = minimock.flatten()\n",
    "minimock_data     = minimock_vector[None, :].expand(number_images, \n",
    "                                                    *minimock_vector.size())\n",
    "minimock_dataset = torch.utils.data.TensorDataset(minimock_data)\n",
    "minimock_data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html#:~:text=A%20straight%2Dthrough%20estimator%20is,function%20was%20an%20identity%20function.\n",
    "\n",
    "class STEFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return (input > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return torch.nn.functional.hardtanh(grad_output)\n",
    "\n",
    "class StraightThroughEstimator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StraightThroughEstimator, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = STEFunction.apply(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### musiGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class miniGenerator (torch.nn.Module) :\n",
    "    \"\"\"\n",
    "    test GAN generator, MLP, one layer.\n",
    "\n",
    "    input : normally distributed random vector of length I, seed vector\n",
    "    output: binary vector of length O, pianoroll\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__ (self, log = False, **kwargs) : \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        I = 4   # length of input vector\n",
    "        O = 16    # length of output vector\n",
    "        \n",
    "        self.generator = torch.nn.Sequential(\n",
    "            torch.nn.Linear(I, I),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(I, O),\n",
    "            StraightThroughEstimator()\n",
    "        )\n",
    "\n",
    "        if log :\n",
    "            print(f\"Generator:\")\n",
    "            print(f\"    I: {I}, O: {O}\")\n",
    "            print(f\"    layers: 2\")\n",
    "            print(f\"    parameters: {self.count_params()}\")\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "    def count_params (self) :\n",
    "        \"\"\"count number of trainable parameters\"\"\"\n",
    "\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "    def forward (self, seed) :\n",
    "        pianoroll = self.generator(seed)\n",
    "\n",
    "        return pianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(torch.nn.Module):\n",
    "    \"\"\" 2d transconv layer, batch normalization & ReLU \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gen_block = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(in_dim, out_dim, kernel, stride),\n",
    "            torch.nn.BatchNorm2d(out_dim),\n",
    "            torch.nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator (torch.nn.Module) :\n",
    "    \"\"\"\n",
    "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
    "    temporal and bar generator)\n",
    "\n",
    "    input : seed vector, a normally distributed random vector, \n",
    "            length: (B + 1) * 64 = 5 * 64 here\n",
    "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 96 x 84) here\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self, log = False, **kwargs) : \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.temporal_generator = torch.nn.Sequential(\n",
    "            \n",
    "            # heuristically added linear layer\n",
    "            torch.nn.Linear(1, 31),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # transconv layer 1\n",
    "            torch.nn.ConvTranspose1d(64, 1024, 2, 2),\n",
    "            torch.nn.BatchNorm1d(1024),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # transconv layer 2\n",
    "            torch.nn.ConvTranspose1d(1024, 1, 3, 1),\n",
    "            torch.nn.BatchNorm1d(1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.bar_generator = torch.nn.Sequential(\n",
    "            \n",
    "            # transconv layers\n",
    "            GeneratorBlock( 128, 1024, (2, 1), (2, 1)),\n",
    "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
    "            GeneratorBlock( 512,  512, (2, 1), (2, 1)), # added heuristically\n",
    "            GeneratorBlock( 512,  256, (2, 1), (2, 1)),\n",
    "            GeneratorBlock( 256,  256, (2, 1), (2, 1)),\n",
    "            GeneratorBlock( 256,  128, (3, 1), (3, 1)),\n",
    "            GeneratorBlock( 128,   64, (1, 7), (1, 7)),\n",
    "\n",
    "            # last layer with tanh & binarization activation fct.s\n",
    "            torch.nn.ConvTranspose2d(64, 1, (1, 12), (1, 12)),\n",
    "            torch.nn.BatchNorm2d(1),\n",
    "            torch.nn.Tanh(),\n",
    "            StraightThroughEstimator() # binarization\n",
    "        )\n",
    "\n",
    "        if log :\n",
    "            print(f\"Generator: parameters: {self.count_params()}\")\n",
    "            print(\"\")\n",
    "\n",
    "    def count_params (self) :\n",
    "        \"\"\"count number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "    def forward (self, seed) :\n",
    "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
    "        \n",
    "        # create time-independent first half of seed for bar generator\n",
    "        bar_seed_1 = seeds[0]\n",
    "        bar_seed_1 = bar_seed_1.view((-1, 64, 1, 1)) # reshape for transconv layers\n",
    "\n",
    "        # generate pianorolls bar by bar\n",
    "        generated_bars = []\n",
    "        for temporal_seed in seeds[1:]:\n",
    "          \n",
    "          ## generate time-dependent second half of seed for bar generator\n",
    "          temporal_seed = temporal_seed.view(-1, 64, 1) # reshape for transconv layers\n",
    "          # print(f\"temporal seed: {temporal_seed.size()}\")\n",
    "          bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 64)\n",
    "          # print(f\"bar seed 2: {bar_seed_2.size()}\")\n",
    "\n",
    "          ## reshape & concatenate both halfs of seed for bar generator \n",
    "          bar_seed_2 = bar_seed_2.view(-1, 64, 1, 1)\n",
    "          bar_seed = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 128 x 1 x 1)\n",
    "          # print(f\"bar seed: {bar_seed.size()}\")\n",
    "\n",
    "          ## generate one bar \n",
    "          generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 96 x 84)\n",
    "          # print(f\"bar seed: {generated_bar.size()}\")\n",
    "          generated_bars.append(generated_bar) \n",
    "\n",
    "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 96 x 84) \n",
    "        # print(f\"output: {pianoroll.size()}\")\n",
    "\n",
    "        return pianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(log=True)\n",
    "gen_out = gen.forward(torch.rand(10, 5*64))\n",
    "pr = gen_out.cpu().detach().numpy()\n",
    "pr = pr.reshape((10, 4 * 96, 84))\n",
    "\n",
    "plt.imshow(pr[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class miniDiscriminator (torch.nn.Module) :\n",
    "    \"\"\"\n",
    "    first GAN discriminator, basically Generator in reverse but \n",
    "    \n",
    "    input : binary vector of length I, pianoroll\n",
    "    output: single number, prob. that the input is a real and not \n",
    "            generated pianoroll\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__ (self, log = False, **kwargs) :\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        I = 16   # length of input vector\n",
    "        O = 1   # length of output vector\n",
    "        \n",
    "        self.discriminator = torch.nn.Sequential(\n",
    "            torch.nn.Linear(I,4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4, O),\n",
    "            #torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        if log :\n",
    "            print(f\"Discriminator:\")\n",
    "            print(f\"    I: {I}, O: {O}\")\n",
    "            print(f\"    layers: 2\")\n",
    "            print(f\"    parameters: {self.count_params()}\")\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "    def count_params (self) :\n",
    "        \"\"\"count number of trainable parameters\"\"\"\n",
    "\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "    def forward (self, pianoroll):\n",
    "        judgement = self.discriminator(pianoroll)\n",
    "        return judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(torch.nn.Module):\n",
    "    \"\"\"3d conv layer & Leaky ReLU\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
    "        super().__init__()\n",
    "        self.dis_block = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(in_dim, out_dim, kernel, stride),\n",
    "            torch.nn.LeakyReLU(negative_slope = 0.2) # MuseGAN Hyperparameter\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dis_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator (torch.nn.Module) :\n",
    "    \"\"\"\n",
    "    1-Track museGAN discriminator\n",
    "    \n",
    "    input : (B x T x P) binary pianoroll\n",
    "    output: single number, prob. that the input pianoroll is a \n",
    "            real and not generated\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__ (self, log = False, **kwargs) :\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # conv layers\n",
    "        self.discriminator_conv = torch.nn.Sequential(\n",
    "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
    "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
    "            DiscriminatorBlock(128, 128, (1, 1, 12), (1, 1, 12)), \n",
    "            DiscriminatorBlock(128, 128, (1, 1,  7), (1, 1,  7)),\n",
    "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
    "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
    "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
    "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1)))\n",
    "        \n",
    "        # fully-connected layers\n",
    "        self.discriminator_fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 5, 1024),\n",
    "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
    "            torch.nn.Linear(1024, 1))\n",
    "        \n",
    "        if log :\n",
    "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
    "            print(\"\")\n",
    "\n",
    "    def count_params (self) :\n",
    "        \"\"\"count number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward (self, pianoroll):\n",
    "        pianoroll = pianoroll.view(10, 1, 4, 96, 84) # reshape for transconvs\n",
    "        conv_output = self.discriminator_conv(pianoroll)\n",
    "        fc_input = conv_output.view(-1, 5*512)\n",
    "        judgement = self.discriminator_fc(fc_input)\n",
    "        # print(judgement.size())\n",
    "        return judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = Discriminator(log=True)\n",
    "\n",
    "dis_out = dis.forward(gen_out)\n",
    "judge = torch.sigmoid(dis_out).cpu().detach().numpy().flatten()\n",
    "\n",
    "print(f\"Prob. that image is real: {judge[0]*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = Discriminator(log=True)\n",
    "\n",
    "dis_out = dis.forward(gen_out)\n",
    "judge = torch.sigmoid(dis_out).cpu().detach().numpy().flatten()\n",
    "\n",
    "print(f\"Prob. that image is real: {judge[0]*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test(GEN, DIS, data): \n",
    "    \n",
    "    # Test on generated data\n",
    "    \n",
    "    ## Generator\n",
    "    test_seeds = torch.normal(0, 1, (5, seed_length))\n",
    "    examples = GEN.forward(test_seeds)\n",
    "    gen_img = examples[0].reshape(number_pitches, number_blips)\n",
    "    gen_img = gen_img.detach().numpy()\n",
    "\n",
    "    ## Discriminator\n",
    "    gen_judgements = DIS.forward(examples)\n",
    "    gen_judgement = torch.sigmoid(gen_judgements[0][0]).detach().numpy()\n",
    "\n",
    "    # Results\n",
    "    plt.title(\"Generator output x\")\n",
    "    plt.imshow(gen_img);\n",
    "    print(f\"Discriminator p(x_gen = real): {gen_judgement*100:.0f}%\")\n",
    "\n",
    "    # Test on real data\n",
    "    real_judgement = torch.sigmoid(DIS.forward(data))[0][0].detach().numpy()\n",
    "    print(f\"Discriminator p(x_real = real) = {real_judgement*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_goodness(generated_batch, real_batch):\n",
    "    \"\"\" compare two batches of data by calculating the absolute mean difference\"\"\"\n",
    "    \n",
    "    # averaged over batches \n",
    "    generated_mean = torch.mean(generated_batch)\n",
    "    real_mean = torch.mean(real_batch)\n",
    "\n",
    "    # take differnece & absolut value, average over features lastly\n",
    "    goodness_criteria = torch.mean(torch.abs(real_mean - generated_mean))\n",
    "\n",
    "    return(goodness_criteria.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training :\n",
    "    def __init__ (self, real_pianorolls, batch_size) :\n",
    "        self.device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
    "\n",
    "        # Storing external parameters\n",
    "        self.batch_size  = batch_size\n",
    "        self.seed_length = seed_length\n",
    "        self.pianoroll_size = pianoroll_size\n",
    "        \n",
    "        # Preparing training data\n",
    "        self.training_set = real_pianorolls\n",
    "        self.data_loader  = torch.utils.data.DataLoader(self.training_set,\n",
    "                                batch_size = self.batch_size, \n",
    "                                drop_last = True,\n",
    "                                shuffle = True)\n",
    "\n",
    "        # Initializing GAN\n",
    "        self.gen = Generator().to(self.device)\n",
    "        self.dis = Discriminator().to(self.device)\n",
    "        self.optimizer_gen = torch.optim.Adam(self.gen.parameters(), \n",
    "                                              lr = 0.001,\n",
    "                                              betas = (0.5, 0.9)) \n",
    "            # ADAM parameters from GAN tutorial\n",
    "        self.optimizer_dis = torch.optim.Adam(self.dis.parameters(), \n",
    "                                              lr = 0.001,\n",
    "                                              betas = (0.5, 0.9))\n",
    "\n",
    "\n",
    "\n",
    "    def train (self, epochs, k = 1, loss_func = \"WGAN\") :\n",
    "        # Training parameters\n",
    "        self.k = k\n",
    "        self.loss_function = loss_func\n",
    "\n",
    "        assert  type(epochs) == int\n",
    "        assert  epochs >= 1\n",
    "        assert  type(k) == int\n",
    "        assert  k >= 1\n",
    "        assert  self.loss_function in [\"GAN\", \"WGAN\"]\n",
    "        \n",
    "        # Logging\n",
    "        self.losses       = np.zeros((5, epochs))\n",
    "        self.probs        = np.zeros((2, epochs))\n",
    "        self.gen_goodness = np.zeros(epochs)\n",
    "        \n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(epochs) :\n",
    "            loss         = []\n",
    "            probs        = []\n",
    "            gen_goodness = []\n",
    "\n",
    "            for train_batch in self.data_loader :\n",
    "                batch_losses, batch_probs, batch_gen_goodness = \\\n",
    "                    self.training_step(train_batch[0].to(self.device))\n",
    "                loss.append(batch_losses)\n",
    "                probs.append(batch_probs)\n",
    "                gen_goodness.append(batch_gen_goodness)\n",
    "                print(\"#\", end = \"\")\n",
    "            \n",
    "            # Loging\n",
    "            self.losses[:, epoch]    = np.array(loss).mean(axis=0)\n",
    "            self.probs[:, epoch]     = np.array(probs).mean(axis=0)\n",
    "            self.gen_goodness[epoch] = np.array(gen_goodness).mean(axis=0)\n",
    "\n",
    "            print(\" epoch\", epoch, \"complete.\")\n",
    "\n",
    "        # Returning trained GAN\n",
    "        return self.gen, self.dis\n",
    "\n",
    "  \n",
    "\n",
    "    def training_step (self, batch_real) :\n",
    "        dis_losses = torch.zeros(size = (self.k,)).to(self.device)  # logging\n",
    "        for i in range(self.k):\n",
    "            # Forward propagation\n",
    "            seed_vector = torch.normal(0, 1, size = \n",
    "                                       (self.batch_size, self.seed_length)) \\\n",
    "                          .to(self.device)\n",
    "            batch_gen = self.gen.forward(seed_vector)\n",
    "            \n",
    "            judgement_real = self.dis.forward(batch_real)\n",
    "            judgement_gen  = self.dis.forward(batch_gen)\n",
    "\n",
    "\n",
    "            # Calculating the Discriminator loss function\n",
    "            if self.loss_function == \"GAN\" :\n",
    "                prob_real = torch.sigmoid(judgement_real)\n",
    "                prob_gen  = torch.sigmoid(judgement_gen)\n",
    "                loss_term_real = - torch.mean(torch.log(prob_real))\n",
    "                loss_term_gen  = torch.mean(torch.log(1 - prob_gen))\n",
    "                reg_term       = torch.tensor(0.)\n",
    "            elif self.loss_function == \"WGAN\" :\n",
    "                loss_term_real = - torch.mean(judgement_real)\n",
    "                loss_term_gen  = torch.mean(judgement_gen)\n",
    "                var_g    = torch.var(judgement_gen)\n",
    "                var_r    = torch.var(judgement_real)\n",
    "                reg_term = torch.where(var_g > 1, (var_g - 1)**2, 0) \\\n",
    "                           + torch.where(var_r > 1, (var_r - 1)**2, 0)\n",
    "            else :\n",
    "                print(f\"Warning: '{self.loss_function}' as loss function isn't\\\n",
    "                        an option!\")\n",
    "\n",
    "            loss_dis = loss_term_real + loss_term_gen + reg_term\n",
    "            \n",
    "            # Discriminator update\n",
    "            self.optimizer_dis.zero_grad()\n",
    "            loss_dis.backward(retain_graph = True)\n",
    "            self.optimizer_dis.step()\n",
    "            \n",
    "            dis_losses[i] = loss_dis   # logging\n",
    "\n",
    "        loss_dis = torch.mean(dis_losses)  # logging\n",
    "\n",
    "\n",
    "        # Calculating the Generator loss function\n",
    "        judgement_new = self.dis.forward(batch_gen)\n",
    "        \n",
    "        if self.loss_function == \"GAN\" :\n",
    "            prob_new = torch.sigmoid(judgement_new)\n",
    "            loss_gen = -torch.mean(torch.log(prob_new)) \n",
    "        elif self.loss_function == \"WGAN\" :\n",
    "            loss_gen = -torch.mean(judgement_new)\n",
    "        \n",
    "        # Generator update\n",
    "        self.optimizer_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        self.optimizer_gen.step()\n",
    "\n",
    "        # Output Losses\n",
    "        losses_out = [loss_dis.detach().numpy(), \n",
    "                      loss_term_real.detach().numpy(), \n",
    "                      loss_term_gen.detach().numpy(), \n",
    "                      reg_term.detach().numpy(), \n",
    "                      loss_gen.detach().numpy()]\n",
    "\n",
    "        # Output averaged judgement probabilities \n",
    "        D_real = torch.mean(torch.sigmoid(judgement_real))\n",
    "        D_gen  = torch.mean(torch.sigmoid(judgement_gen))\n",
    "        probs_out = [D_real.detach().numpy(), D_gen.detach().numpy()]\n",
    "\n",
    "        # Output generator goodness (already averaged over batch)\n",
    "        gen_goodness = generator_goodness(batch_gen, batch_real)\n",
    "\n",
    "        # Quick Test for debugging \n",
    "        # quick_test(self.gen, self.dis) \n",
    "\n",
    "        return losses_out, probs_out, gen_goodness\n",
    "\n",
    "\n",
    "\n",
    "    def save (self, name) :\n",
    "        file_name = training_filepath + name + \".obj\"\n",
    "        file      = open(file_name, \"wb\")\n",
    "        pickle.dump(self, file)\n",
    "        print(f\"Saved training under '{file_name}'\")\n",
    "\n",
    "\n",
    "    def load (name) :\n",
    "        file_name = training_filepath + name + \".obj\"\n",
    "        file      = open(file_name, \"rb\")\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANTrainer  = Training(minimock_dataset, batch_size = 33)\n",
    "trained_GAN = GANTrainer.train(epochs = 150, k = 4, loss_func=\"WGAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gen, Dis = trained_GAN\n",
    "quick_test(Gen, Dis, data = minimock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GANTrainer.save(\"WGAN-minimock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk & quick tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
