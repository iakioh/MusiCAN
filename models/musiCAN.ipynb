{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iakioh/MusiCAN/blob/main/models/musiCAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ATBcagDnDpM"
      },
      "source": [
        "### Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mjvf82O3S2Yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374b5101-f39b-43ab-a9b1-071be57ab0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LCaQQJb6TXgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da218a7-9ef3-461b-ebe0-6bb8996339ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MusiCAN/models\n"
          ]
        }
      ],
      "source": [
        "# Go to this notebook's directory\n",
        "repo_path = \"/content/drive/MyDrive/MusiCAN/\"\n",
        "%cd {repo_path}/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wvGZ0ptpZQNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f05befd-e721-4650-8242-0adcd1472613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 20 10:57:22 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU connection\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O7_ny_A1Z6L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25f8a3a-2f10-4d44-92fa-2fe65f3a2f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "# Check RAM access\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nnLdyhmuzL2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8993d8e2-e8e4-4592-9508-ae7b7a2e9b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting muspy\n",
            "  Downloading muspy-0.5.0-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.7/dist-packages (from muspy) (1.1.0)\n",
            "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.7/dist-packages (from muspy) (3.2.2)\n",
            "Collecting bidict>=0.21\n",
            "  Downloading bidict-0.22.0-py3-none-any.whl (36 kB)\n",
            "Collecting pretty-midi>=0.2\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 20.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (4.64.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (6.0)\n",
            "Collecting music21>=6.0\n",
            "  Downloading music21-7.3.3-py3-none-any.whl (22.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.4 MB 62.0 MB/s \n",
            "\u001b[?25hCollecting miditoolkit>=0.1\n",
            "  Downloading miditoolkit-0.1.16-py3-none-any.whl (20 kB)\n",
            "Collecting pypianoroll>=1.0\n",
            "  Downloading pypianoroll-1.0.4-py3-none-any.whl (26 kB)\n",
            "Collecting mido>=1.0\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from muspy) (2.23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->muspy) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5->muspy) (4.1.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from music21>=6.0->muspy) (3.0.4)\n",
            "Collecting jsonpickle\n",
            "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from music21>=6.0->muspy) (8.14.0)\n",
            "Collecting webcolors>=1.5\n",
            "  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty-midi>=0.2->muspy) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pypianoroll>=1.0->muspy) (1.7.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->muspy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->muspy) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->muspy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonpickle->music21>=6.0->muspy) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle->music21>=6.0->muspy) (3.8.1)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591955 sha256=86f3e13cec99a21ab818f043ff530b2b48ad7836136faced05ffbe7eda2c2676\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, webcolors, pretty-midi, jsonpickle, pypianoroll, music21, miditoolkit, bidict, muspy\n",
            "  Attempting uninstall: music21\n",
            "    Found existing installation: music21 5.5.0\n",
            "    Uninstalling music21-5.5.0:\n",
            "      Successfully uninstalled music21-5.5.0\n",
            "Successfully installed bidict-0.22.0 jsonpickle-2.2.0 miditoolkit-0.1.16 mido-1.2.10 music21-7.3.3 muspy-0.5.0 pretty-midi-0.2.9 pypianoroll-1.0.4 webcolors-1.12\n",
            "Start downloading MuseScore General soundfont.\n",
            "MuseScore General soundfont has successfully been downloaded to : /root/.muspy/musescore-general.\n",
            "Start downloading Bravura font.\n",
            "Bravura font has successfully been downloaded to : /root/.muspy/musescore-general.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fluid-soundfont-gm libfluidsynth1 libqt5x11extras5 qsynth\n",
            "Suggested packages:\n",
            "  fluid-soundfont-gs timidity jackd\n",
            "The following NEW packages will be installed:\n",
            "  fluid-soundfont-gm fluidsynth libfluidsynth1 libqt5x11extras5 qsynth\n",
            "0 upgraded, 5 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 120 MB of archives.\n",
            "After this operation, 150 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fluid-soundfont-gm all 3.1-5.1 [119 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libfluidsynth1 amd64 1.1.9-1 [137 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fluidsynth amd64 1.1.9-1 [20.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libqt5x11extras5 amd64 5.9.5-0ubuntu1 [8,596 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 qsynth amd64 0.5.0-2 [191 kB]\n",
            "Fetched 120 MB in 4s (30.6 MB/s)\n",
            "Selecting previously unselected package fluid-soundfont-gm.\n",
            "(Reading database ... 155569 files and directories currently installed.)\n",
            "Preparing to unpack .../fluid-soundfont-gm_3.1-5.1_all.deb ...\n",
            "Unpacking fluid-soundfont-gm (3.1-5.1) ...\n",
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Selecting previously unselected package fluidsynth.\n",
            "Preparing to unpack .../fluidsynth_1.1.9-1_amd64.deb ...\n",
            "Unpacking fluidsynth (1.1.9-1) ...\n",
            "Selecting previously unselected package libqt5x11extras5:amd64.\n",
            "Preparing to unpack .../libqt5x11extras5_5.9.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libqt5x11extras5:amd64 (5.9.5-0ubuntu1) ...\n",
            "Selecting previously unselected package qsynth.\n",
            "Preparing to unpack .../qsynth_0.5.0-2_amd64.deb ...\n",
            "Unpacking qsynth (0.5.0-2) ...\n",
            "Setting up libqt5x11extras5:amd64 (5.9.5-0ubuntu1) ...\n",
            "Setting up fluid-soundfont-gm (3.1-5.1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Setting up fluidsynth (1.1.9-1) ...\n",
            "Setting up qsynth (0.5.0-2) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyfluidsynth\n",
            "  Downloading pyFluidSynth-1.3.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyfluidsynth) (1.21.6)\n",
            "Installing collected packages: pyfluidsynth\n",
            "Successfully installed pyfluidsynth-1.3.1\n"
          ]
        }
      ],
      "source": [
        "# Install muspy related code\n",
        "!pip install muspy\n",
        "import muspy\n",
        "muspy.download_musescore_soundfont() \n",
        "muspy.download_bravura_font() \n",
        "\n",
        "# Install fluidsynth related code\n",
        "!apt install fluidsynth\n",
        "!pip install pyfluidsynth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgRRhU6SSfmz"
      },
      "source": [
        "# musiGAN\n",
        "\n",
        "**Description:** 1-Track MuseGAN architecture build on MiniGAN.\\\n",
        "**Purpose:** implement a composing GAN.\\\n",
        "**Results:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3Zy36G-CGamF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from tqdm import notebook\n",
        "from datetime import datetime\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('tableau-colorblind10')\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import ndimage\n",
        "\n",
        "import muspy\n",
        "import fluidsynth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trwcbyaoSfm2"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ePkslq2wSfm4"
      },
      "outputs": [],
      "source": [
        "class Pianoroll :\n",
        "    def __init__ (self, filepath, bars, lowest_pitch, genre_list) :\n",
        "        assert  type(filepath) == str\n",
        "\n",
        "        # Creating the dataset from a file\n",
        "        stored_data  = np.load(filepath)\n",
        "        data_array   = stored_data[\"data\"]\n",
        "        labels_array = stored_data[\"labels\"]\n",
        "\n",
        "        ## Split dataset into training and test parts\n",
        "        np.random.seed(seed=902741)\n",
        "        perm         = np.random.permutation(len(labels_array))\n",
        "        data_array   = data_array[perm]\n",
        "        labels_array = labels_array[perm]\n",
        "        split_line   = len(labels_array) // 11\n",
        "        data_test, data     = np.array_split(data_array, [split_line], axis = 0)\n",
        "        labels_test, labels = np.array_split(labels_array, [split_line], axis = 0)\n",
        "\n",
        "        # Making a torch dataset\n",
        "        self.data   = torch.as_tensor(data, dtype = torch.float32)\n",
        "        self.labels = torch.as_tensor(labels, dtype = torch.int64)\n",
        "        self.data_test   = torch.as_tensor(data_test, dtype = torch.float32)\n",
        "        self.labels_test = torch.as_tensor(labels_test, dtype = torch.int64)\n",
        "\n",
        "        self.dataset      = torch.utils.data.TensorDataset(self.data, \n",
        "                                                           self.labels)\n",
        "        self.dataset_test = torch.utils.data.TensorDataset(self.data_test, \n",
        "                                                           self.labels_test)\n",
        "\n",
        "        # Storing additional info about it\n",
        "        self.shape  = tuple(self.data.shape[1:])   # shape of one pianoroll image\n",
        "        self.size   = self.shape[0] * self.shape[1]\n",
        "        self.height       = self.data.shape[2]\n",
        "        self.width        = self.data.shape[1]\n",
        "        self.dataset_size = self.data.shape[0]\n",
        "        self.test_size    = self.data_test.shape[0]\n",
        "\n",
        "        self.bars         = bars\n",
        "        self.lowest_pitch = lowest_pitch\n",
        "        self.genre_list   = genre_list\n",
        "\n",
        "        self.blips_per_bar  = self.width // self.bars\n",
        "        self.blips_per_beat = self.blips_per_bar // 4\n",
        "        self.pitches        = self.height\n",
        "        self.octaves        = self.pitches // 12\n",
        "        self.n_labels       = len(self.genre_list)\n",
        "\n",
        "    \n",
        "    def show (self, number = None) :\n",
        "        if number == None :\n",
        "            number = np.random.randint(self.dataset_size)\n",
        "        else :\n",
        "            assert  type(number) == int\n",
        "            assert  number >= 0 and number < self.dataset_size\n",
        "\n",
        "        plt.figure(figsize = (12, 6))\n",
        "        plt.title(f\"pianoroll #{number}\")\n",
        "        plt.imshow(self.data[number].T)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5pWodkypE8U"
      },
      "source": [
        "### LPD5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fH4y0TdiRjuD"
      },
      "outputs": [],
      "source": [
        "default_training_path = \"../experiments\"\n",
        "\n",
        "#default_dataset       = \"lpd5_full_4bars\"\n",
        "default_dataset       = \"datacombi_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KCEcB4f4pE8V"
      },
      "outputs": [],
      "source": [
        "lpd5_path = \"../experiments/lpd5_full_4bars/prepared_arrays.npz\"\n",
        "lpd5_bars = 4\n",
        "lpd5_lowest_pitch = 24\n",
        "lpd5_genre_list = ['Rap', 'Latin', 'International', 'Electronic', \n",
        "                   'Country', 'Folk', 'Blues', 'Reggae', 'Jazz',\n",
        "                   'Vocal', 'New-Age', 'RnB', 'Pop_Rock']\n",
        "#lpd5 = Pianoroll(lpd5_path, lpd5_bars, lpd5_lowest_pitch, lpd5_genre_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t-k4JCCXOIXS"
      },
      "outputs": [],
      "source": [
        "dc1_path = \"../experiments/datacombi_1/prepared_arrays.npz\"\n",
        "dc1_bars = 4 # actually 12 but 4 for gen and dis compatibiliy reasons.\n",
        "dc1_lowest_pitch = 24\n",
        "dc1_genre_list   = ['Latin', 'Electronic', 'Country', 'RnB', 'Pop_Rock', 'Classical', 'Game']\n",
        "\n",
        "lpd5 = Pianoroll(dc1_path, dc1_bars, dc1_lowest_pitch, dc1_genre_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpd5.show()"
      ],
      "metadata": {
        "id": "Eba7NctgrmY8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "9ea7a170-3c16-4888-804a-d005544d2b6b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 703,
              "height": 297
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpd5.dataset_size\", lpd5.dataset_size)\n",
        "print(\"lpd5.test_size\", lpd5.test_size)\n",
        "print(\"lpd5.shape\", lpd5.shape)"
      ],
      "metadata": {
        "id": "UvXOK3nnrnkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7f9741-a26b-46eb-8ee5-29a896c378b6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lpd5.dataset_size 35170\n",
            "lpd5.test_size 3517\n",
            "lpd5.shape (192, 72)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpd5.bars\", lpd5.bars)\n",
        "print(\"lpd5.blips_per_bar\", lpd5.blips_per_bar)\n",
        "print(\"lpd5.pitches\", lpd5.pitches)\n",
        "print(\"lpd5.octaves\", lpd5.octaves)"
      ],
      "metadata": {
        "id": "p3OZmtvgrqod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a855e7a-f8f0-4b25-a0c8-5a9516d6d8fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lpd5.bars 4\n",
            "lpd5.blips_per_bar 48\n",
            "lpd5.pitches 72\n",
            "lpd5.octaves 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpd5.n_labels\", lpd5.n_labels)\n",
        "print(\"lpd5.genre_list\", lpd5.genre_list)"
      ],
      "metadata": {
        "id": "Ruxd7HGKrsdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948f31ec-a31a-4a17-86f3-e1b07a0281c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lpd5.n_labels 7\n",
            "lpd5.genre_list ['Latin', 'Electronic', 'Country', 'RnB', 'Pop_Rock', 'Classical', 'Game']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15, 5))\n",
        "plt.suptitle(\"Relative genre frequencies in the dataset\")\n",
        "x_pos = np.arange(lpd5.n_labels)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Training data\")\n",
        "label_freqs = np.array([np.mean(lpd5.labels.numpy() == i) \n",
        "                        for i in range(lpd5.n_labels)])\n",
        "plt.bar(x_pos, label_freqs, tick_label = lpd5.genre_list)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test data\")\n",
        "label_freqs_test = np.array([np.mean(lpd5.labels_test.numpy() == i) \n",
        "                             for i in range(lpd5.n_labels)])\n",
        "plt.bar(np.arange(lpd5.n_labels), label_freqs_test, tick_label = lpd5.genre_list)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PySc5NmZrwvu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "041c7948-25d2-466d-df18-ca4f27fc4aab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 880,
              "height": 339
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel_diff = label_freqs / label_freqs_test - 1\n",
        "print(\"Relative difference in genre frequencies of test compared to training data:\")\n",
        "for diff, genre in zip(rel_diff, lpd5.genre_list) :\n",
        "    print(f\"{genre:10} : {diff*100:4.1f}%\")"
      ],
      "metadata": {
        "id": "7TfnAP0Zr12U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c407af-d858-43b6-85e6-c74c4f36f945"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relative difference in genre frequencies of test compared to training data:\n",
            "Latin      :  6.9%\n",
            "Electronic : 11.1%\n",
            "Country    : 10.9%\n",
            "RnB        : -3.8%\n",
            "Pop_Rock   :  1.4%\n",
            "Classical  : -4.4%\n",
            "Game       : -2.4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tnu9HElSfm5"
      },
      "source": [
        "## Architecture classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdGO-KlvSfm5"
      },
      "source": [
        "### Support classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "acpAO9NSSfm5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    These two classes serves as torch layers to binarize the output of the Generator while keeping the layer still \"backpropagatable\" (via a hardtanh).\n",
        "    This is not our own code. For source, see:\n",
        "    https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html#:~:text=A%20straight%2Dthrough%20estimator%20is,function%20was%20an%20identity%20function.\n",
        "\"\"\"\n",
        "\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0.5).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return torch.nn.functional.hardtanh(grad_output)\n",
        "\n",
        "class StraightThroughEstimator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StraightThroughEstimator, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # only binarize in eval() mode, not in training\n",
        "        x = x  if self.training  else  STEFunction.apply(x)\n",
        "        #x = STEFunction.apply(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-_Tgofn5pfRU"
      },
      "outputs": [],
      "source": [
        "class GeneratorBlock(torch.nn.Module):\n",
        "    \"\"\" 2d transconv layer, batch normalization & ReLU \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gen_block = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(in_dim, out_dim, kernel, stride),\n",
        "            torch.nn.BatchNorm2d(out_dim),\n",
        "            torch.nn.ReLU()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen_block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n38oto6cpfRW"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorBlock(torch.nn.Module):\n",
        "    \"\"\"3d conv layer & Leaky ReLU\"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
        "        super().__init__()\n",
        "        self.dis_block = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_dim, out_dim, kernel, stride),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2)   # MuseGAN Hyperparameter\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dis_block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU1bKYfdSfm6"
      },
      "source": [
        "### Main neural network classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "swqlncpkJKeh"
      },
      "outputs": [],
      "source": [
        "class MusiGen (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 64\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(64, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 3, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 128, 1024, (2, 1), (2, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 256,  256, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 256,  128, (3, 1), (3, 1)),\n",
        "            GeneratorBlock( 128,   64, (1, self.octaves), (1, self.octaves)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (1, 12), (1, 12)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "        \n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward_custom (self, seed) :\n",
        "        assert  type(seed) == torch.Tensor\n",
        "        assert  len(seed.shape) == 2\n",
        "        assert  seed.shape[0] >= 1\n",
        "        assert  seed.shape[1] == (1 + self.bars) * self.seedlength\n",
        "\n",
        "        batchsize = seed.shape[0]\n",
        "        return self.forward(batchsize, seed)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        bar_seed_1 = seeds[0]\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "\n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            #print(f\"temporal seed: {temporal_seed.size()}\")\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 64)\n",
        "            #print(f\"bar seed 2: {bar_seed_2.size()}\")\n",
        "\n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "            \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 128 x 1 x 1)\n",
        "            #print(f\"bar seed: {bar_seed d.size()}\")\n",
        "\n",
        "            ## generate one bar \n",
        "            \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 24 x 84)\n",
        "            #print(f\"generated_bar: {generated_bar.size()}\")\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 24 x 84) \n",
        "        #print(f\"gen output: {pianoroll.size()}\")\n",
        "\n",
        "        return pianoroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rOkq0b4VpfRV"
      },
      "outputs": [],
      "source": [
        "class MusiGenMod1 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 64\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(64, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 3, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 128, 1024, (3, 1), (3, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (1, self.octaves), (1, self.octaves)),\n",
        "            GeneratorBlock( 256,  256, (1, 12), (1, 12)),\n",
        "            GeneratorBlock( 256,  128, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 128,   64, (2, 1), (2, 1)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (2, 1), (2, 1)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward_custom (self, seed) :\n",
        "        assert  type(seed) == torch.Tensor\n",
        "        assert  len(seed.shape) == 2\n",
        "        assert  seed.shape[0] >= 1\n",
        "        assert  seed.shape[1] == (1 + self.bars) * self.seedlength\n",
        "\n",
        "        batchsize = seed.shape[0]\n",
        "        return self.forward(batchsize, seed)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        bar_seed_1 = seeds[0]\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "\n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            #print(f\"temporal seed: {temporal_seed.size()}\")\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 64)\n",
        "            #print(f\"bar seed 2: {bar_seed_2.size()}\")\n",
        "\n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "            \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 128 x 1 x 1)\n",
        "            #print(f\"bar seed: {bar_seed d.size()}\")\n",
        "\n",
        "            ## generate one bar \n",
        "            \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 24 x 84)\n",
        "            #print(f\"generated_bar: {generated_bar.size()}\")\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 24 x 84) \n",
        "        #print(f\"gen output: {pianoroll.size()}\")\n",
        "\n",
        "        return pianoroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7K4uNT3CJbDo"
      },
      "outputs": [],
      "source": [
        "class MusiGenMod2 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 128\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(self.seedlength),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(self.seedlength, 512, 2, 2),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(512, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 3\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 5, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 2*self.seedlength, 1024, (2, 1), (2, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (1, self.octaves), (1, self.octaves)),\n",
        "            GeneratorBlock( 256,  256, (1, 4), (1, 4)),\n",
        "            GeneratorBlock( 256,  256, (1, 3), (1, 3)),\n",
        "            GeneratorBlock( 256,  128, (3, 1), (3, 1)),\n",
        "            GeneratorBlock( 128,   64, (2, 1), (2, 1)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (2, 1), (2, 1)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        bar_seed_1 = seeds[0]\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "\n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            #print(f\"temporal seed: {temporal_seed.size()}\", temporal_seed.device)\n",
        "            #print(f\"temporal generator: {self.temporal_generator[0].device}\")\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 128)\n",
        "            #print(f\"bar seed 2: {bar_seed_2.size()}\")\n",
        "\n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "        \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 256 x 1 x 1)\n",
        "            #print(f\"bar seed: {bar_seed.size()}\")\n",
        "\n",
        "            ## generate one bar \n",
        "            \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 48 x 84)\n",
        "            #print(f\"generated_bar: {generated_bar.size()}\")\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 48 x 84) \n",
        "        #print(f\"gen output: {pianoroll.size()}\")\n",
        "\n",
        "        return pianoroll"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiGenMod3 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 128\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(self.seedlength),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(self.seedlength, 512, 2, 2),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(512, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 3\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 5, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 2*self.seedlength, 1024, (2, 1), (2, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (1, self.octaves), (1, self.octaves)),\n",
        "            GeneratorBlock( 256,  256, (1, 4), (1, 4)),\n",
        "            GeneratorBlock( 256,  256, (1, 3), (1, 3)),\n",
        "            GeneratorBlock( 256,  128, (3, 1), (3, 1)),\n",
        "            GeneratorBlock( 128,   64, (2, 1), (2, 1)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (2, 1), (2, 1)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        track_seed = seeds[0].view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "        bar_seed_1 = self.temporal_generator(track_seed) # (batch size x 1 x 128)\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "        \n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 128)\n",
        "            \n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 256 x 1 x 1)\n",
        "            \n",
        "            ## generate one bar \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 48 x 84)\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 48 x 84) \n",
        "        \n",
        "        return pianoroll"
      ],
      "metadata": {
        "id": "I3O6lUnvtSqO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Aax8PSv8Rm2_"
      },
      "outputs": [],
      "source": [
        "class MusiDis (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 12), (1, 1, 12)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 1))\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512*2, 512),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512, self.n_labels))\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "I5vGYaCfpfRW"
      },
      "outputs": [],
      "source": [
        "class MusiDisMod1 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 12), (1, 1, 12)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            #torch.nn.Linear(1024, 1))\n",
        "            # added\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1))\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512*2, 512),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512, self.n_labels))\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jB4I8sulRypo"
      },
      "outputs": [],
      "source": [
        "class MusiDisMod2 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 3), (1, 1, 3)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1, 4), (1, 1, 4)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            #torch.nn.Linear(1024, 1)\n",
        "            # added\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1)\n",
        "        )\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 512),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512, self.n_labels)\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDisMod2b (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 3), (1, 1, 3)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1, 4), (1, 1, 4)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1)\n",
        "        )\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 64),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(64, self.n_labels)\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "L_Lj8nAKHq4H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDisMod3 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 3), (1, 1, 3)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1, 4), (1, 1, 4)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1)\n",
        "        )\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 32),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(32, self.n_labels)\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "a3Y1F50Q4aZk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swF_vr8kSfm9"
      },
      "source": [
        "## Training & evaluation classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8eX-HGSfm-"
      },
      "source": [
        "### Training support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_lrgdWMSrIk"
      },
      "source": [
        "#### Training metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "D0p-ZDYGSfm-"
      },
      "outputs": [],
      "source": [
        "def abs_mean_diff (generated_batch, real_batch) :\n",
        "    \"\"\"\n",
        "        compare two batches of data by calculating the absolute mean difference\n",
        "    \"\"\"\n",
        "    \n",
        "    # equalize shapes\n",
        "    real_shape = real_batch.shape[-2:]\n",
        "    real_batch = real_batch.view(-1, *real_shape)\n",
        "    generated_batch = generated_batch.view(-1, *real_shape)\n",
        "    assert  generated_batch.shape == real_batch.shape\n",
        "\n",
        "    # averaged over batches \n",
        "    generated_mean = torch.mean(generated_batch, dim = 0)\n",
        "    real_mean      = torch.mean(real_batch, dim = 0)\n",
        "\n",
        "    # take differnece & absolut value, average over features lastly\n",
        "    absolute_mean_difference = torch.mean(torch.abs(real_mean - generated_mean))\n",
        "\n",
        "    return absolute_mean_difference.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1uWF5EdvzU8Y"
      },
      "outputs": [],
      "source": [
        "def abs_std_diff (generated_batch, real_batch) :\n",
        "    \"\"\"\n",
        "        compare two batches of data by calculating the absolute standard deviation difference\n",
        "    \"\"\"\n",
        "    \n",
        "    # equalize shapes\n",
        "    real_shape = real_batch.shape[-2:]\n",
        "    real_batch = real_batch.view(-1, *real_shape)\n",
        "    generated_batch = generated_batch.view(-1, *real_shape)\n",
        "    assert  generated_batch.shape == real_batch.shape\n",
        "\n",
        "    # averaged over batches \n",
        "    generated_std = torch.std(generated_batch, dim = 0, unbiased = True)\n",
        "    real_std      = torch.std(real_batch, dim = 0, unbiased = True)\n",
        "    \n",
        "    # take differnece & absolut value, average over features lastly\n",
        "    absolute_std_difference = torch.mean(torch.abs(real_std - generated_std))\n",
        "\n",
        "    return absolute_std_difference.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cvYGRG9z2dR-"
      },
      "outputs": [],
      "source": [
        "def inter_bar_var (generated_batch) :\n",
        "    \"\"\"\n",
        "        computes the inter-bar standard deviation\n",
        "    \"\"\"\n",
        "\n",
        "    inter_bar_std_dev = torch.mean(torch.std(generated_batch, dim = 1, \n",
        "                                             unbiased = True)) # std over bars\n",
        "    \n",
        "    return inter_bar_std_dev.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3NjqpcV34U3t"
      },
      "outputs": [],
      "source": [
        "def inter_track_var (generated_batch) :\n",
        "    \"\"\"\n",
        "        computes the inter-track standard deviation\n",
        "    \"\"\"\n",
        "\n",
        "    inter_track_std_dev = torch.mean(torch.std(generated_batch, dim = 0, \n",
        "                                               unbiased = True)) # std over tracks\n",
        "    \n",
        "    return inter_track_std_dev.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS0kN-LMSzV0"
      },
      "source": [
        "#### Loss function support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eYO2ATSm3oRb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def unif_cross_entropy(probabilities, weight):\n",
        "    return(torch.mean(weight * torch.log(probabilities), dim = 1))\n",
        "''';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TWKiWR-hYbJG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def softmax(probabilities, safe_normalization = True, eps = 0.000001):\n",
        "  \n",
        "    if safe_normalization == \"safe\":\n",
        "        exp_probs = torch.exp(probabilities)\n",
        "        normalization = torch.maximum(torch.sum(exp_probs, dim = 1), eps)\n",
        "  \n",
        "        if normalization > 0: \n",
        "            return(exp_probs / normalization)\n",
        "    \n",
        "        else:\n",
        "            return(exp_probs)\n",
        "  \n",
        "    else:\n",
        "        return(torch.nn.functional.softmax(probabilities, dim = 1))\n",
        "''';"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def sigmoid_sum(probabilities):\n",
        "    sig_probs = torch.sigmoid(probabilities)\n",
        "    normalization = torch.sum(sig_probs, dim = 1)\n",
        "    return(sig_probs / normalization)\n",
        "''';"
      ],
      "metadata": {
        "id": "EUWhfPU55OlM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "h0LYqJUeUJne"
      },
      "outputs": [],
      "source": [
        "# Note: this function comes directly from the museGAN tutorial [1].\n",
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
        "    \"\"\"Compute the gradient penalty for regularization. Intuitively, the\n",
        "    gradient penalty help stablize the magnitude of the gradients that the\n",
        "    discriminator provides to the generator, and thus help stablize the training\n",
        "    of the generator.\"\"\"\n",
        "    # Get random interpolations between real and fake samples\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).to(device)\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n",
        "    interpolates = interpolates.requires_grad_(True)\n",
        "    \n",
        "    # Get the discriminator output for the interpolations\n",
        "    d_interpolates, _ = discriminator(interpolates)\n",
        "    # Get gradients w.r.t. the interpolations\n",
        "    fake = torch.ones(real_samples.size(0)).to(device)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    # Compute gradient penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Sources:\n",
        "# [1] https://github.com/salu133445/ismir2019tutorial/blob/main/musegan.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ejVE_JNS2VE"
      },
      "source": [
        "#### Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Jxg1fBDoSfm_"
      },
      "outputs": [],
      "source": [
        "class Log :\n",
        "    \"\"\"\n",
        "        container class for GANTraining logs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self, rounds, dis_rounds, n_labels, batch_size) :\n",
        "        self.losses        = np.zeros((9, rounds)) \n",
        "        self.music_probs   = np.zeros((2, rounds))\n",
        "        self.genre_probs   = np.zeros((1 + n_labels, rounds))\n",
        "        self.abs_diff      = np.zeros((2, rounds))  # abs_mean_diff(), abs_std_diff()\n",
        "        self.gen_var       = np.zeros((2, rounds))  # inter_bar_var(), inter_track_var()\n",
        "        \n",
        "        self._dis_losses   = torch.zeros((7, dis_rounds)).cpu()\n",
        "        self._music_probs  = torch.zeros((2, dis_rounds)).cpu()\n",
        "        self._genre_probs  = torch.zeros((1 + n_labels, dis_rounds)).cpu()\n",
        "\n",
        "        #self.dis_real = torch.zeros((rounds, dis_rounds, batch_size, 1 + n_labels)).cpu()\n",
        "        #self.dis_gen  = torch.zeros((rounds, dis_rounds, batch_size, 1 + n_labels)).cpu()\n",
        "        #self.dis_new  = torch.zeros((rounds, batch_size, 1 + n_labels)).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "jMlCiiOwBybk"
      },
      "outputs": [],
      "source": [
        "class LogLoaded :\n",
        "    \"\"\"\n",
        "        A class to load stored Log data from an .npz file\n",
        "        and to use it exactly like Log.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log_dictionary) :\n",
        "        for keyword, value in log_dictionary.items() :\n",
        "            setattr(self, keyword, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WstRQBd2SfnA"
      },
      "source": [
        "### GANTraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fg1SZJl7SfnA"
      },
      "outputs": [],
      "source": [
        "class GANTraining :\n",
        "    \"\"\"\n",
        "        general GAN training class\n",
        "        How To Use:\n",
        "        * `MyTrain = GANTraining(<Generator>, <Discriminator>, <torch_dataset>)`\n",
        "        * `MyTrain.setup(<int_rounds>, batchsize = 1, discriminator_rounds = 1,     \n",
        "                        loss_function = [\"WGAN\", \"GAN\"])`\n",
        "        * `MyTrain.train()`\n",
        "      \n",
        "        After That:\n",
        "        * `MyTrain.gen` contains trained Generator\n",
        "        * `MyTrain.dis` contains trained Discriminator\n",
        "        * `MyTrain.log` contains metrics from each round (see class Log)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__ (self, Gen, Dis, dataset) :\n",
        "        assert  type(dataset) == torch.utils.data.dataset.TensorDataset\n",
        "        \n",
        "        self.device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "\n",
        "        # GAN classes and dataset\n",
        "        self.n_labels = lpd5.n_labels     # number of labels in dataset, automate maybe\n",
        "        self.GenClass = Gen\n",
        "        self.DisClass = Dis\n",
        "        self.dataset  = dataset\n",
        "        \n",
        "\n",
        "    def setup (self, rounds, batch_size = 1, discriminator_rounds = 1, \n",
        "               loss_function = \"CAN\", info_text = \"\",\n",
        "               norm_dis_probs = False, norm_dis_genre = False,\n",
        "               genre_ambiguity = (0, 0), learning_rates = (0.001, 0.001)) :\n",
        "        assert  type(rounds) == int\n",
        "        assert  rounds >= 1\n",
        "        assert  type(batch_size) == int\n",
        "        assert  batch_size >= 1\n",
        "        assert  type(discriminator_rounds) == int\n",
        "        assert  discriminator_rounds >= 1\n",
        "        assert  loss_function in [\"GAN\", \"WGAN\", \"WGAN-GP\", \"CAN\",  \"WCAN-GP\"]\n",
        "\n",
        "        # Training parameters\n",
        "        self.rounds     = rounds\n",
        "        self.batch_size = batch_size\n",
        "        self.dis_rounds = discriminator_rounds\n",
        "        self.loss       = loss_function\n",
        "        \n",
        "        self.lr_dis, self.lr_gen       = learning_rates\n",
        "        self.betas_dis, self.betas_gen = ((0.5, 0.9), (0.5, 0.9))\n",
        "        \n",
        "        self.norm_dis_probs = norm_dis_probs  # toggle for music output normalization\n",
        "        self.norm_dis_genre = norm_dis_genre  # toggle for genre output normalization\n",
        "        self.amb_dis, self.amb_gen = genre_ambiguity  # prefactor for CAN dis and gen loss\n",
        "        \n",
        "        self.start_round = 0\n",
        "        self.info = info_text\n",
        "        self._round_range = torch.arange(self.batch_size)  # used each round\n",
        "        \n",
        "\n",
        "        # Dataloader\n",
        "        self.data_loader = torch.utils.data.DataLoader(self.dataset,\n",
        "                                batch_size = self.batch_size, \n",
        "                                drop_last = True,\n",
        "                                shuffle = True)\n",
        "        self.dataset_size = self.dataset.tensors[0].shape[0]  # number of instances in dataset\n",
        "        self.batch_count = self.dataset_size // self.batch_size\n",
        "        self._batch_idx  = self.batch_count \n",
        "        \n",
        "        # Logs\n",
        "        self.log    = Log(self.rounds, self.dis_rounds, self.n_labels, self.batch_size)\n",
        "        self.backup = False   # only on if self.set_backup() is run\n",
        "        \n",
        "        # Initialize GAN\n",
        "        self.gen = self.GenClass().to(self.device)\n",
        "        self.dis = self.DisClass().to(self.device)\n",
        "        self.optimizer_gen = torch.optim.Adam(self.gen.parameters(), \n",
        "                                              lr = self.lr_gen,\n",
        "                                              betas = self.betas_gen)\n",
        "        self.optimizer_dis = torch.optim.Adam(self.dis.parameters(), \n",
        "                                              lr = self.lr_dis,\n",
        "                                              betas = self.betas_dis)\n",
        "        # Note: ADAM parameters from GAN tutorial [1].\n",
        "\n",
        "\n",
        "    def resume (self, load_folder, round) :\n",
        "        train_state = load_train_state(load_folder, round)\n",
        "\n",
        "        self.start_round = round\n",
        "        if self.info == \"\" :\n",
        "            self.info = train_state[\"info\"]\n",
        "        self.gen.load_state_dict(train_state[\"gen\"])\n",
        "        self.dis.load_state_dict(train_state[\"dis\"])\n",
        "        self.optimizer_gen.load_state_dict(train_state[\"opt_g\"])\n",
        "        self.optimizer_dis.load_state_dict(train_state[\"opt_d\"])\n",
        "        log_dict = train_state[\"log\"]   \n",
        "        for name, array in log_dict.items() :\n",
        "            setattr(self.log, name, array)\n",
        "            \n",
        "       \n",
        "    def set_backups (self, training_name, checkpoints, save_folder = \"\") :\n",
        "        assert  type(training_name) == str\n",
        "        for element in checkpoints :\n",
        "            assert  type(element) == int\n",
        "            assert  element >= 0  and  element <= self.rounds\n",
        "        \n",
        "        self.training_name   = training_name\n",
        "        self.training_folder = save_folder   # 'timestamp+training_name', gets set at first backup\n",
        "        self.checkpoints     = [point for point in checkpoints\n",
        "                                if point > self.start_round]\n",
        "        self.backup          = True  # Flag for rest of code\n",
        "\n",
        "\n",
        "\n",
        "    def train (self) :\n",
        "        assert  hasattr(self, \"data_loader\")  # If test fails, you haven't run set_params()\n",
        "\n",
        "        print(f\"Training\")\n",
        "        for round in notebook.tqdm(range(self.start_round, self.rounds)) :\n",
        "            print(f\"Round {round}\", end = \"\")\n",
        "            for dis_round in range(self.dis_rounds) :\n",
        "                self._discriminator_update(round, dis_round)\n",
        "\n",
        "            self._generator_update(round)\n",
        "        \n",
        "            # Make a backup\n",
        "            if self.backup  and  (round + 1) in self.checkpoints :\n",
        "                self._save_state(round + 1)\n",
        "            print(\"\\r\", end = \"\")\n",
        "\n",
        "            # Stop Training if diverges\n",
        "            divergence = torch.any(torch.isnan(self.loss_dis.detach().cpu()))\n",
        "            if divergence:\n",
        "                print(\"\\rTraining stopped: nan values encontered.\")\n",
        "                self._save_state(round + 1)\n",
        "                return\n",
        "            \n",
        "        # End training\n",
        "        self._save_state(self.rounds)\n",
        "        self.gen.eval()\n",
        "        self.dis.eval()\n",
        "        print(\"Training complete. GAN now in eval() mode.\")\n",
        "\n",
        "\n",
        "    def _get_batch (self) :\n",
        "        \"\"\"\n",
        "            samples one batch of data from self.data_loader without replacement.\n",
        "            When the self.data_set is depleted of fresh batches, \n",
        "            self.data_loader will shuffle a list of new batches.\n",
        "        \"\"\"\n",
        "        if self._batch_idx >= self.batch_count :\n",
        "            self._data_iter = iter(self.data_loader)\n",
        "            self._batch_idx = 0\n",
        "        batch_data, batch_labels = self._data_iter.next()\n",
        "        batch_data = batch_data.view(-1, lpd5.bars, \n",
        "                                     lpd5.blips_per_bar, lpd5.pitches)\n",
        "        self._batch_idx += 1\n",
        "\n",
        "        return batch_data.to(self.device), batch_labels.to(self.device)\n",
        "\n",
        "\n",
        "    def _discriminator_update (self, round, dis_round) :\n",
        "        \n",
        "        #self._dis_real = torch.zeros((self.batch_size, 1 + self.n_labels)).to(self.device)\n",
        "        #self._dis_gen  = torch.zeros((self.batch_size, 1 + self.n_labels)).to(self.device)\n",
        "        #self._dis_new  = torch.zeros((self.batch_size, 1 + self.n_labels)).to(self.device)\n",
        "\n",
        "        # Forward propagation\n",
        "        batch_real, labels_real = self._get_batch()  # training data\n",
        "        batch_gen               = self.gen.forward(batch_size = self.batch_size)   # generated data\n",
        "        \n",
        "        music_dis_real, genre_dis_real = self.dis.forward(batch_real)\n",
        "        #self._dis_real[:, 0]  = music_dis_real\n",
        "        #self._dis_real[:, 1:] = genre_dis_real\n",
        "        music_dis_gen, genre_dis_gen   = self.dis.forward(batch_gen)\n",
        "        #self._dis_gen[:, 0]  = music_dis_gen\n",
        "        #self._dis_gen[:, 1:] = genre_dis_gen\n",
        "        \n",
        "        self.music_prob_real    = torch.sigmoid(music_dis_real)\n",
        "        genre_logprobs_real     = torch.log_softmax(genre_dis_real, dim = 1)\n",
        "        self.genre_logprob_real = genre_logprobs_real[self._round_range, labels_real] # get prob of real genre\n",
        "        \n",
        "        self.music_prob_gen     = torch.sigmoid(music_dis_gen)\n",
        "        self.genre_logprobs_gen = torch.log_softmax(genre_dis_gen, dim = 1)\n",
        "                   \n",
        "\n",
        "        # Calculating the Discriminator loss terms\n",
        "        if self.loss == \"GAN\" :\n",
        "            self.loss_reg        = torch.tensor(0.).to(self.device)\n",
        "            self.loss_real_music = - torch.mean(torch.log(self.music_prob_real))\n",
        "            self.loss_gen_music  = - torch.mean(torch.log(1 - self.music_prob_gen))\n",
        "            self.loss_real_genre = torch.tensor(0.).to(self.device)\n",
        "        \n",
        "        elif self.loss == \"WGAN\" :\n",
        "            var_gen   = torch.var(music_dis_gen)\n",
        "            var_real  = torch.var(music_dis_real)\n",
        "            self.loss_reg  = torch.where(var_gen > 1, \n",
        "                                            (var_gen - 1)**2, 0) \\\n",
        "                                + torch.where(var_real > 1, \n",
        "                                            (var_real - 1)**2, 0)\n",
        "            self.loss_real_music = - torch.mean(music_dis_real)\n",
        "            self.loss_real_genre = torch.tensor(0.).to(self.device)\n",
        "            self.loss_gen_music  = torch.mean(music_dis_gen)\n",
        "        \n",
        "        elif self.loss == \"WGAN-GP\" :    \n",
        "            gradient_penalty = compute_gradient_penalty(\n",
        "                self.dis, batch_real, batch_gen, self.device)\n",
        "            self.loss_reg        = 10 * gradient_penalty\n",
        "            self.loss_real_music = - torch.mean(music_dis_real)\n",
        "            self.loss_real_genre = torch.tensor(0.).to(self.device)\n",
        "            self.loss_gen_music  = torch.mean(music_dis_gen)\n",
        "        \n",
        "\n",
        "        elif self.loss == \"CAN\" :\n",
        "            self.loss_reg        = torch.tensor(0.).to(self.device)\n",
        "            self.loss_real_music = - torch.mean(torch.log(self.music_prob_real))\n",
        "            self.loss_real_genre = - torch.mean(self.genre_logprob_real)\n",
        "            self.loss_gen_music  = - torch.mean(torch.log(1 - self.music_prob_gen))\n",
        "\n",
        "        elif self.loss == \"WCAN-GP\" : \n",
        "            self.loss_reg        = 10.0 * compute_gradient_penalty(\n",
        "                                   self.dis, batch_real, batch_gen, self.device)\n",
        "            self.loss_real_music = - torch.mean(music_dis_real)\n",
        "            self.loss_real_genre = - torch.mean(self.genre_logprob_real)\n",
        "            self.loss_gen_music  = torch.mean(music_dis_gen)\n",
        "\n",
        "\n",
        "        # Optional discriminator output normalization losses for stability\n",
        "        self.prob_loss = torch.tensor(0.).to(self.device)\n",
        "        if self.norm_dis_probs :\n",
        "            severity       = 1\n",
        "            prob_norm_loss = torch.mean((self.music_prob_real + \n",
        "                             self.music_prob_gen - 1)**2)\n",
        "            self.prob_loss = severity * prob_norm_loss\n",
        "            \n",
        "        self.genre_loss = torch.tensor(0.).to(self.device)\n",
        "        if self.norm_dis_genre :\n",
        "            severity   = 1\n",
        "            threshold  = 5\n",
        "            laxity     = 1\n",
        "            self.genre_loss = severity * torch.sum(\n",
        "                (torch.threshold(torch.abs(genre_dis_real) - threshold, 0, 0)\n",
        "                 / laxity)**2)\n",
        "\n",
        "\n",
        "        # Loss function\n",
        "        self.loss_dis = self.loss_real_music \\\n",
        "                        + self.amb_dis * self.loss_real_genre \\\n",
        "                        + self.loss_gen_music \\\n",
        "                        + self.loss_reg \\\n",
        "                        + self.prob_loss \\\n",
        "                        + self.genre_loss\n",
        "        self._log_all(round, k = dis_round)\n",
        "\n",
        "\n",
        "        # Discriminator update\n",
        "        self.optimizer_dis.zero_grad()\n",
        "        self.loss_dis.backward()\n",
        "        self.optimizer_dis.step()\n",
        "\n",
        "\n",
        "    def _generator_update (self, round) :\n",
        "        \n",
        "        # Forward propagation\n",
        "        batch_new = self.gen.forward(batch_size = self.batch_size)\n",
        "        music_dis_new, genre_dis_new = self.dis.forward(batch_new)\n",
        "        #self._dis_new[:, 0]  = music_dis_new\n",
        "        #self._dis_new[:, 1:] = genre_dis_new\n",
        "\n",
        "        music_prob_new = torch.sigmoid(music_dis_new)\n",
        "\n",
        "\n",
        "        # Calculating the Generator loss terms\n",
        "        if self.loss == \"GAN\" :\n",
        "            self.loss_gen_music = -torch.mean(torch.log(music_prob_new)) \n",
        "            self.loss_gen_genre = torch.tensor(0.)\n",
        "        \n",
        "        elif self.loss == \"WGAN\" :\n",
        "            self.loss_gen_music = -torch.mean(music_dis_new)\n",
        "            self.loss_gen_genre = torch.tensor(0.)\n",
        "\n",
        "        elif self.loss == \"WGAN-GP\" :\n",
        "            self.loss_gen_music = -torch.mean(music_dis_new)\n",
        "            self.loss_gen_genre = torch.tensor(0.)\n",
        "        \n",
        "        elif self.loss == \"CAN\" :\n",
        "            self.loss_gen_music    = - torch.mean(music_prob_new)\n",
        "            genre_logprobs_new     = torch.log_softmax(genre_dis_new, dim = 1)\n",
        "            genre_antilogprobs_new = torch.log(1 - torch.softmax(genre_dis_new, dim = 1))\n",
        "            self.loss_gen_genre    = - torch.mean( \\\n",
        "                torch.sum(1/self.n_labels * genre_logprobs_new, dim = 1) + \\\n",
        "                torch.sum((1 - 1/self.n_labels) * genre_antilogprobs_new, dim = 1))\n",
        "            \n",
        "        elif self.loss == \"WCAN-GP\" :\n",
        "            self.loss_gen_music    = - torch.mean(music_dis_new)\n",
        "            genre_logprobs_new     = torch.log_softmax(genre_dis_new, dim = 1)\n",
        "            genre_antilogprobs_new = torch.log(1 - torch.softmax(genre_dis_new, dim = 1))\n",
        "            self.loss_gen_genre    = - torch.mean( \\\n",
        "                torch.sum(1/self.n_labels * genre_logprobs_new, dim = 1) + \\\n",
        "                torch.sum((1 - 1/self.n_labels) * genre_antilogprobs_new, dim = 1))\n",
        "\n",
        "\n",
        "        # Loss function  \n",
        "        self.loss_gen = self.loss_gen_music \\\n",
        "                        + self.amb_gen * self.loss_gen_genre\n",
        "\n",
        "        self._log_all(round)\n",
        "            \n",
        "        \n",
        "        # Generator update\n",
        "        self.optimizer_gen.zero_grad()\n",
        "        self.loss_gen.backward()\n",
        "        self.optimizer_gen.step()\n",
        "\n",
        "\n",
        "    def _log_all (self, round, k = -1) :\n",
        "        if k >= 0 : # before each Discriminator update\n",
        "            # Discriminator outputs\n",
        "            #self.log.dis_real[round, k] = self._dis_real.cpu().detach()\n",
        "            #self.log.dis_gen[round, k]  = self._dis_gen.cpu().detach()\n",
        "            \n",
        "            # Losses\n",
        "            self.log._dis_losses[0, k] = self.loss_dis.cpu().detach()\n",
        "            self.log._dis_losses[1, k] = self.loss_real_music.cpu().detach()\n",
        "            self.log._dis_losses[2, k] = self.amb_dis * self.loss_real_genre.cpu().detach()\n",
        "            self.log._dis_losses[3, k] = self.loss_gen_music.cpu().detach()\n",
        "            self.log._dis_losses[4, k] = self.loss_reg.cpu().detach()\n",
        "            self.log._dis_losses[5, k] = self.prob_loss.cpu().detach()\n",
        "            self.log._dis_losses[6, k] = self.genre_loss.cpu().detach()\n",
        "\n",
        "            # Discrimantor probabilities\n",
        "            self.log._music_probs[0, k] = self.music_prob_real.mean().cpu().detach()\n",
        "            self.log._music_probs[1, k] = self.music_prob_gen.mean().cpu().detach()\n",
        "            genre_prob_real = torch.exp(self.genre_logprob_real)\n",
        "            genre_probs_gen = torch.exp(self.genre_logprobs_gen)\n",
        "            self.log._genre_probs[0, k]  = genre_prob_real.mean().cpu().detach() # prob of right label of real batch\n",
        "            self.log._genre_probs[1:, k] = genre_probs_gen.mean(dim = 0).cpu().detach() # prob of genres of generated batch\n",
        "        \n",
        "\n",
        "        if k == -1 : # before each Generator update\n",
        "            # Discriminator outputs\n",
        "            #self.log.dis_new[round] = self._dis_new.cpu().detach()\n",
        "            \n",
        "            # Losses\n",
        "            dis_losses = self.log._dis_losses.detach().cpu().numpy()\n",
        "            self.log.losses[:7, round] = dis_losses[:7].mean(axis = 1)\n",
        "            self.log.losses[7, round]  = self.loss_gen.detach().cpu().numpy()\n",
        "            self.log.losses[8, round]  = self.amb_gen * self.loss_gen_genre.detach().cpu().numpy()\n",
        "            \n",
        "            # Discriminator Probabilities\n",
        "            music_probs = self.log._music_probs.cpu().detach().numpy()\n",
        "            genre_probs = self.log._genre_probs.cpu().detach().numpy()\n",
        "            self.log.music_probs[:, round] = music_probs.mean(axis = 1)\n",
        "            self.log.genre_probs[:, round] = genre_probs.mean(axis = 1)\n",
        "            \n",
        "            # Generator metrics\n",
        "            batch_real, _ = self._get_batch()\n",
        "            batch_gen     = self.gen.forward(batch_size = self.batch_size)\n",
        "            self.log.abs_diff[0, round] = abs_mean_diff(batch_gen, batch_real)\n",
        "            self.log.abs_diff[1, round] = abs_std_diff(batch_gen, batch_real)\n",
        "            self.log.gen_var[0, round]  = inter_bar_var(batch_gen)\n",
        "            self.log.gen_var[1, round]  = inter_track_var(batch_gen)\n",
        "\n",
        "\n",
        "    def _save_state (self, round) :\n",
        "        if self.training_folder == \"\" :\n",
        "            self.training_folder = save_train_state(self, round)\n",
        "            print(f\"\\rSaved checkpoint {round} under '{self.training_folder}'.\")\n",
        "        else :\n",
        "            save_train_state(self, round)\n",
        "            print(f\"\\rSaved checkpoint {round}.\")\n",
        "\n",
        "\n",
        "# Sources:\n",
        "# [1] https://github.com/salu133445/ismir2019tutorial/blob/main/musegan.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UjwRjHiook6"
      },
      "source": [
        "### Evaluation support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixzSjpbOS64f"
      },
      "source": [
        "#### Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3STzYDPRS9HD"
      },
      "outputs": [],
      "source": [
        "def empty_bar_ratio (data) :\n",
        "    \"\"\"\n",
        "        ratio of bars devoid of notes\n",
        "        \n",
        "        also called:\n",
        "            EB = \"empty bar ratio\"\n",
        "    \"\"\"\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches)) # split into bars\n",
        "    data_reduced = np.mean(data, axis = (2, 3)).flatten() # mean over bar pixels\n",
        "    data_mask    = np.array(data_reduced == 0)  # bool of which bars are empty\n",
        "    empty_bar_fraction = np.mean(data_mask)  # mean over all bars\n",
        "\n",
        "    return empty_bar_fraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GclWeQ3TU7hU"
      },
      "outputs": [],
      "source": [
        "def pitch_classes_per_bar (data) :\n",
        "    \"\"\"\n",
        "        number of pitch classes used per bar (from 0 to 12)\n",
        "        \n",
        "        also called:\n",
        "            UPC = \"used pitch classes per bar\"\n",
        "    \"\"\"\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches)) # split into bars\n",
        "    data = data.reshape((-1, lpd5.blips_per_bar, lpd5.pitches))  # array of bars\n",
        "    data = data.reshape((-1, lpd5.blips_per_bar, lpd5.octaves, 12)) # split into octaves\n",
        "    \n",
        "    pitches_used = np.any(data, axis = (1, 2))  # OR over timesteps and octaves\n",
        "    number_pitches = np.sum(pitches_used, axis = 1) # sum over pitches\n",
        "    mean_pitch_classes_per_bar = np.mean(number_pitches) # mean over all bars\n",
        "    \n",
        "    return mean_pitch_classes_per_bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "10tasJPahEvQ"
      },
      "outputs": [],
      "source": [
        "def qualified_note_ratio (data) :\n",
        "    \"\"\"\n",
        "        ratio of \"qualified\" notes,\n",
        "        defined as a 3 blips/timesteps or longer. \n",
        "        In the current lpd5 dataset with 48-blip bars that is a 1/16 note.\n",
        "        ! Not like in museGAN (used 96-blip bars and thus a 1/32 note threshold)\n",
        "        \n",
        "        also called:\n",
        "            QN = \"qualified note ratio\"\n",
        "    \"\"\"\n",
        "    minimum_length = 3 # blips\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.width, lpd5.height)) # whole tracks\n",
        "    conv = np.array([-1, 1]) # used to measure note start and ends\n",
        "\n",
        "    total_notes       = 0\n",
        "    total_quali_notes = 0\n",
        "    for track in data :\n",
        "        for pitch_line in track.T :\n",
        "            note_starts = np.convolve(pitch_line, conv)\n",
        "            note_stops  = np.convolve(pitch_line, -conv)\n",
        "            start_indices = np.where(note_starts == -1)[0]\n",
        "            stop_indices  = np.where(note_stops == -1)[0]\n",
        "            \n",
        "            note_lengths     = stop_indices - start_indices\n",
        "            note_count       = note_lengths.shape[0]\n",
        "            quali_note_count = np.sum(note_lengths >= minimum_length)\n",
        "            total_notes       += note_count\n",
        "            total_quali_notes += quali_note_count\n",
        "\n",
        "    quali_note_ratio = total_quali_notes / total_notes\n",
        "\n",
        "    return quali_note_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jEpHEch60X02"
      },
      "outputs": [],
      "source": [
        "def muspy_metrics (data) :\n",
        "    \"\"\"\n",
        "    computes 4 muspy metrics from a batch of pianoroll data\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    averaged_metrics : np.array, size = (4), dtype = float\n",
        "        all values taken from whole pianoroll tracks and\n",
        "        are averaged over all tracks\n",
        "        1. muspy.pitch_range()\n",
        "            pitch range from lowest to highest pitch\n",
        "        2. muspy.polyphony()\n",
        "            average number of pitches being played concurrently\n",
        "        3. muspy.scale_consistency()\n",
        "            how many of the notes are in the track’s main scale \n",
        "            (max of notes in any scale)\n",
        "        4. muspy.empty_measure_rate()\n",
        "            ratio of 1/4 note beats where no note is played\n",
        "            \"measure\" is here defined as 1/4 notes by us.\n",
        "\n",
        "        For more details, see [1]\n",
        "\n",
        "    [1] https://muspy.readthedocs.io/en/stable/metrics.html?highlight=measures#other-metrics\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.width, lpd5.height)) # whole tracks\n",
        "    data.dtype = bool\n",
        "    \n",
        "    pianorolls = np.pad(data, \n",
        "                        ((0, 0), (0, 0), \n",
        "                         (lpd5.lowest_pitch, \n",
        "                          128 - lpd5.lowest_pitch - lpd5.height))\n",
        "                 )   # complete the pitch range\n",
        "    \n",
        "    muspy_stats = np.zeros((4, data.shape[0]))\n",
        "    for i, track in enumerate(pianorolls):\n",
        "        piano_music = muspy.from_pianoroll_representation(\n",
        "                        track,\n",
        "                        resolution = lpd5.blips_per_beat, \n",
        "                        encode_velocity = False\n",
        "                    )   # convert to muspy.music_object\n",
        "                  \n",
        "        muspy_stats[0, i] = muspy.pitch_range(piano_music)\n",
        "        muspy_stats[1, i] = muspy.polyphony(piano_music)\n",
        "        muspy_stats[2, i] = muspy.scale_consistency(piano_music)\n",
        "        muspy_stats[3, i] = muspy.empty_measure_rate(piano_music, \n",
        "                                                     lpd5.blips_per_beat)\n",
        "        \n",
        "    averaged_metrics = np.nanmean(muspy_stats, axis = 1)\n",
        "    \n",
        "    return averaged_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XqBIvah5RK7k"
      },
      "outputs": [],
      "source": [
        "# Calculate key metrics of dataset for evaluation\n",
        "\n",
        "lpd5_metrics_file = f\"{default_training_path}/{default_dataset}/lpd5_metrics.json\"\n",
        "\n",
        "if not os.path.exists(lpd5_metrics_file):\n",
        "    # Calculating these metrics takes several minutes for lpd5.\n",
        "    # Therefore, they are calculated once and then stored in a file.\n",
        "    metrics = {}\n",
        "    metrics[\"abs_mean_diff\"]   = 0   # difference of dataset to itself\n",
        "    metrics[\"abs_std_diff\"]    = 0\n",
        "    metrics[\"inter_bar_var\"]   = inter_bar_var(lpd5.data.view(-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches))\n",
        "    metrics[\"inter_track_var\"] = inter_track_var(lpd5.data)\n",
        "    metrics[\"empty_bar_ratio\"]        = empty_bar_ratio(lpd5.data)\n",
        "    metrics[\"pitch_classses_per_bar\"] = pitch_classes_per_bar(lpd5.data)\n",
        "    metrics[\"qualified_note_ratio\"]   = qualified_note_ratio(lpd5.data)\n",
        "    metrics[\"muspy_metrics\"] = muspy_metrics(lpd5.data)\n",
        "    with open(lpd5_metrics_file, 'wb') as file :\n",
        "        pickle.dump(metrics, file)\n",
        "\n",
        "\n",
        "with open(lpd5_metrics_file, 'rb') as file :\n",
        "    # Loading all metrics is much quicker than recalculating them\n",
        "    metrics = pickle.load(file)\n",
        "    lpd5.abs_mean_diff   = metrics[\"abs_mean_diff\"]\n",
        "    lpd5.abs_std_diff    = metrics[\"abs_std_diff\"]\n",
        "    lpd5.inter_bar_var   = metrics[\"inter_bar_var\"]\n",
        "    lpd5.inter_track_var = metrics[\"inter_track_var\"]\n",
        "    lpd5.empty_bar_ratio        = metrics[\"empty_bar_ratio\"]\n",
        "    lpd5.pitch_classses_per_bar = metrics[\"pitch_classses_per_bar\"]\n",
        "    lpd5.qualified_note_ratio   = metrics[\"qualified_note_ratio\"]\n",
        "    lpd5.muspy_metrics = metrics[\"muspy_metrics\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyJRhxx8L_4y"
      },
      "source": [
        "#### Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "_VBC5mONqw-T"
      },
      "outputs": [],
      "source": [
        "def plot_training (log, dataset = lpd5, CAN = True, show_loss_terms = True) :\n",
        "    training_rounds = log.losses.shape[1]\n",
        "    rounds          = np.arange(training_rounds) + 1\n",
        "    filter_size     = math.floor(np.sqrt(training_rounds))\n",
        "    med_filter      = lambda x: ndimage.median_filter(x, size = filter_size)\n",
        "    \n",
        "    \n",
        "    # Training metrics\n",
        "\n",
        "    plt.figure(figsize = (16, 8))\n",
        "    plt.suptitle(\"Training metrics\", size=18)\n",
        "    \n",
        "    ## Losses\n",
        "    plt.title(\"Losses\")\n",
        "    dis_loss = log.losses[0]\n",
        "    gen_loss = log.losses[7]\n",
        "    plt.plot(rounds, dis_loss, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, gen_loss, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(dis_loss), label=\"Discriminator Loss\", \n",
        "             c=\"b\") #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(gen_loss), label=\"Generator Loss\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale('symlog', linthreshy = 10)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    if show_loss_terms :\n",
        "        plt.figure(figsize=(16,6))\n",
        "        plt.title(r\"Discriminator & Generator loss terms\")\n",
        "        loss_term_labels = [\"music dis. loss for real data\",\n",
        "                            \"genre dis. loss for real label\",\n",
        "                            \"music dis. loss for generated data\",\n",
        "                            \"genre gen. loss for generated data\",\n",
        "                            \"dis. regularization loss\",\n",
        "                            \"dis. music prob. normalization loss\",\n",
        "                            \"dis. genre prob. normalization losss\",\n",
        "                            ]\n",
        "        for label_idx, loss_idx in enumerate([1, 2, 3, 8, 4, 5, 6]):\n",
        "            loss_term = log.losses[loss_idx]\n",
        "            plt.plot(rounds, med_filter(loss_term), \n",
        "                     label = loss_term_labels[label_idx])\n",
        "        plt.xlabel(\"round\")\n",
        "        plt.yscale('symlog', linthreshy = 10)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    ## Probabilities\n",
        "    plt.figure(figsize=(16,4))\n",
        "    prob_real = log.music_probs[0]\n",
        "    prob_gen  = log.music_probs[1]\n",
        "    prob_diff = prob_real - prob_gen\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(r\"$p_{Dis}(data_{real} = real)$\")\n",
        "    plt.plot(rounds, np.ones_like(prob_real), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, np.zeros_like(prob_real), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, prob_real, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(prob_real), c=\"b\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(r\"$p_{Dis}(data_{real} = real) - p_{Dis}(data_{gen} = real)$\")\n",
        "    plt.plot(rounds, np.ones_like(prob_diff), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, np.zeros_like(prob_diff), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, prob_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(prob_diff), c=\"b\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")  \n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "    # CAN metrics\n",
        "    if CAN :\n",
        "        plt.figure(figsize=(16,6))\n",
        "        plt.suptitle(\"CAN metrics\", size=18)\n",
        "        genre_prob_real = log.genre_probs[0]\n",
        "        genre_probs_gen = log.genre_probs[1:]\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Genre probability of true label given to real samples\")\n",
        "        plt.plot(rounds, np.ones_like(genre_prob_real), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        plt.plot(rounds, np.zeros_like(genre_prob_real), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        plt.plot(rounds, genre_prob_real, lw = 0.5, alpha=0.5)\n",
        "        plt.plot(rounds, med_filter(genre_prob_real), c='b')\n",
        "        plt.xlabel(\"round\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Genre probabilities given to generated samples\")        \n",
        "        plt.plot(rounds, np.ones_like(genre_prob_real), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        plt.plot(rounds, np.zeros_like(genre_prob_real), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        for i, prob in enumerate(genre_probs_gen) :\n",
        "            plt.plot(rounds, med_filter(prob), label = dataset.genre_list[i])\n",
        "        plt.xlabel(\"round\")\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # Generator metrics\n",
        "\n",
        "    plt.figure(figsize=(16,6))\n",
        "    plt.suptitle(\"Generator metrics\", size=18)\n",
        "    abs_mean_diff   = log.abs_diff[0]\n",
        "    abs_std_diff    = log.abs_diff[1]\n",
        "    inter_bar_var   = log.gen_var[0]\n",
        "    inter_track_var = log.gen_var[1]\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Absolute mean and std difference to dataset\")\n",
        "    plt.plot(rounds, abs_mean_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, abs_std_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(abs_mean_diff), label = \"abs_mean_diff\", \n",
        "             c='b') #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(abs_std_diff), label = \"abs_std_diff\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Generator variation\")\n",
        "    plt.plot(rounds, np.ones_like(inter_bar_var) * lpd5.inter_bar_var, \n",
        "             linestyle=\"--\", lw=0.5, color='b', label=\"dataset bar-wise std.\")\n",
        "    plt.plot(rounds, np.ones_like(inter_bar_var) * lpd5.inter_track_var, \n",
        "             linestyle=\"--\", lw=0.5, color='r', label=\"dataset track-wise std.\")\n",
        "    plt.plot(rounds, inter_bar_var, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, inter_track_var, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(inter_bar_var), label = \"bar-wise std dev.\", \n",
        "             c=\"b\") #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(inter_track_var), label = \"track-wise std dev.\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mWBpY1DkSfm_"
      },
      "outputs": [],
      "source": [
        "def long_test (generator, discriminator, data, test_size = 1000, \n",
        "               dataset = default_dataset): \n",
        "    \"\"\"\n",
        "        runs a detailed evaluation of generator performance\n",
        "        and compares it to the training data set in a pandas table\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    with torch.inference_mode() :  # saves gpu memory\n",
        "        device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "        \n",
        "        # Test on generated data\n",
        "        generator.eval().to(device)\n",
        "        discriminator.eval().to(device)\n",
        "        data_real, labels_real = iter(torch.utils.data.DataLoader(data.dataset,\n",
        "                        batch_size = test_size, \n",
        "                        shuffle = True, drop_last = True)\n",
        "                    ).next()  # make one batch of test_size\n",
        "        data_real   = data_real.to(device)\n",
        "        labels_real = labels_real.to(device)\n",
        "\n",
        "        ## Generator\n",
        "        data_generated = generator.forward(batch_size = test_size)\n",
        "        \n",
        "        gen_abs_mean_diff   = abs_mean_diff(data_generated, data_real)\n",
        "        gen_abs_std_diff    = abs_std_diff(data_generated, data_real)\n",
        "        gen_inter_bar_var   = inter_bar_var(data_generated)\n",
        "        gen_inter_track_var = inter_track_var(data_generated)\n",
        "\n",
        "        gen_empty_bar_ratio        = empty_bar_ratio(data_generated)\n",
        "        gen_pitch_classses_per_bar = pitch_classes_per_bar(data_generated)\n",
        "        gen_qualified_note_ratio   = qualified_note_ratio(data_generated)\n",
        "\n",
        "        gen_muspy_metrics = muspy_metrics(data_generated)\n",
        "\n",
        "    # Create comparison table: generated data vs. real data\n",
        "    \n",
        "    real_music_metrics = np.array([\n",
        "        lpd5.abs_mean_diff,\n",
        "        lpd5.abs_std_diff,\n",
        "        lpd5.inter_bar_var,\n",
        "        lpd5.inter_track_var,\n",
        "        lpd5.empty_bar_ratio,\n",
        "        lpd5.pitch_classses_per_bar,\n",
        "        lpd5.qualified_note_ratio,\n",
        "        lpd5.muspy_metrics[0],\n",
        "        lpd5.muspy_metrics[1],\n",
        "        lpd5.muspy_metrics[2],\n",
        "        lpd5.muspy_metrics[3],\n",
        "    ], dtype = float).round(2)\n",
        "    gen_music_metrics = np.array([\n",
        "        gen_abs_mean_diff,\n",
        "        gen_abs_std_diff,\n",
        "        gen_inter_bar_var,\n",
        "        gen_inter_track_var,\n",
        "        gen_empty_bar_ratio,\n",
        "        gen_pitch_classses_per_bar,\n",
        "        gen_qualified_note_ratio,\n",
        "        gen_muspy_metrics[0],\n",
        "        gen_muspy_metrics[1],\n",
        "        gen_muspy_metrics[2],\n",
        "        gen_muspy_metrics[3],\n",
        "    ], dtype = float).round(2)\n",
        "\n",
        "    table_dict = {\n",
        "        \"Metrics\":[\n",
        "            \"Absoluted mean difference\", \n",
        "            \"Absoluted standard deviation difference\", \n",
        "            \"Inter-bar standard deviation\",\n",
        "            \"Inter-track standard deviation\",\n",
        "            \"Empty bar ratio\",\n",
        "            \"Used pitch classes per bar\",\n",
        "            \"Qualified note ratio\",\n",
        "            \"Pitch range\",\n",
        "            \"Polyphony\",\n",
        "            \"Scale consistency\",\n",
        "            \"Empty 1/4 note ratio\",\n",
        "        ],\n",
        "        \"real music\":real_music_metrics,\n",
        "        \"generated music\":gen_music_metrics,\n",
        "        \"Abbreviation\":[\"AMD\", \"ASD\", \"IBS\", \"ITS\", \"EB\", \"UPC\", \"QN\", \n",
        "                        \"PR\", \"PL\", \"SC\", \"EN\",],\n",
        "        \"metric source\":[\"own\", \"own\", \"own\", \"own\", \n",
        "                        \"museGAN\", \"museGAN\", \"museGAN\",\n",
        "                        \"muspy\", \"muspy\", \"muspy\", \"muspy\",],        \n",
        "    }\n",
        "    \n",
        "    table_panda = pd.DataFrame(table_dict)\n",
        "    \n",
        "    print(\"Comparison between the real and the generated music\\n\")\n",
        "    display(table_panda)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ntUausIx9L9q"
      },
      "outputs": [],
      "source": [
        "# Tweak1: Toggle to only `show_best` according to discriminator\n",
        "# Tweak2: `show_real` data instead of generated\n",
        "# Tweak3: Choose `playback_speed` (3 for 4-bar tracks, 1 for 12-bar tracks)\n",
        "# Tweak4: Save samples from a `checkpoint` in a subfolder of that name.\n",
        "def quick_test (generator, discriminator, data, test_size = 100, num_images = 1, \n",
        "                dataset = default_dataset, save_to = None, show_best = False,\n",
        "                show_real = False, playback_speed = 3, checkpoint = 0) : \n",
        "\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "\n",
        "    # Calculating Discriminator predictions\n",
        "    with torch.inference_mode() :    \n",
        "        # Loading real data and models\n",
        "        generator.eval().to(device)\n",
        "        discriminator.eval().to(device)\n",
        "        data_real, labels_real = iter(torch.utils.data.DataLoader(data.dataset,\n",
        "                        batch_size = test_size, \n",
        "                        shuffle = True, drop_last = True)\n",
        "                    ).next()  # make one batch of test_size\n",
        "        data_real   = data_real.to(device)\n",
        "        labels_real = labels_real.to(device)\n",
        "\n",
        "        # Generator\n",
        "        data_generated = generator.forward(batch_size = test_size)\n",
        "        \n",
        "\n",
        "        # Discriminator\n",
        "        music_dis_gen,  genre_dis_gen  = discriminator.forward(data_generated)\n",
        "        music_prob_gen  = torch.sigmoid(music_dis_gen)\n",
        "        std_prob_gen    = torch.std_mean(music_prob_gen, unbiased=True)\n",
        "        \n",
        "        music_dis_real, genre_dis_real = discriminator.forward(data_real)\n",
        "        music_prob_real = torch.sigmoid(music_dis_real)\n",
        "        std_prob_real   = torch.std_mean(music_prob_real, unbiased=True)\n",
        "        \n",
        "        # Converting some generated data to pianorolls\n",
        "        if show_real :\n",
        "            show_data = data_real.cpu().detach().numpy()\n",
        "            probs = music_prob_real.cpu().detach().numpy()\n",
        "        else :\n",
        "            show_data = data_generated.cpu().detach().numpy()\n",
        "            probs = music_prob_gen.cpu().detach().numpy()\n",
        "        if show_best :\n",
        "            best        = np.argpartition(-probs, num_images)\n",
        "            images      = show_data[best]\n",
        "            their_probs = probs[best]\n",
        "        else :\n",
        "            images      = show_data[:num_images]\n",
        "            their_probs = probs[:num_images]\n",
        "        images         = images.reshape(-1, data.width, data.height)\n",
        "        pianorolls     = np.pad(images, ((0, 0), (0, 0), \n",
        "                                        (data.lowest_pitch, \n",
        "                                        128 - data.lowest_pitch - data.height)))   \n",
        "                            # complete the pitch range\n",
        "\n",
        "    # Create audio save folder\n",
        "    default_path = f\"{default_training_path}/{dataset}\"\n",
        "    if save_to == None :\n",
        "        audio_folder = f\"{default_path}/temp_audio\"\n",
        "    else :\n",
        "        if checkpoint == 0 :\n",
        "            subfolder = \"\"\n",
        "        else :\n",
        "            subfolder = f\"/{checkpoint}\"\n",
        "        audio_folder = f\"{default_path}/{save_to}/audio{subfolder}\"\n",
        "    try:   # make new folder\n",
        "        os.makedirs(audio_folder)\n",
        "    except OSError:   # it already exists\n",
        "        pass\n",
        "\n",
        "\n",
        "    # Discriminator Results\n",
        "    print(f\"Discriminator p(x_real = real) = \" +\n",
        "        f\"{std_prob_real[1]*100:.0f}±{std_prob_real[0]*100:.0f}%\")\n",
        "    print(f\"Discriminator p(x_gen = real)  = \" +\n",
        "        f\"{std_prob_gen[1]*100:.0f}±{std_prob_gen[0]*100:.0f}%\")\n",
        "    print(\"\\n\\n\")\n",
        "    \n",
        "    # Generator examples\n",
        "    if show_real :\n",
        "        print(\"Example of the real music\")\n",
        "    else :\n",
        "        print(\"Example of the generated music\")\n",
        "    print(f\"saved under '{audio_folder}'\")\n",
        "    for i in range(num_images) :\n",
        "        beat_resolution = playback_speed * data.blips_per_beat // 3 \n",
        "        piano_music = muspy.from_pianoroll_representation(pianorolls[i] > 0,\n",
        "                        resolution = beat_resolution, \n",
        "                        encode_velocity = False)   # convert to muspy.music_object\n",
        "        \n",
        "        # save audio tracks\n",
        "        timestamp     = datetime.now()\n",
        "        audiopath     = f\"{audio_folder}/{timestamp}.wav\"\n",
        "        pianorollpath = f\"{audio_folder}/{timestamp}.npy\"\n",
        "        muspy.write_audio(path = audiopath, music = piano_music)\n",
        "        np.save(pianorollpath, pianorolls[i])\n",
        "\n",
        "        # Display example pianorolls with audio\n",
        "        kind = 'real'  if show_real else  'gen'\n",
        "        print(\"\")\n",
        "        print(f\"p(x_{kind} = real)  = {their_probs[i]*100:.2f}%\")\n",
        "        print(f\"file: {timestamp}.wav\")\n",
        "        \n",
        "        display(Audio(filename = audiopath))\n",
        "        muspy.visualization.show_pianoroll(piano_music)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "def numpify (tensor) :\n",
        "    return tensor.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def music_stats (dis_output) :\n",
        "    stats               = {}\n",
        "    stats['batch_size'] = dis_output.shape[0]\n",
        "    stats['dis_out']    = dis_output\n",
        "    stats['prob']       = torch.sigmoid(stats['dis_out'])\n",
        "    stats['std'], stats['mean'] \\\n",
        "                              = torch.std_mean(stats['prob'], unbiased=True)\n",
        "    stats['stat_err']   = stats['std'] \\\n",
        "                                / np.sqrt(stats['batch_size'])\n",
        "    stats['decision']   = torch.mean((stats['prob'] >= 0.5).float())\n",
        "    \n",
        "    for key, value in stats.items() :\n",
        "        if type(value) == torch.Tensor :\n",
        "            stats[key] = numpify(value)\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def print_music_performance (stats, mode, mode_long) :\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f\"Music discriminator performance on {mode_long} data\")\n",
        "    print(\"--------------------------------------------\")\n",
        "    print(\"\")\n",
        "    print(f\"{mode} size : {stats['dis_out'].shape[0]}\")\n",
        "    print(f\"mean[ p(x_{mode} = real) ]          : {stats['mean']*100:.2f} ± {stats['stat_err']*100:.2f} %\")\n",
        "    print(f\"std[ p(x_{mode} = real) ]           : {stats['std']*100:.2f} %\")\n",
        "    print(f\"accuracy: p(x_{mode} = real) >= 50% : {stats['decision']*100:.2f} %\")\n",
        "    print(\"\")\n",
        "    \n",
        "\n",
        "def confusion_matrix (true_labels, predicted_labels, n_labels, \n",
        "                      unknown_label = False) :\n",
        "    true_labels      = numpify(true_labels)\n",
        "    predicted_labels = numpify(predicted_labels)\n",
        "    \n",
        "    n_plabels = n_labels + unknown_label\n",
        "    matrix    = np.zeros((n_labels, n_plabels))\n",
        "    for i, true_label in enumerate(range(n_labels)) :\n",
        "        this_label_mask = true_labels == true_label\n",
        "        n_true          = np.sum(this_label_mask)\n",
        "        predictions_for_this_label = predicted_labels[this_label_mask] # Selects predictions for one specific label\n",
        "        prediction_distribution    = [np.mean(predictions_for_this_label == pred_label)  \n",
        "                                      for pred_label in range(n_plabels)]\n",
        "        matrix[i]                  = np.array(prediction_distribution)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def genre_stats (dis_output, dis_labels) :\n",
        "    stats               = {}\n",
        "    stats['batch_size'] = dis_output.shape[0]\n",
        "    stats['dis_out']    = dis_output\n",
        "    stats['label']      = dis_labels\n",
        "    \n",
        "    # probabilities\n",
        "    stats['logprob']    = torch.log_softmax(stats['dis_out'], dim = 1)\n",
        "    stats['prob']       = torch.exp(stats['logprob'])\n",
        "    \n",
        "    # max probability labels\n",
        "    stats['m_prob'], stats['m_choice'] \\\n",
        "                        = torch.max(stats['prob'], dim = 1) \n",
        "    stats['m_std'], stats['m_mean'] \\\n",
        "                        = torch.std_mean(stats['m_prob'], unbiased=True) \n",
        "    stats['m_stat_err'] = stats['m_std'] \\\n",
        "                                / np.sqrt(stats['batch_size'])\n",
        "    \n",
        "    # confusion matrix\n",
        "    stats['right']      = stats['m_choice'] == stats['label']\n",
        "    stats['accuracy']   = torch.mean(stats['right'].float())\n",
        "    stats['decision']   = torch.mean((stats['m_prob'] > 0.5).float())\n",
        "    stats['choice']     = torch.where(stats['m_prob'] > 0.5, \n",
        "                                            stats['m_choice'], # = most prob. label\n",
        "                                            lpd5.n_labels)  # = unknown label\n",
        "    stats['cf_matrix']  = confusion_matrix(stats['label'], \n",
        "                                                 stats['m_choice'], \n",
        "                                                 lpd5.n_labels,\n",
        "                                                 unknown_label = False)\n",
        "    \n",
        "    # accuracy\n",
        "    stats['r_prob']     = stats['prob'][:, stats['label']]\n",
        "    stats['r_std'], stats['r_mean'] \\\n",
        "                        = torch.std_mean(stats['r_prob'], unbiased=True) \n",
        "    stats['r_stat_err'] = stats['r_std'] \\\n",
        "                                / np.sqrt(stats['batch_size'])\n",
        "    \n",
        "    # entropy\n",
        "    stats['entropy']    = torch.sum(- stats['prob'] \n",
        "                                          * stats['logprob'], dim = 1)\n",
        "    stats['e_std'], stats['e_mean'] \\\n",
        "                        = torch.std_mean(stats['entropy'], unbiased=True) \n",
        "    stats['e_stat_err'] = stats['e_std'] \\\n",
        "                                / np.sqrt(stats['batch_size'])\n",
        "    \n",
        "    # return\n",
        "    for key, value in stats.items() :\n",
        "        if type(value) == torch.Tensor :\n",
        "            stats[key] = numpify(value)\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def print_genre_performance (stats, mode, mode_long) :\n",
        "    print(f\"\"\" \n",
        "    \n",
        "    Genre discriminator performance on {mode_long} data\n",
        "    --------------------------------------------\n",
        "\n",
        "    {mode} size : {stats['dis_out'].shape[0]}\n",
        "\n",
        "    Discriminator accuracy:\n",
        "    mean[ predicted_label == real_label ] : {stats['r_mean']*100:.2f} ± {stats['r_stat_err']*100:.2f} %\n",
        "    std[ predicted_label == real_label ]  : {stats['r_std']*100:.2f} %\n",
        "    accuracy: mean[ argmax[p(predicted_label)] == real_label ]   : {stats['accuracy']*100:.2f} %\n",
        "    \n",
        "    Discriminator confidence:\n",
        "    mean[ max[p(predicted_label)] ] : {stats['m_mean']*100:.2f} ± {stats['m_stat_err']*100:.2f} %\n",
        "    std[ max[p(predicted_label)] ] : {stats['m_std']*100:.2f} %\n",
        "    \n",
        "    Discriminator entropy\n",
        "    mean[ entropy[p(predicted_label)] ] : {stats['e_mean']*100:.2f} ± {stats['e_stat_err']*100:.2f} %\n",
        "    std[ entropy[p(predicted_label)] ] : {stats['e_std']*100:.2f} %\n",
        "    \"\"\")\n",
        "\n",
        "    ### --- code adapted from [1] --- ###\n",
        "    cf_matrix_table = pd.DataFrame(stats['cf_matrix'], \n",
        "                                   index   = [i for i in lpd5.genre_list],\n",
        "                                   columns = [i for i in lpd5.genre_list])# + [\"unknown\"]])\n",
        "    plt.figure(figsize = (12,7))\n",
        "    sn.heatmap(cf_matrix_table , annot=True)\n",
        "    ### --- --------------------- --- ###\n",
        "\n",
        "\n",
        "\n",
        "def CAN_test (generator, discriminator) :\n",
        "    device    = get_device()\n",
        "    test_size = len(lpd5.labels_test)\n",
        "    batch_size = 100\n",
        "\n",
        "    # Calculating Discriminator predictions\n",
        "    with torch.inference_mode() :    \n",
        "        # Loading models as well as training and test data\n",
        "        generator.eval().to(device)\n",
        "        discriminator.eval().to(device)\n",
        "        \n",
        "        # Calculate Test performance\n",
        "        dataloader = iter(torch.utils.data.DataLoader(lpd5.dataset_test,\n",
        "                        batch_size = batch_size, \n",
        "                        shuffle = True, drop_last = False))\n",
        "        music_dis = torch.zeros(test_size).to(device)\n",
        "        genre_dis = torch.zeros((test_size, lpd5.n_labels)).to(device)\n",
        "        labels_dis = torch.zeros(test_size, dtype = torch.int64).to(device)\n",
        "\n",
        "        n_batches = math.ceil(test_size / batch_size)\n",
        "        for i in range(n_batches) :\n",
        "            print(f\"\\rRunning test batch {i+1} / {n_batches}.\", end = \"\")\n",
        "            data, labels = dataloader.next()\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            music_batch_dis, genre_batch_dis = discriminator.forward(data)\n",
        "            batch_slice = np.s_[i*batch_size:min((i+1)*batch_size, test_size)]\n",
        "            music_dis[batch_slice] = music_batch_dis\n",
        "            genre_dis[batch_slice] = genre_batch_dis\n",
        "            labels_dis[batch_slice] = labels\n",
        "\n",
        "        print('\\r')\n",
        "        music_test = music_stats(music_dis)\n",
        "        print_music_performance(music_test, \"test\", \"test\")\n",
        "        genre_test = genre_stats(genre_dis, labels_dis)\n",
        "        print_genre_performance(genre_test, \"test\", \"test\")\n",
        "        \n",
        "\n",
        "        # Calculate Training performance\n",
        "        indices = np.random.permutation(np.arange(lpd5.dataset_size))[:test_size]\n",
        "        datasubset = torch.utils.data.Subset(lpd5.dataset, indices)\n",
        "        dataloader = iter(torch.utils.data.DataLoader(datasubset,\n",
        "                          batch_size = batch_size, \n",
        "                          shuffle = True, drop_last = False))\n",
        "        music_dis = torch.zeros(test_size).to(device)\n",
        "        genre_dis = torch.zeros((test_size, lpd5.n_labels)).to(device)\n",
        "        labels_dis = torch.zeros(test_size, dtype = torch.int64).to(device)\n",
        "\n",
        "        n_batches = math.ceil(test_size / batch_size)\n",
        "        for i in range(n_batches) :\n",
        "            print(f\"\\rRunning training batch {i+1} / {n_batches}.\", end = \"\")\n",
        "            data, labels = dataloader.next()\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            music_batch_dis, genre_batch_dis = discriminator.forward(data)\n",
        "            batch_slice = np.s_[i*batch_size:min((i+1)*batch_size, test_size)]\n",
        "            music_dis[batch_slice] = music_batch_dis\n",
        "            genre_dis[batch_slice] = genre_batch_dis\n",
        "            labels_dis[batch_slice] = labels\n",
        "\n",
        "        print('\\r')\n",
        "        music_train = music_stats(music_dis)\n",
        "        print_music_performance(music_train, \"train\", \"training\")\n",
        "        genre_train = genre_stats(genre_dis, labels_dis)\n",
        "        print_genre_performance(genre_train, \"train\", \"training\")\n",
        "\n",
        "\n",
        "        # Calculate Generated performance\n",
        "        music_dis = torch.zeros(test_size).to(device)\n",
        "        genre_dis = torch.zeros((test_size, lpd5.n_labels)).to(device)\n",
        "        \n",
        "        for i in range(math.ceil(test_size / batch_size)) :\n",
        "            print(f\"\\rRunning generator batch {i+1} / {n_batches}.\", end = \"\")\n",
        "            gen_batch_size = min(batch_size, test_size - i*batch_size)\n",
        "            data = generator.forward(gen_batch_size)\n",
        "            \n",
        "            music_batch_dis, genre_batch_dis = discriminator.forward(data)\n",
        "            batch_slice = np.s_[i*batch_size:min((i+1)*batch_size, test_size)]\n",
        "            music_dis[batch_slice] = music_batch_dis\n",
        "            genre_dis[batch_slice] = genre_batch_dis\n",
        "    \n",
        "        print('\\r')\n",
        "        music_gen = music_stats(music_dis)\n",
        "        print_music_performance(music_gen, \"gen\", \"generated\") \n",
        "        genre_gen = genre_stats(genre_dis, labels_dis)\n",
        "        print_genre_performance(genre_gen, \"gen\", \"generated\") \n",
        "\n",
        "\n",
        "\n",
        "# Sources\n",
        "# [1] https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7"
      ],
      "metadata": {
        "id": "V_t88a045PDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4rFGcgSMO7j"
      },
      "source": [
        "#### Save and load trained models and logs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device () :\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "    return torch.device(device)"
      ],
      "metadata": {
        "id": "hksd094i0lBt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_train_state (trainer, round) :\n",
        "\n",
        "    # dictionary to be saved\n",
        "    train_state = {\n",
        "        'info'  : trainer.info,\n",
        "        'gen'   : trainer.gen.state_dict(),\n",
        "        'dis'   : trainer.dis.state_dict(),\n",
        "        'opt_g' : trainer.optimizer_gen.state_dict(), \n",
        "        'opt_d' : trainer.optimizer_dis.state_dict(), \n",
        "        'log'   : trainer.log.__dict__,\n",
        "    }\n",
        "    \n",
        "    # creating the folder path\n",
        "    if trainer.training_folder == \"\" :\n",
        "        now       = datetime.now()\n",
        "        timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        training_folder = f\"{timestamp}_{trainer.training_name}\"\n",
        "    else :\n",
        "        training_folder = trainer.training_folder\n",
        "    folder_path = f\"{default_training_path}/{default_dataset}/\" \\\n",
        "                + f\"{training_folder}\"\n",
        "    file_path   = f\"{folder_path}/train_state{round}.pth\"\n",
        "\n",
        "    # saving the train_state\n",
        "    try :  os.makedirs(folder_path) # make new folder\n",
        "    except OSError :  pass          # it already exists        \n",
        "    torch.save(train_state, file_path)\n",
        "\n",
        "    return training_folder"
      ],
      "metadata": {
        "id": "st0Lr1LDurXx"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_train_state (training_folder, round) :\n",
        "    \n",
        "    load_path = f\"{default_training_path}/{default_dataset}/\" \\\n",
        "              + f\"{training_folder}/train_state{round}.pth\"\n",
        "    if os.path.isfile(load_path) :\n",
        "        train_state = torch.load(load_path, map_location = get_device())\n",
        "    else :\n",
        "        train_state = None\n",
        "        print(f\"\\nNo train state found at\\n'{load_path}'!\\n\")\n",
        "\n",
        "    return train_state"
      ],
      "metadata": {
        "id": "GKyx1K5_5GkR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_CAN (training_folder, round, print_info = False,\n",
        "              gen_class = MusiGenMod2, dis_class = MusiDisMod2b) :\n",
        "\n",
        "    train_state = load_train_state(training_folder, round)\n",
        "    device      = get_device()\n",
        "\n",
        "    gen, dis = gen_class(), dis_class()\n",
        "    gen.load_state_dict(train_state['gen'])\n",
        "    dis.load_state_dict(train_state['dis'])\n",
        "    gen.to(device), dis.to(device)\n",
        "    gen.eval()    , dis.eval()\n",
        "\n",
        "    log_dict = train_state[\"log\"]\n",
        "    for key, value in log_dict.items() :\n",
        "        if type(value) == np.ndarray :\n",
        "            log_dict[key] = value[:, :round]\n",
        "    log = LogLoaded(log_dict)\n",
        "    \n",
        "\n",
        "    if print_info :\n",
        "        print(train_state[\"info\"])\n",
        "\n",
        "    return gen, dis, log"
      ],
      "metadata": {
        "id": "leKK9pCayN5u"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "m92AAeQbMMl1"
      },
      "outputs": [],
      "source": [
        "def save_training (training_name, trainer, info_txt = None, \n",
        "                   dataset = default_dataset, new_folder = True, \n",
        "                   checkpoint = 0) :\n",
        "    assert  type(training_name) == str               \n",
        "    assert  info_txt == None  or  type(info_txt) == str\n",
        "    \n",
        "    # If trainer has already saved a checkpoint, no new folder is needed.\n",
        "    if hasattr(trainer, \"training_name\") :\n",
        "        if (training_name == trainer.training_name  and\n",
        "            trainer.training_folder != \"\") :\n",
        "            \n",
        "            training_name = trainer.training_folder\n",
        "            new_folder    = False\n",
        "    \n",
        "    # Name the save folder\n",
        "    if new_folder:\n",
        "        now             = datetime.now()\n",
        "        date            = f\"{now.year}-{now.month:02d}-{now.day:02d}\"\n",
        "        time            = f\"{now.hour:02d}-{now.minute:02d}\"\n",
        "        timestamp       = f\"{date}_{time}\"\n",
        "        training_folder = f\"{timestamp}_{training_name}\"\n",
        "    else :\n",
        "        training_folder = training_name\n",
        "    save_folder = f\"{default_training_path}/{dataset}/{training_folder}\"\n",
        "    \n",
        "    model_folder = f\"{save_folder}/model\"\n",
        "    try:   # make new folder\n",
        "        os.makedirs(model_folder)\n",
        "    except OSError:   # it already exists\n",
        "        pass\n",
        "    \n",
        "\n",
        "    # save models\n",
        "    gen = trainer.gen\n",
        "    dis = trainer.dis\n",
        "    \n",
        "    if checkpoint == 0 :\n",
        "        torch.save(gen.state_dict(), f\"{model_folder}/gen.pt\")\n",
        "        torch.save(dis.state_dict(), f\"{model_folder}/dis.pt\")\n",
        "    else : \n",
        "        # here, checkpoint is an int: the current training round number\n",
        "        torch.save(gen.state_dict(), f\"{model_folder}/gen{checkpoint}.pt\")\n",
        "        torch.save(dis.state_dict(), f\"{model_folder}/dis{checkpoint}.pt\")\n",
        "\n",
        "    # save logs\n",
        "    if checkpoint == 0 :\n",
        "        log_file = f\"{save_folder}/logs.npz\"\n",
        "        log_dict = trainer.log.__dict__\n",
        "    else :\n",
        "        # here, checkpoint is an int: the current training round number\n",
        "        total_rounds = checkpoint\n",
        "        log_file = f\"{save_folder}/logs{checkpoint}.npz\"\n",
        "        log_dict = trainer.log.__dict__.copy()\n",
        "        # shorten the log arrays to current checkpoint \n",
        "        for key, value in log_dict.items() :\n",
        "            if type(value) == np.ndarray :\n",
        "                log_dict[key] = value[:, :total_rounds]\n",
        "    \n",
        "    np.savez(log_file, **log_dict)     \n",
        "    \n",
        "\n",
        "    # save additional info about training\n",
        "    info_path  = f\"{save_folder}/info.txt\"\n",
        "    and_info   = \"\"\n",
        "    if info_txt != None :\n",
        "        with open(info_path, \"w+\") as f :\n",
        "            f.writelines(info_txt)\n",
        "        and_info = \"and info text \"  \n",
        "    \n",
        "    if checkpoint == 0:\n",
        "        print(f\"Saved models {and_info}under:\\n\",\n",
        "            f\"'{default_training_path}/{dataset}/\\n\",\n",
        "            f\" {training_folder}'\")\n",
        "        \n",
        "    return training_folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "FK6Iwny8CvsL"
      },
      "outputs": [],
      "source": [
        "def load_training (training_folder, model = (MusiGen, MusiDis), \n",
        "                   print_info = False, dataset = default_dataset, \n",
        "                   checkpoint = 0) :\n",
        "    save_folder = f\"{default_training_path}/{dataset}/{training_folder}\"\n",
        "    assert  os.path.exists(save_folder)\n",
        "\n",
        "    # load models\n",
        "    model_folder       = f\"{save_folder}/model\"\n",
        "    GenClass, DisClass = model\n",
        "    gen, dis           = GenClass(), DisClass()\n",
        "    cp = \"\"  if checkpoint == 0 else  f\"{checkpoint}\"   # here, checkpoint is an int: the current training round number\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    gen.load_state_dict(torch.load(f\"{model_folder}/gen{cp}.pt\", \n",
        "                                   map_location = device))\n",
        "    dis.load_state_dict(torch.load(f\"{model_folder}/dis{cp}.pt\",\n",
        "                                   map_location = device))\n",
        "    \n",
        "\n",
        "    # Prepare models for evaluation\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "    gen    = gen.to(device)\n",
        "    dis    = dis.to(device)\n",
        "    gen.eval()\n",
        "    dis.eval()\n",
        "\n",
        "    # load logs\n",
        "    if checkpoint == 0 :\n",
        "        log_file = f\"{save_folder}/logs.npz\"\n",
        "    else :\n",
        "        log_file = f\"{save_folder}/logs{checkpoint}.npz\"\n",
        "\n",
        "    logs = None\n",
        "    with np.load(log_file) as log_dict :\n",
        "        logs = LogLoaded(log_dict)\n",
        "    \n",
        "    # load info\n",
        "    info_path  = f\"{save_folder}/info.txt\"\n",
        "    if print_info :\n",
        "        with open(info_path, \"r\") as f :\n",
        "            print(f.read())\n",
        "    \n",
        "    return gen, dis, logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7J679rwSfnC"
      },
      "source": [
        "## Network training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6uuHHR6fQOe"
      },
      "source": [
        "### Main Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "2eBKbfRJbh6D"
      },
      "outputs": [],
      "source": [
        "training_name = \"16k-wcan-gp-higher-lr\"\n",
        "summary       = \\\n",
        "f\"\"\"\n",
        "    Training info: {training_name}\n",
        "    =======================\n",
        "\n",
        "    models: MusiGenMod2, MusiDisMod2b\n",
        "    dataset: datacombi_1\n",
        "\n",
        "    rounds = 16000\n",
        "    batch_size = 25\n",
        "    discriminator_rounds = 5\n",
        "    loss_function = WCAN-GP\n",
        "    checkpoints   = [2000, 4000, 6000, 8000, 10_000, 12_000, 14_000]\n",
        "\n",
        "    adam_optimizer_params:\n",
        "        dis: (lr = 0.001, betas = (0.5, 0.9))\n",
        "        gen: (lr = 0.001, betas = (0.5, 0.9))\n",
        "    genre_ambiguity:\n",
        "        dis: 1\n",
        "        gen: 1\n",
        "    optional normalizations:\n",
        "        music output: ON\n",
        "        genre output: ON\n",
        "\n",
        "\n",
        "    additional comments:\n",
        "        \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gasGW3KpSfnM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "578e25df07f94b5a91fe8d1bb0b87c39",
            "5164fd416aa4427cab20ca470c639cdf",
            "9a24ac45b12e42798a4c38e48db30469",
            "d6a4d81f70d9428d984d0322f5f0ef07",
            "8cd999fe0bab4b5fa5201603e87f989c",
            "ca8941c4d4b54600ace6a9ce8ebf0f50",
            "e7331d4d2c4a4749bdb59dae43434f22",
            "41cb45507ab54d9294f270090c9cff5c",
            "a2a43473bd454ad797f3d5bfd25cc4ec",
            "69cd5a0325a84d6e9a3a98275cf0542a",
            "055bf1b3adfb479c8f0bc4b42bc481ad"
          ]
        },
        "outputId": "2355c567-5f49-47d7-9bc2-44cc750a7d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/16000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "578e25df07f94b5a91fe8d1bb0b87c39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint 1000 under '2022-09-20_11-25-44_16k-wcan-gp-higher-lr'.\n",
            "Saved checkpoint 2000.\n",
            "Saved checkpoint 3000.\n",
            "Saved checkpoint 4000.\n",
            "Round 4955"
          ]
        }
      ],
      "source": [
        "lpd5Train     = GANTraining(MusiGenMod2, MusiDisMod2b, lpd5.dataset)\n",
        "lpd5Train.setup(16000, batch_size = 25, discriminator_rounds = 5, \n",
        "                loss_function = \"WCAN-GP\", info_text = summary, \n",
        "                norm_dis_probs = True, norm_dis_genre = True, \n",
        "                genre_ambiguity = (1, 1), learning_rates = (1e-3, 1e-3))\n",
        "#lpd5Train.resume(\"2022-09-18_20-43-31_16k-long-loss-test\", 12000)\n",
        "lpd5Train.set_backups(training_name, checkpoints = list(range(0, 16_000, 1000)),\n",
        "                      save_folder = \"\")\n",
        "lpd5Train.train()\n",
        "\n",
        "\n",
        "\"Hi there *:>  This test ought to get very useful!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47lzP9FrGRaI"
      },
      "outputs": [],
      "source": [
        "training_folder_name = save_training(training_name, lpd5Train, info_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXwYSkWt3oRg"
      },
      "outputs": [],
      "source": [
        "plot_training(lpd5Train.log, CAN = True, show_loss_terms = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd-LOltQSfnN"
      },
      "outputs": [],
      "source": [
        "long_test(lpd5Train.gen, lpd5Train.dis, lpd5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlOBH55thlJQ"
      },
      "outputs": [],
      "source": [
        "quick_test(lpd5Train.gen, lpd5Train.dis, lpd5, num_images = 5, \n",
        "           save_to = training_folder_name, playback_speed = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb-NJGprfWGK"
      },
      "source": [
        "### Loading and investigating old trained models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_folder = \"2022-09-17_16-50-04_4k-does-norm-work\"\n",
        "gen, dis, log = load_CAN(training_folder, 1000)"
      ],
      "metadata": {
        "id": "PJGlxDXep2P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(log, CAN = True, show_loss_terms = True)"
      ],
      "metadata": {
        "id": "jS9mUvPA3fD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t0WZhyTn4-t3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8ATBcagDnDpM",
        "trwcbyaoSfm2",
        "3tnu9HElSfm5",
        "KdGO-KlvSfm5",
        "kU1bKYfdSfm6",
        "vG8eX-HGSfm-",
        "L_lrgdWMSrIk",
        "xS0kN-LMSzV0",
        "6ejVE_JNS2VE",
        "ixzSjpbOS64f",
        "k4rFGcgSMO7j",
        "zb-NJGprfWGK"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('aml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e9db57278ae8397bc5fe3bec6b9ba53c33a2aa76e79d386f678fc754e34f9547"
      }
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "578e25df07f94b5a91fe8d1bb0b87c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5164fd416aa4427cab20ca470c639cdf",
              "IPY_MODEL_9a24ac45b12e42798a4c38e48db30469",
              "IPY_MODEL_d6a4d81f70d9428d984d0322f5f0ef07"
            ],
            "layout": "IPY_MODEL_8cd999fe0bab4b5fa5201603e87f989c"
          }
        },
        "5164fd416aa4427cab20ca470c639cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8941c4d4b54600ace6a9ce8ebf0f50",
            "placeholder": "​",
            "style": "IPY_MODEL_e7331d4d2c4a4749bdb59dae43434f22",
            "value": " 31%"
          }
        },
        "9a24ac45b12e42798a4c38e48db30469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41cb45507ab54d9294f270090c9cff5c",
            "max": 16000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2a43473bd454ad797f3d5bfd25cc4ec",
            "value": 4955
          }
        },
        "d6a4d81f70d9428d984d0322f5f0ef07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69cd5a0325a84d6e9a3a98275cf0542a",
            "placeholder": "​",
            "style": "IPY_MODEL_055bf1b3adfb479c8f0bc4b42bc481ad",
            "value": " 4955/16000 [2:14:09&lt;4:58:58,  1.62s/it]"
          }
        },
        "8cd999fe0bab4b5fa5201603e87f989c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca8941c4d4b54600ace6a9ce8ebf0f50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7331d4d2c4a4749bdb59dae43434f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41cb45507ab54d9294f270090c9cff5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a43473bd454ad797f3d5bfd25cc4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69cd5a0325a84d6e9a3a98275cf0542a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "055bf1b3adfb479c8f0bc4b42bc481ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}