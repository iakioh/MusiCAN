{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iakioh/MusiCAN/blob/main/models/musiCAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ATBcagDnDpM"
      },
      "source": [
        "### Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjvf82O3S2Yz"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCaQQJb6TXgV"
      },
      "outputs": [],
      "source": [
        "# Go to this notebook's directory\n",
        "repo_path = \"/content/drive/MyDrive/MusiCAN/\"\n",
        "%cd {repo_path}/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvGZ0ptpZQNH"
      },
      "outputs": [],
      "source": [
        "# Check GPU connection\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7_ny_A1Z6L9"
      },
      "outputs": [],
      "source": [
        "# Check RAM access\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnLdyhmuzL2Z"
      },
      "outputs": [],
      "source": [
        "# Install muspy related code\n",
        "!pip install muspy\n",
        "import muspy\n",
        "muspy.download_musescore_soundfont() \n",
        "muspy.download_bravura_font() \n",
        "\n",
        "# Install fluidsynth related code\n",
        "!apt install fluidsynth\n",
        "!pip install pyfluidsynth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgRRhU6SSfmz"
      },
      "source": [
        "# musiCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zy36G-CGamF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "from tqdm import notebook\n",
        "from datetime import datetime\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.style.use('tableau-colorblind10')\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy import ndimage\n",
        "\n",
        "import muspy\n",
        "import fluidsynth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trwcbyaoSfm2"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePkslq2wSfm4"
      },
      "outputs": [],
      "source": [
        "class Pianoroll :\n",
        "    def __init__ (self, filepath, bars, lowest_pitch, genre_list) :\n",
        "        assert  type(filepath) == str\n",
        "\n",
        "        # Creating the dataset from a file\n",
        "        stored_data  = np.load(filepath)\n",
        "        data_array   = stored_data[\"data\"]\n",
        "        labels_array = stored_data[\"labels\"]\n",
        "\n",
        "        ## Split dataset into training and test parts\n",
        "        np.random.seed(seed=902741)\n",
        "        perm         = np.random.permutation(len(labels_array))\n",
        "        data_array   = data_array[perm]\n",
        "        labels_array = labels_array[perm]\n",
        "        split_line   = len(labels_array) // 11\n",
        "        data_test, data     = np.array_split(data_array, [split_line], axis = 0)\n",
        "        labels_test, labels = np.array_split(labels_array, [split_line], axis = 0)\n",
        "\n",
        "        # Making a torch dataset\n",
        "        self.data   = torch.as_tensor(data, dtype = torch.float32)\n",
        "        self.labels = torch.as_tensor(labels, dtype = torch.int64)\n",
        "        self.data_test   = torch.as_tensor(data_test, dtype = torch.float32)\n",
        "        self.labels_test = torch.as_tensor(labels_test, dtype = torch.int64)\n",
        "\n",
        "        self.dataset      = torch.utils.data.TensorDataset(self.data, \n",
        "                                                           self.labels)\n",
        "        self.dataset_test = torch.utils.data.TensorDataset(self.data_test, \n",
        "                                                           self.labels_test)\n",
        "\n",
        "        # Storing additional info about it\n",
        "        self.shape  = tuple(self.data.shape[1:])   # shape of one pianoroll image\n",
        "        self.size   = self.shape[0] * self.shape[1]\n",
        "        self.height       = self.data.shape[2]\n",
        "        self.width        = self.data.shape[1]\n",
        "        self.dataset_size = self.data.shape[0]\n",
        "        self.test_size    = self.data_test.shape[0]\n",
        "\n",
        "        self.bars         = bars\n",
        "        self.lowest_pitch = lowest_pitch\n",
        "        self.genre_list   = genre_list\n",
        "\n",
        "        self.blips_per_bar  = self.width // self.bars\n",
        "        self.blips_per_beat = self.blips_per_bar // 4\n",
        "        self.pitches        = self.height\n",
        "        self.octaves        = self.pitches // 12\n",
        "        self.n_labels       = len(self.genre_list)\n",
        "\n",
        "    \n",
        "    def show (self, number = None) :\n",
        "        if number == None :\n",
        "            number = np.random.randint(self.dataset_size)\n",
        "        else :\n",
        "            assert  type(number) == int\n",
        "            assert  number >= 0 and number < self.dataset_size\n",
        "\n",
        "        plt.figure(figsize = (12, 6))\n",
        "        plt.title(f\"pianoroll #{number}\")\n",
        "        plt.imshow(self.data[number].T)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5pWodkypE8U"
      },
      "source": [
        "### LPD5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH4y0TdiRjuD"
      },
      "outputs": [],
      "source": [
        "default_training_path = \"../experiments\"\n",
        "\n",
        "#default_dataset       = \"lpd5_full_4bars\"\n",
        "default_dataset       = \"datacombi_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCEcB4f4pE8V"
      },
      "outputs": [],
      "source": [
        "lpd5_path = \"../experiments/lpd5_full_4bars/prepared_arrays.npz\"\n",
        "lpd5_bars = 4\n",
        "lpd5_lowest_pitch = 24\n",
        "lpd5_genre_list = ['Rap', 'Latin', 'International', 'Electronic', \n",
        "                   'Country', 'Folk', 'Blues', 'Reggae', 'Jazz',\n",
        "                   'Vocal', 'New-Age', 'RnB', 'Pop_Rock']\n",
        "#lpd5 = Pianoroll(lpd5_path, lpd5_bars, lpd5_lowest_pitch, lpd5_genre_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-k4JCCXOIXS"
      },
      "outputs": [],
      "source": [
        "dc1_path = \"../experiments/datacombi_1/prepared_arrays.npz\"\n",
        "dc1_bars = 4 # actually 12 but 4 for gen and dis compatibiliy reasons.\n",
        "dc1_lowest_pitch = 24\n",
        "dc1_genre_list   = ['Latin', 'Electronic', 'Country', 'RnB', 'Pop_Rock', 'Classical', 'Game']\n",
        "\n",
        "lpd5 = Pianoroll(dc1_path, dc1_bars, dc1_lowest_pitch, dc1_genre_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpd5.show()"
      ],
      "metadata": {
        "id": "Eba7NctgrmY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpd5.dataset_size\", lpd5.dataset_size)\n",
        "print(\"lpd5.test_size\", lpd5.test_size)\n",
        "print(\"lpd5.shape\", lpd5.shape)"
      ],
      "metadata": {
        "id": "UvXOK3nnrnkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpd5.bars\", lpd5.bars)\n",
        "print(\"lpd5.blips_per_bar\", lpd5.blips_per_bar)\n",
        "print(\"lpd5.pitches\", lpd5.pitches)\n",
        "print(\"lpd5.octaves\", lpd5.octaves)"
      ],
      "metadata": {
        "id": "p3OZmtvgrqod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lpd5.n_labels\", lpd5.n_labels)\n",
        "print(\"lpd5.genre_list\", lpd5.genre_list)"
      ],
      "metadata": {
        "id": "Ruxd7HGKrsdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15, 5))\n",
        "plt.suptitle(\"Relative genre frequencies in the dataset\")\n",
        "x_pos = np.arange(lpd5.n_labels)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Training data\")\n",
        "label_freqs = np.array([np.mean(lpd5.labels.numpy() == i) \n",
        "                        for i in range(lpd5.n_labels)])\n",
        "plt.bar(x_pos, label_freqs, tick_label = lpd5.genre_list)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test data\")\n",
        "label_freqs_test = np.array([np.mean(lpd5.labels_test.numpy() == i) \n",
        "                             for i in range(lpd5.n_labels)])\n",
        "plt.bar(np.arange(lpd5.n_labels), label_freqs_test, tick_label = lpd5.genre_list)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PySc5NmZrwvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rel_diff = label_freqs / label_freqs_test - 1\n",
        "print(\"Relative difference in genre frequencies of test compared to training data:\")\n",
        "for diff, genre in zip(rel_diff, lpd5.genre_list) :\n",
        "    print(f\"{genre:10} : {diff*100:4.1f}%\")"
      ],
      "metadata": {
        "id": "7TfnAP0Zr12U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tnu9HElSfm5"
      },
      "source": [
        "## Architecture classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdGO-KlvSfm5"
      },
      "source": [
        "### Support classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acpAO9NSSfm5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    These two classes serves as torch layers to binarize the output of the Generator while keeping the layer still \"backpropagatable\" (via a hardtanh).\n",
        "    This is not our own code. For source, see:\n",
        "    https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html#:~:text=A%20straight%2Dthrough%20estimator%20is,function%20was%20an%20identity%20function.\n",
        "\"\"\"\n",
        "\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0.5).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return torch.nn.functional.hardtanh(grad_output)\n",
        "\n",
        "class StraightThroughEstimator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StraightThroughEstimator, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # only binarize in eval() mode, not in training\n",
        "        x = x  if self.training  else  STEFunction.apply(x)\n",
        "        #x = STEFunction.apply(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Tgofn5pfRU"
      },
      "outputs": [],
      "source": [
        "class GeneratorBlock(torch.nn.Module):\n",
        "    \"\"\" 2d transconv layer, batch normalization & ReLU \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gen_block = torch.nn.Sequential(\n",
        "            torch.nn.ConvTranspose2d(in_dim, out_dim, kernel, stride),\n",
        "            torch.nn.BatchNorm2d(out_dim),\n",
        "            torch.nn.ReLU()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen_block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n38oto6cpfRW"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorBlock(torch.nn.Module):\n",
        "    \"\"\"3d conv layer & Leaky ReLU\"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
        "        super().__init__()\n",
        "        self.dis_block = torch.nn.Sequential(\n",
        "            torch.nn.Conv3d(in_dim, out_dim, kernel, stride),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2)   # MuseGAN Hyperparameter\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dis_block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU1bKYfdSfm6"
      },
      "source": [
        "### Main neural network classes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiGen (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 64\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(64, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 3, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 128, 1024, (2, 1), (2, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 256,  256, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 256,  128, (3, 1), (3, 1)),\n",
        "            GeneratorBlock( 128,   64, (1, self.octaves), (1, self.octaves)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (1, 12), (1, 12)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "        \n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward_custom (self, seed) :\n",
        "        assert  type(seed) == torch.Tensor\n",
        "        assert  len(seed.shape) == 2\n",
        "        assert  seed.shape[0] >= 1\n",
        "        assert  seed.shape[1] == (1 + self.bars) * self.seedlength\n",
        "\n",
        "        batchsize = seed.shape[0]\n",
        "        return self.forward(batchsize, seed)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        bar_seed_1 = seeds[0]\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "\n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            #print(f\"temporal seed: {temporal_seed.size()}\")\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 64)\n",
        "            #print(f\"bar seed 2: {bar_seed_2.size()}\")\n",
        "\n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "            \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 128 x 1 x 1)\n",
        "            #print(f\"bar seed: {bar_seed d.size()}\")\n",
        "\n",
        "            ## generate one bar \n",
        "            \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 24 x 84)\n",
        "            #print(f\"generated_bar: {generated_bar.size()}\")\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 24 x 84) \n",
        "        #print(f\"gen output: {pianoroll.size()}\")\n",
        "\n",
        "        return pianoroll"
      ],
      "metadata": {
        "id": "vC9gEofB4Phs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiGenMod1 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 64\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(64, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 3, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 128, 1024, (3, 1), (3, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (1, self.octaves), (1, self.octaves)),\n",
        "            GeneratorBlock( 256,  256, (1, 12), (1, 12)),\n",
        "            GeneratorBlock( 256,  128, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 128,   64, (2, 1), (2, 1)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (2, 1), (2, 1)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward_custom (self, seed) :\n",
        "        assert  type(seed) == torch.Tensor\n",
        "        assert  len(seed.shape) == 2\n",
        "        assert  seed.shape[0] >= 1\n",
        "        assert  seed.shape[1] == (1 + self.bars) * self.seedlength\n",
        "\n",
        "        batchsize = seed.shape[0]\n",
        "        return self.forward(batchsize, seed)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        bar_seed_1 = seeds[0]\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "\n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            #print(f\"temporal seed: {temporal_seed.size()}\")\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 64)\n",
        "            #print(f\"bar seed 2: {bar_seed_2.size()}\")\n",
        "\n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "            \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 128 x 1 x 1)\n",
        "            #print(f\"bar seed: {bar_seed d.size()}\")\n",
        "\n",
        "            ## generate one bar \n",
        "            \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 24 x 84)\n",
        "            #print(f\"generated_bar: {generated_bar.size()}\")\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 24 x 84) \n",
        "        #print(f\"gen output: {pianoroll.size()}\")\n",
        "\n",
        "        return pianoroll"
      ],
      "metadata": {
        "id": "SEJYSDr14Z6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiGenMod2 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 128\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(self.seedlength),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(self.seedlength, 512, 2, 2),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(512, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 3\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 5, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 2*self.seedlength, 1024, (2, 1), (2, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (1, self.octaves), (1, self.octaves)),\n",
        "            GeneratorBlock( 256,  256, (1, 4), (1, 4)),\n",
        "            GeneratorBlock( 256,  256, (1, 3), (1, 3)),\n",
        "            GeneratorBlock( 256,  128, (3, 1), (3, 1)),\n",
        "            GeneratorBlock( 128,   64, (2, 1), (2, 1)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (2, 1), (2, 1)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        bar_seed_1 = seeds[0]\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "\n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            #print(f\"temporal seed: {temporal_seed.size()}\", temporal_seed.device)\n",
        "            #print(f\"temporal generator: {self.temporal_generator[0].device}\")\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 128)\n",
        "            #print(f\"bar seed 2: {bar_seed_2.size()}\")\n",
        "\n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "        \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 256 x 1 x 1)\n",
        "            #print(f\"bar seed: {bar_seed.size()}\")\n",
        "\n",
        "            ## generate one bar \n",
        "            \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 48 x 84)\n",
        "            #print(f\"generated_bar: {generated_bar.size()}\")\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 48 x 84) \n",
        "        #print(f\"gen output: {pianoroll.size()}\")\n",
        "\n",
        "        return pianoroll"
      ],
      "metadata": {
        "id": "ncGPIl1_4bP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiGenMod3 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-track museGAN generator, consisting of two sub-networks (so-called \n",
        "    temporal and bar generator)\n",
        "\n",
        "    input : seed vector, a normally distributed random vector, \n",
        "            length: (B + 1) * 64 = 5 * 64 here\n",
        "    output: pianaroll, binary tensor, shape: (B x T x P) = (4 x 48 x 84) here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) : \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        self.seedlength = 128\n",
        "        \n",
        "        self.temporal_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # heuristically added linear layer\n",
        "            torch.nn.Linear(1, 31),\n",
        "            torch.nn.BatchNorm1d(self.seedlength),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 1\n",
        "            torch.nn.ConvTranspose1d(self.seedlength, 512, 2, 2),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 2\n",
        "            torch.nn.ConvTranspose1d(512, 1024, 2, 2),\n",
        "            torch.nn.BatchNorm1d(1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # transconv layer 3\n",
        "            torch.nn.ConvTranspose1d(1024, 1, 5, 1),\n",
        "            torch.nn.BatchNorm1d(1),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.bar_generator = torch.nn.Sequential(\n",
        "            \n",
        "            # transconv layers\n",
        "            GeneratorBlock( 2*self.seedlength, 1024, (2, 1), (2, 1)),\n",
        "            GeneratorBlock(1024,  512, (2, 1), (2, 1)),\n",
        "            GeneratorBlock( 512,  256, (1, self.octaves), (1, self.octaves)),\n",
        "            GeneratorBlock( 256,  256, (1, 4), (1, 4)),\n",
        "            GeneratorBlock( 256,  256, (1, 3), (1, 3)),\n",
        "            GeneratorBlock( 256,  128, (3, 1), (3, 1)),\n",
        "            GeneratorBlock( 128,   64, (2, 1), (2, 1)),\n",
        "\n",
        "            # last layer with tanh & binarization activation fct.s\n",
        "            torch.nn.ConvTranspose2d(64, 1, (2, 1), (2, 1)),\n",
        "            torch.nn.BatchNorm2d(1),\n",
        "            torch.nn.Tanh(),\n",
        "            StraightThroughEstimator() # binarization\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Generator: parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "    def forward (self, batch_size, seed = None) :\n",
        "        \n",
        "        if seed == None :\n",
        "            assert type(batch_size) == int\n",
        "            assert batch_size >= 1\n",
        "            device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "            seed = torch.normal(0., 1, (batch_size, (1 + self.bars) * self.seedlength)).to(device)\n",
        "            \n",
        "        seeds = torch.chunk(seed, chunks = 5, dim = 1)\n",
        "        \n",
        "        # create time-independent first half of seed for bar generator\n",
        "        track_seed = seeds[0].view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "        bar_seed_1 = self.temporal_generator(track_seed) # (batch size x 1 x 128)\n",
        "        bar_seed_1 = bar_seed_1.view((-1, self.seedlength, 1, 1)) # reshape for transconv layers\n",
        "        \n",
        "        # generate pianorolls bar by bar\n",
        "        generated_bars = []\n",
        "        for temporal_seed in seeds[1:]:\n",
        "            \n",
        "            ## generate time-dependent second half of seed for bar generator\n",
        "            temporal_seed = temporal_seed.view(-1, self.seedlength, 1) # reshape for transconv layers\n",
        "            bar_seed_2 = self.temporal_generator(temporal_seed) # (batch size x 1 x 128)\n",
        "            \n",
        "            ## reshape & concatenate both halfs of seed for bar generator \n",
        "            bar_seed_2 = bar_seed_2.view(-1, self.seedlength, 1, 1)\n",
        "            bar_seed   = torch.cat((bar_seed_1, bar_seed_2), dim = 1) # (batch size x 256 x 1 x 1)\n",
        "            \n",
        "            ## generate one bar \n",
        "            generated_bar = self.bar_generator(bar_seed) # (batch size x 1 x 48 x 84)\n",
        "            generated_bars.append(generated_bar) \n",
        "\n",
        "        pianoroll = torch.cat(generated_bars, dim = 1) # (batch size x 4 x 48 x 84) \n",
        "        \n",
        "        return pianoroll"
      ],
      "metadata": {
        "id": "Ky4Y1uwc4e5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDis (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 12), (1, 1, 12)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 1))\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, self.n_labels))\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "M8bX2IDC4i-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDisMod0a (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 12), (1, 1, 12)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 1))\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512*2, 512),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512, self.n_labels))\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "DoORne104nFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDisMod1 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 12), (1, 1, 12)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            #torch.nn.Linear(1024, 1))\n",
        "            # added\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1))\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512*2, 512),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512, self.n_labels))\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "14zPoIHc4sSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDisMod2 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 3), (1, 1, 3)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1, 4), (1, 1, 4)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            #torch.nn.Linear(1024, 1)\n",
        "            # added\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1)\n",
        "        )\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, self.n_labels)\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "2XnBqjcL4uqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDisMod2a (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 3), (1, 1, 3)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1, 4), (1, 1, 4)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            #torch.nn.Linear(1024, 1)\n",
        "            # added\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1)\n",
        "        )\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 512),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(512, self.n_labels)\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "AGmeJKQs4w69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDisMod2b (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 3), (1, 1, 3)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1, 4), (1, 1, 4)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1)\n",
        "        )\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 64),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(64, self.n_labels)\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "MMTR-a5Y4zjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusiDisMod3 (torch.nn.Module) :\n",
        "    \"\"\"\n",
        "    1-Track musiCAN discriminator, with 2 heads \n",
        "    \n",
        "    input : (B x T x P) binary pianoroll\n",
        "\n",
        "    output: 1. single number, prob. that the input pianoroll is a \n",
        "            real and not generated\n",
        "            2. vector of length = number of genres, prob. that the input \n",
        "            pianoroll is of a certain genre type\n",
        "\n",
        "    n_labels : number of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log = False, **kwargs) :\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Data parameters\n",
        "        self.octaves    = lpd5.octaves\n",
        "        self.bars       = lpd5.bars    # bars per pianoroll\n",
        "        self.T          = lpd5.blips_per_bar  # timesteps per bar\n",
        "        self.P          = lpd5.pitches   # pitches\n",
        "        \n",
        "        self.n_labels   = lpd5.n_labels\n",
        "      \n",
        "        # common body: conv layers\n",
        "        self.discriminator_conv = torch.nn.Sequential(\n",
        "            DiscriminatorBlock(  1, 128, (2, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (3, 1,  1), (1, 1,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 128, (1, 1, 3), (1, 1, 3)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1, 4), (1, 1, 4)), \n",
        "            DiscriminatorBlock(128, 128, (1, 1,  self.octaves), (1, 1,  self.octaves)),\n",
        "            DiscriminatorBlock(128, 128, (1, 2,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(128, 256, (1, 4,  1), (1, 2,  1)),\n",
        "            DiscriminatorBlock(256, 512, (1, 3,  1), (1, 2,  1))\n",
        "            )\n",
        "        \n",
        "        # heads: fully-connected layers\n",
        "        self.discriminator_music_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 16),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(16, 1)\n",
        "        )\n",
        "        \n",
        "        self.discriminator_genre_head = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*2, 1024),  \n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(1024, 256),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(256, 32),\n",
        "            torch.nn.LeakyReLU(negative_slope = 0.2),\n",
        "            torch.nn.Linear(32, self.n_labels)\n",
        "        )\n",
        "\n",
        "        if log :\n",
        "            print(f\"Discriminator parameters: {self.count_params()}\")\n",
        "            print(\"\")\n",
        "\n",
        "    def count_params (self) :\n",
        "        \"\"\"count number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward (self, pianoroll):\n",
        "\n",
        "        # reshape input for transconvs\n",
        "        pianoroll   = pianoroll.view(-1, 1, self.bars, self.T, self.P) \n",
        "        # print(\"dis input prep.\", pianoroll.shape)\n",
        "\n",
        "        # put through common body and flatten instances\n",
        "        common_conv_output = self.discriminator_conv(pianoroll)\n",
        "        common_fc_input = common_conv_output.view(-1, 512*2)  \n",
        "        # print(\"dis conv out\", common_conv_output.size())\n",
        "\n",
        "        # put through each head to judge music (real / fake) and genre labels\n",
        "        music_judgement = self.discriminator_music_head(common_fc_input).flatten().float()\n",
        "        genre_judgement = self.discriminator_genre_head(common_fc_input).view(-1, self.n_labels).float()\n",
        "        # print(\"dis out \", music_judgement.size(), genre_judgement.size())\n",
        "\n",
        "        return music_judgement, genre_judgement"
      ],
      "metadata": {
        "id": "O7DDObe141hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swF_vr8kSfm9"
      },
      "source": [
        "## Training & evaluation classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8eX-HGSfm-"
      },
      "source": [
        "### Training support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_lrgdWMSrIk"
      },
      "source": [
        "#### Training metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0p-ZDYGSfm-"
      },
      "outputs": [],
      "source": [
        "def abs_mean_diff (generated_batch, real_batch) :\n",
        "    \"\"\"\n",
        "        compare two batches of data by calculating the absolute mean difference\n",
        "    \"\"\"\n",
        "    \n",
        "    # equalize shapes\n",
        "    real_shape = real_batch.shape[-2:]\n",
        "    real_batch = real_batch.view(-1, *real_shape)\n",
        "    generated_batch = generated_batch.view(-1, *real_shape)\n",
        "    assert  generated_batch.shape == real_batch.shape\n",
        "\n",
        "    # averaged over batches \n",
        "    generated_mean = torch.mean(generated_batch, dim = 0)\n",
        "    real_mean      = torch.mean(real_batch, dim = 0)\n",
        "\n",
        "    # take differnece & absolut value, average over features lastly\n",
        "    absolute_mean_difference = torch.mean(torch.abs(real_mean - generated_mean))\n",
        "\n",
        "    return absolute_mean_difference.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uWF5EdvzU8Y"
      },
      "outputs": [],
      "source": [
        "def abs_std_diff (generated_batch, real_batch) :\n",
        "    \"\"\"\n",
        "        compare two batches of data by calculating the absolute standard deviation difference\n",
        "    \"\"\"\n",
        "    \n",
        "    # equalize shapes\n",
        "    real_shape = real_batch.shape[-2:]\n",
        "    real_batch = real_batch.view(-1, *real_shape)\n",
        "    generated_batch = generated_batch.view(-1, *real_shape)\n",
        "    assert  generated_batch.shape == real_batch.shape\n",
        "\n",
        "    # averaged over batches \n",
        "    generated_std = torch.std(generated_batch, dim = 0, unbiased = True)\n",
        "    real_std      = torch.std(real_batch, dim = 0, unbiased = True)\n",
        "    \n",
        "    # take differnece & absolut value, average over features lastly\n",
        "    absolute_std_difference = torch.mean(torch.abs(real_std - generated_std))\n",
        "\n",
        "    return absolute_std_difference.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvYGRG9z2dR-"
      },
      "outputs": [],
      "source": [
        "def inter_bar_var (generated_batch) :\n",
        "    \"\"\"\n",
        "        computes the inter-bar standard deviation\n",
        "    \"\"\"\n",
        "\n",
        "    inter_bar_std_dev = torch.mean(torch.std(generated_batch, dim = 1, \n",
        "                                             unbiased = True)) # std over bars\n",
        "    \n",
        "    return inter_bar_std_dev.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NjqpcV34U3t"
      },
      "outputs": [],
      "source": [
        "def inter_track_var (generated_batch) :\n",
        "    \"\"\"\n",
        "        computes the inter-track standard deviation\n",
        "    \"\"\"\n",
        "\n",
        "    inter_track_std_dev = torch.mean(torch.std(generated_batch, dim = 0, \n",
        "                                               unbiased = True)) # std over tracks\n",
        "    \n",
        "    return inter_track_std_dev.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS0kN-LMSzV0"
      },
      "source": [
        "#### Loss function support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYO2ATSm3oRb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def unif_cross_entropy(probabilities, weight):\n",
        "    return(torch.mean(weight * torch.log(probabilities), dim = 1))\n",
        "''';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWKiWR-hYbJG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def softmax(probabilities, safe_normalization = True, eps = 0.000001):\n",
        "  \n",
        "    if safe_normalization == \"safe\":\n",
        "        exp_probs = torch.exp(probabilities)\n",
        "        normalization = torch.maximum(torch.sum(exp_probs, dim = 1), eps)\n",
        "  \n",
        "        if normalization > 0: \n",
        "            return(exp_probs / normalization)\n",
        "    \n",
        "        else:\n",
        "            return(exp_probs)\n",
        "  \n",
        "    else:\n",
        "        return(torch.nn.functional.softmax(probabilities, dim = 1))\n",
        "''';"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def sigmoid_sum(probabilities):\n",
        "    sig_probs = torch.sigmoid(probabilities)\n",
        "    normalization = torch.sum(sig_probs, dim = 1)\n",
        "    return(sig_probs / normalization)\n",
        "''';"
      ],
      "metadata": {
        "id": "EUWhfPU55OlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0LYqJUeUJne"
      },
      "outputs": [],
      "source": [
        "# Note: this function comes directly from the museGAN tutorial [1].\n",
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
        "    \"\"\"Compute the gradient penalty for regularization. Intuitively, the\n",
        "    gradient penalty help stablize the magnitude of the gradients that the\n",
        "    discriminator provides to the generator, and thus help stablize the training\n",
        "    of the generator.\"\"\"\n",
        "    # Get random interpolations between real and fake samples\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).to(device)\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n",
        "    interpolates = interpolates.requires_grad_(True)\n",
        "    \n",
        "    # Get the discriminator output for the interpolations\n",
        "    d_interpolates, _ = discriminator(interpolates)\n",
        "    # Get gradients w.r.t. the interpolations\n",
        "    fake = torch.ones(real_samples.size(0)).to(device)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    # Compute gradient penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Sources:\n",
        "# [1] https://github.com/salu133445/ismir2019tutorial/blob/main/musegan.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ejVE_JNS2VE"
      },
      "source": [
        "#### Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxg1fBDoSfm_"
      },
      "outputs": [],
      "source": [
        "class Log :\n",
        "    \"\"\"\n",
        "        container class for GANTraining logs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self, rounds, dis_rounds, n_labels, batch_size) :\n",
        "        self.losses        = np.zeros((9, rounds)) \n",
        "        self.music_probs   = np.zeros((2, rounds))\n",
        "        self.genre_probs   = np.zeros((1 + n_labels, rounds))\n",
        "        self.abs_diff      = np.zeros((2, rounds))  # abs_mean_diff(), abs_std_diff()\n",
        "        self.gen_var       = np.zeros((2, rounds))  # inter_bar_var(), inter_track_var()\n",
        "        \n",
        "        self._dis_losses   = torch.zeros((7, dis_rounds)).cpu()\n",
        "        self._music_probs  = torch.zeros((2, dis_rounds)).cpu()\n",
        "        self._genre_probs  = torch.zeros((1 + n_labels, dis_rounds)).cpu()\n",
        "\n",
        "        #self.dis_real = torch.zeros((rounds, dis_rounds, batch_size, 1 + n_labels)).cpu()\n",
        "        #self.dis_gen  = torch.zeros((rounds, dis_rounds, batch_size, 1 + n_labels)).cpu()\n",
        "        #self.dis_new  = torch.zeros((rounds, batch_size, 1 + n_labels)).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMlCiiOwBybk"
      },
      "outputs": [],
      "source": [
        "class LogLoaded :\n",
        "    \"\"\"\n",
        "        A class to load stored Log data from an .npz file\n",
        "        and to use it exactly like Log.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self, log_dictionary) :\n",
        "        for keyword, value in log_dictionary.items() :\n",
        "            setattr(self, keyword, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WstRQBd2SfnA"
      },
      "source": [
        "### Training class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg1SZJl7SfnA"
      },
      "outputs": [],
      "source": [
        "class GANTraining :\n",
        "    \"\"\"\n",
        "        general GAN training class\n",
        "        How To Use:\n",
        "        * `MyTrain = GANTraining(<Generator>, <Discriminator>, <torch_dataset>)`\n",
        "        * `MyTrain.setup(<int_rounds>, batchsize = 1, discriminator_rounds = 1,     \n",
        "                        loss_function = [\"WGAN\", \"GAN\"])`\n",
        "        * `MyTrain.train()`\n",
        "      \n",
        "        After That:\n",
        "        * `MyTrain.gen` contains trained Generator\n",
        "        * `MyTrain.dis` contains trained Discriminator\n",
        "        * `MyTrain.log` contains metrics from each round (see class Log)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__ (self, Gen, Dis, dataset) :\n",
        "        assert  type(dataset) == torch.utils.data.dataset.TensorDataset\n",
        "        \n",
        "        self.device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "\n",
        "        # GAN classes and dataset\n",
        "        self.n_labels = lpd5.n_labels     # number of labels in dataset, automate maybe\n",
        "        self.GenClass = Gen\n",
        "        self.DisClass = Dis\n",
        "        self.dataset  = dataset\n",
        "        \n",
        "\n",
        "    def setup (self, rounds, batch_size = 1, discriminator_rounds = 1, \n",
        "               loss_function = \"CAN\", info_text = \"\",\n",
        "               norm_dis_probs = False, norm_dis_genre = False,\n",
        "               genre_ambiguity = (0, 0), learning_rates = (0.001, 0.001)) :\n",
        "        assert  type(rounds) == int\n",
        "        assert  rounds >= 1\n",
        "        assert  type(batch_size) == int\n",
        "        assert  batch_size >= 1\n",
        "        assert  type(discriminator_rounds) == int\n",
        "        assert  discriminator_rounds >= 1\n",
        "        assert  loss_function in [\"GAN\", \"WGAN\", \"WGAN-GP\", \"CAN\",  \"WCAN-GP\"]\n",
        "\n",
        "        # Training parameters\n",
        "        self.rounds     = rounds\n",
        "        self.batch_size = batch_size\n",
        "        self.dis_rounds = discriminator_rounds\n",
        "        self.loss       = loss_function\n",
        "        \n",
        "        self.lr_dis, self.lr_gen       = learning_rates\n",
        "        self.betas_dis, self.betas_gen = ((0.5, 0.9), (0.5, 0.9))\n",
        "        \n",
        "        self.norm_dis_probs = norm_dis_probs  # toggle for music output normalization\n",
        "        self.norm_dis_genre = norm_dis_genre  # toggle for genre output normalization\n",
        "        self.amb_dis, self.amb_gen = genre_ambiguity  # prefactor for CAN dis and gen loss\n",
        "        \n",
        "        self.start_round = 0\n",
        "        self.info = info_text\n",
        "        self._round_range = torch.arange(self.batch_size)  # used each round\n",
        "        \n",
        "\n",
        "        # Dataloader\n",
        "        self.data_loader = torch.utils.data.DataLoader(self.dataset,\n",
        "                                batch_size = self.batch_size, \n",
        "                                drop_last = True,\n",
        "                                shuffle = True)\n",
        "        self.dataset_size = self.dataset.tensors[0].shape[0]  # number of instances in dataset\n",
        "        self.batch_count = self.dataset_size // self.batch_size\n",
        "        self._batch_idx  = self.batch_count \n",
        "        \n",
        "        # Logs\n",
        "        self.log    = Log(self.rounds, self.dis_rounds, self.n_labels, self.batch_size)\n",
        "        self.backup = False   # only on if self.set_backup() is run\n",
        "        \n",
        "        # Initialize GAN\n",
        "        self.gen = self.GenClass().to(self.device)\n",
        "        self.dis = self.DisClass().to(self.device)\n",
        "        self.optimizer_gen = torch.optim.Adam(self.gen.parameters(), \n",
        "                                              lr = self.lr_gen,\n",
        "                                              betas = self.betas_gen)\n",
        "        self.optimizer_dis = torch.optim.Adam(self.dis.parameters(), \n",
        "                                              lr = self.lr_dis,\n",
        "                                              betas = self.betas_dis)\n",
        "        # Note: ADAM parameters from GAN tutorial [1].\n",
        "\n",
        "\n",
        "    def resume (self, load_folder, round) :\n",
        "        train_state = load_train_state(load_folder, round)\n",
        "\n",
        "        self.start_round = round\n",
        "        if self.info == \"\" :\n",
        "            self.info = train_state[\"info\"]\n",
        "        self.gen.load_state_dict(train_state[\"gen\"])\n",
        "        self.dis.load_state_dict(train_state[\"dis\"])\n",
        "        self.optimizer_gen.load_state_dict(train_state[\"opt_g\"])\n",
        "        self.optimizer_dis.load_state_dict(train_state[\"opt_d\"])\n",
        "        log_dict = train_state[\"log\"]   \n",
        "        for name, array in log_dict.items() :\n",
        "            setattr(self.log, name, array)\n",
        "            \n",
        "       \n",
        "    def set_backups (self, training_name, checkpoints, save_folder = \"\") :\n",
        "        assert  type(training_name) == str\n",
        "        for element in checkpoints :\n",
        "            assert  type(element) == int\n",
        "            assert  element >= 0  and  element <= self.rounds\n",
        "        \n",
        "        self.training_name   = training_name\n",
        "        self.training_folder = save_folder   # 'timestamp+training_name', gets set at first backup\n",
        "        self.checkpoints     = [point for point in checkpoints\n",
        "                                if point > self.start_round]\n",
        "        self.backup          = True  # Flag for rest of code\n",
        "\n",
        "\n",
        "\n",
        "    def train (self) :\n",
        "        assert  hasattr(self, \"data_loader\")  # If test fails, you haven't run set_params()\n",
        "\n",
        "        print(f\"Training\")\n",
        "        for round in notebook.tqdm(range(self.start_round, self.rounds)) :\n",
        "            print(f\"Round {round}\", end = \"\")\n",
        "            for dis_round in range(self.dis_rounds) :\n",
        "                self._discriminator_update(round, dis_round)\n",
        "\n",
        "            self._generator_update(round)\n",
        "        \n",
        "            # Make a backup\n",
        "            if self.backup  and  (round + 1) in self.checkpoints :\n",
        "                self._save_state(round + 1)\n",
        "            print(\"\\r\", end = \"\")\n",
        "\n",
        "            # Stop Training if diverges\n",
        "            divergence = torch.any(torch.isnan(self.loss_dis.detach().cpu()))\n",
        "            if divergence:\n",
        "                print(\"\\rTraining stopped: nan values encontered.\")\n",
        "                self._save_state(round + 1)\n",
        "                return\n",
        "            \n",
        "        # End training\n",
        "        self._save_state(self.rounds)\n",
        "        self.gen.eval()\n",
        "        self.dis.eval()\n",
        "        print(\"Training complete. GAN now in eval() mode.\")\n",
        "\n",
        "\n",
        "    def _get_batch (self) :\n",
        "        \"\"\"\n",
        "            samples one batch of data from self.data_loader without replacement.\n",
        "            When the self.data_set is depleted of fresh batches, \n",
        "            self.data_loader will shuffle a list of new batches.\n",
        "        \"\"\"\n",
        "        if self._batch_idx >= self.batch_count :\n",
        "            self._data_iter = iter(self.data_loader)\n",
        "            self._batch_idx = 0\n",
        "        batch_data, batch_labels = self._data_iter.next()\n",
        "        batch_data = batch_data.view(-1, lpd5.bars, \n",
        "                                     lpd5.blips_per_bar, lpd5.pitches)\n",
        "        self._batch_idx += 1\n",
        "\n",
        "        return batch_data.to(self.device), batch_labels.to(self.device)\n",
        "\n",
        "\n",
        "    def _discriminator_update (self, round, dis_round) :\n",
        "        \n",
        "        #self._dis_real = torch.zeros((self.batch_size, 1 + self.n_labels)).to(self.device)\n",
        "        #self._dis_gen  = torch.zeros((self.batch_size, 1 + self.n_labels)).to(self.device)\n",
        "        #self._dis_new  = torch.zeros((self.batch_size, 1 + self.n_labels)).to(self.device)\n",
        "\n",
        "        # Forward propagation\n",
        "        batch_real, labels_real = self._get_batch()  # training data\n",
        "        batch_gen               = self.gen.forward(batch_size = self.batch_size)   # generated data\n",
        "        \n",
        "        music_dis_real, genre_dis_real = self.dis.forward(batch_real)\n",
        "        #self._dis_real[:, 0]  = music_dis_real\n",
        "        #self._dis_real[:, 1:] = genre_dis_real\n",
        "        music_dis_gen, genre_dis_gen   = self.dis.forward(batch_gen)\n",
        "        #self._dis_gen[:, 0]  = music_dis_gen\n",
        "        #self._dis_gen[:, 1:] = genre_dis_gen\n",
        "        \n",
        "        self.music_prob_real    = torch.sigmoid(music_dis_real)\n",
        "        genre_logprobs_real     = torch.log_softmax(genre_dis_real, dim = 1)\n",
        "        self.genre_logprob_real = genre_logprobs_real[self._round_range, labels_real] # get prob of real genre\n",
        "        \n",
        "        self.music_prob_gen     = torch.sigmoid(music_dis_gen)\n",
        "        self.genre_logprobs_gen = torch.log_softmax(genre_dis_gen, dim = 1)\n",
        "                   \n",
        "\n",
        "        # Calculating the Discriminator loss terms\n",
        "        if self.loss == \"GAN\" :\n",
        "            self.loss_reg        = torch.tensor(0.).to(self.device)\n",
        "            self.loss_real_music = - torch.mean(torch.log(self.music_prob_real))\n",
        "            self.loss_gen_music  = - torch.mean(torch.log(1 - self.music_prob_gen))\n",
        "            self.loss_real_genre = torch.tensor(0.).to(self.device)\n",
        "        \n",
        "        elif self.loss == \"WGAN\" :\n",
        "            var_gen   = torch.var(music_dis_gen)\n",
        "            var_real  = torch.var(music_dis_real)\n",
        "            self.loss_reg  = torch.where(var_gen > 1, \n",
        "                                            (var_gen - 1)**2, 0) \\\n",
        "                                + torch.where(var_real > 1, \n",
        "                                            (var_real - 1)**2, 0)\n",
        "            self.loss_real_music = - torch.mean(music_dis_real)\n",
        "            self.loss_real_genre = torch.tensor(0.).to(self.device)\n",
        "            self.loss_gen_music  = torch.mean(music_dis_gen)\n",
        "        \n",
        "        elif self.loss == \"WGAN-GP\" :    \n",
        "            gradient_penalty = compute_gradient_penalty(\n",
        "                self.dis, batch_real, batch_gen, self.device)\n",
        "            self.loss_reg        = 10 * gradient_penalty\n",
        "            self.loss_real_music = - torch.mean(music_dis_real)\n",
        "            self.loss_real_genre = torch.tensor(0.).to(self.device)\n",
        "            self.loss_gen_music  = torch.mean(music_dis_gen)\n",
        "        \n",
        "\n",
        "        elif self.loss == \"CAN\" :\n",
        "            self.loss_reg        = torch.tensor(0.).to(self.device)\n",
        "            self.loss_real_music = - torch.mean(torch.log(self.music_prob_real))\n",
        "            self.loss_real_genre = - torch.mean(self.genre_logprob_real)\n",
        "            self.loss_gen_music  = - torch.mean(torch.log(1 - self.music_prob_gen))\n",
        "\n",
        "        elif self.loss == \"WCAN-GP\" : \n",
        "            self.loss_reg        = 10.0 * compute_gradient_penalty(\n",
        "                                   self.dis, batch_real, batch_gen, self.device)\n",
        "            self.loss_real_music = - torch.mean(music_dis_real)\n",
        "            self.loss_real_genre = - torch.mean(self.genre_logprob_real)\n",
        "            self.loss_gen_music  = torch.mean(music_dis_gen)\n",
        "\n",
        "\n",
        "        # Optional discriminator output normalization losses for stability\n",
        "        self.prob_loss = torch.tensor(0.).to(self.device)\n",
        "        if self.norm_dis_probs :\n",
        "            severity       = 1\n",
        "            prob_norm_loss = torch.mean((self.music_prob_real + \n",
        "                             self.music_prob_gen - 1)**2)\n",
        "            self.prob_loss = severity * prob_norm_loss\n",
        "            \n",
        "        self.genre_loss = torch.tensor(0.).to(self.device)\n",
        "        if self.norm_dis_genre :\n",
        "            severity   = 1\n",
        "            threshold  = 5\n",
        "            laxity     = 1\n",
        "            self.genre_loss = severity * torch.sum(\n",
        "                (torch.threshold(torch.abs(genre_dis_real) - threshold, 0, 0)\n",
        "                 / laxity)**2)\n",
        "\n",
        "\n",
        "        # Loss function\n",
        "        self.loss_dis = self.loss_real_music \\\n",
        "                        + self.amb_dis * self.loss_real_genre \\\n",
        "                        + self.loss_gen_music \\\n",
        "                        + self.loss_reg \\\n",
        "                        + self.prob_loss \\\n",
        "                        + self.genre_loss\n",
        "        self._log_all(round, k = dis_round)\n",
        "\n",
        "\n",
        "        # Discriminator update\n",
        "        self.optimizer_dis.zero_grad()\n",
        "        self.loss_dis.backward()\n",
        "        self.optimizer_dis.step()\n",
        "\n",
        "\n",
        "    def _generator_update (self, round) :\n",
        "        \n",
        "        # Forward propagation\n",
        "        batch_new = self.gen.forward(batch_size = self.batch_size)\n",
        "        music_dis_new, genre_dis_new = self.dis.forward(batch_new)\n",
        "        #self._dis_new[:, 0]  = music_dis_new\n",
        "        #self._dis_new[:, 1:] = genre_dis_new\n",
        "\n",
        "        music_prob_new = torch.sigmoid(music_dis_new)\n",
        "\n",
        "\n",
        "        # Calculating the Generator loss terms\n",
        "        if self.loss == \"GAN\" :\n",
        "            self.loss_gen_music = -torch.mean(torch.log(music_prob_new)) \n",
        "            self.loss_gen_genre = torch.tensor(0.)\n",
        "        \n",
        "        elif self.loss == \"WGAN\" :\n",
        "            self.loss_gen_music = -torch.mean(music_dis_new)\n",
        "            self.loss_gen_genre = torch.tensor(0.)\n",
        "\n",
        "        elif self.loss == \"WGAN-GP\" :\n",
        "            self.loss_gen_music = -torch.mean(music_dis_new)\n",
        "            self.loss_gen_genre = torch.tensor(0.)\n",
        "        \n",
        "        elif self.loss == \"CAN\" :\n",
        "            self.loss_gen_music    = - torch.mean(music_prob_new)\n",
        "            genre_logprobs_new     = torch.log_softmax(genre_dis_new, dim = 1)\n",
        "            genre_antilogprobs_new = torch.log(1 - torch.softmax(genre_dis_new, dim = 1))\n",
        "            self.loss_gen_genre    = - torch.mean( \\\n",
        "                torch.sum(1/self.n_labels * genre_logprobs_new, dim = 1) + \\\n",
        "                torch.sum((1 - 1/self.n_labels) * genre_antilogprobs_new, dim = 1))\n",
        "            \n",
        "        elif self.loss == \"WCAN-GP\" :\n",
        "            self.loss_gen_music    = - torch.mean(music_dis_new)\n",
        "            genre_logprobs_new     = torch.log_softmax(genre_dis_new, dim = 1)\n",
        "            genre_antilogprobs_new = torch.log(1 - torch.softmax(genre_dis_new, dim = 1))\n",
        "            self.loss_gen_genre    = - torch.mean( \\\n",
        "                torch.sum(1/self.n_labels * genre_logprobs_new, dim = 1) + \\\n",
        "                torch.sum((1 - 1/self.n_labels) * genre_antilogprobs_new, dim = 1))\n",
        "\n",
        "\n",
        "        # Loss function  \n",
        "        self.loss_gen = self.loss_gen_music \\\n",
        "                        + self.amb_gen * self.loss_gen_genre\n",
        "\n",
        "        self._log_all(round)\n",
        "            \n",
        "        \n",
        "        # Generator update\n",
        "        self.optimizer_gen.zero_grad()\n",
        "        self.loss_gen.backward()\n",
        "        self.optimizer_gen.step()\n",
        "\n",
        "\n",
        "    def _log_all (self, round, k = -1) :\n",
        "        if k >= 0 : # before each Discriminator update\n",
        "            # Discriminator outputs\n",
        "            #self.log.dis_real[round, k] = self._dis_real.cpu().detach()\n",
        "            #self.log.dis_gen[round, k]  = self._dis_gen.cpu().detach()\n",
        "            \n",
        "            # Losses\n",
        "            self.log._dis_losses[0, k] = self.loss_dis.cpu().detach()\n",
        "            self.log._dis_losses[1, k] = self.loss_real_music.cpu().detach()\n",
        "            self.log._dis_losses[2, k] = self.amb_dis * self.loss_real_genre.cpu().detach()\n",
        "            self.log._dis_losses[3, k] = self.loss_gen_music.cpu().detach()\n",
        "            self.log._dis_losses[4, k] = self.loss_reg.cpu().detach()\n",
        "            self.log._dis_losses[5, k] = self.prob_loss.cpu().detach()\n",
        "            self.log._dis_losses[6, k] = self.genre_loss.cpu().detach()\n",
        "\n",
        "            # Discrimantor probabilities\n",
        "            self.log._music_probs[0, k] = self.music_prob_real.mean().cpu().detach()\n",
        "            self.log._music_probs[1, k] = self.music_prob_gen.mean().cpu().detach()\n",
        "            genre_prob_real = torch.exp(self.genre_logprob_real)\n",
        "            genre_probs_gen = torch.exp(self.genre_logprobs_gen)\n",
        "            self.log._genre_probs[0, k]  = genre_prob_real.mean().cpu().detach() # prob of right label of real batch\n",
        "            self.log._genre_probs[1:, k] = genre_probs_gen.mean(dim = 0).cpu().detach() # prob of genres of generated batch\n",
        "        \n",
        "\n",
        "        if k == -1 : # before each Generator update\n",
        "            # Discriminator outputs\n",
        "            #self.log.dis_new[round] = self._dis_new.cpu().detach()\n",
        "            \n",
        "            # Losses\n",
        "            dis_losses = self.log._dis_losses.detach().cpu().numpy()\n",
        "            self.log.losses[:7, round] = dis_losses[:7].mean(axis = 1)\n",
        "            self.log.losses[7, round]  = self.loss_gen.detach().cpu().numpy()\n",
        "            self.log.losses[8, round]  = self.amb_gen * self.loss_gen_genre.detach().cpu().numpy()\n",
        "            \n",
        "            # Discriminator Probabilities\n",
        "            music_probs = self.log._music_probs.cpu().detach().numpy()\n",
        "            genre_probs = self.log._genre_probs.cpu().detach().numpy()\n",
        "            self.log.music_probs[:, round] = music_probs.mean(axis = 1)\n",
        "            self.log.genre_probs[:, round] = genre_probs.mean(axis = 1)\n",
        "            \n",
        "            # Generator metrics\n",
        "            batch_real, _ = self._get_batch()\n",
        "            batch_gen     = self.gen.forward(batch_size = self.batch_size)\n",
        "            self.log.abs_diff[0, round] = abs_mean_diff(batch_gen, batch_real)\n",
        "            self.log.abs_diff[1, round] = abs_std_diff(batch_gen, batch_real)\n",
        "            self.log.gen_var[0, round]  = inter_bar_var(batch_gen)\n",
        "            self.log.gen_var[1, round]  = inter_track_var(batch_gen)\n",
        "\n",
        "\n",
        "    def _save_state (self, round) :\n",
        "        if self.training_folder == \"\" :\n",
        "            self.training_folder = save_train_state(self, round)\n",
        "            print(f\"\\rSaved checkpoint {round} under '{self.training_folder}'.\")\n",
        "        else :\n",
        "            save_train_state(self, round)\n",
        "            print(f\"\\rSaved checkpoint {round}.\")\n",
        "\n",
        "\n",
        "# Sources:\n",
        "# [1] https://github.com/salu133445/ismir2019tutorial/blob/main/musegan.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UjwRjHiook6"
      },
      "source": [
        "### Evaluation support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixzSjpbOS64f"
      },
      "source": [
        "#### Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3STzYDPRS9HD"
      },
      "outputs": [],
      "source": [
        "def empty_bar_ratio (data) :\n",
        "    \"\"\"\n",
        "        ratio of bars devoid of notes\n",
        "        \n",
        "        also called:\n",
        "            EB = \"empty bar ratio\"\n",
        "    \"\"\"\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches)) # split into bars\n",
        "    data_reduced = np.mean(data, axis = (2, 3)).flatten() # mean over bar pixels\n",
        "    data_mask    = np.array(data_reduced == 0)  # bool of which bars are empty\n",
        "    empty_bar_fraction = np.mean(data_mask)  # mean over all bars\n",
        "\n",
        "    return empty_bar_fraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GclWeQ3TU7hU"
      },
      "outputs": [],
      "source": [
        "def pitch_classes_per_bar (data) :\n",
        "    \"\"\"\n",
        "        number of pitch classes used per bar (from 0 to 12)\n",
        "        \n",
        "        also called:\n",
        "            UPC = \"used pitch classes per bar\"\n",
        "    \"\"\"\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches)) # split into bars\n",
        "    data = data.reshape((-1, lpd5.blips_per_bar, lpd5.pitches))  # array of bars\n",
        "    data = data.reshape((-1, lpd5.blips_per_bar, lpd5.octaves, 12)) # split into octaves\n",
        "    \n",
        "    pitches_used = np.any(data, axis = (1, 2))  # OR over timesteps and octaves\n",
        "    number_pitches = np.sum(pitches_used, axis = 1) # sum over pitches\n",
        "    mean_pitch_classes_per_bar = np.mean(number_pitches) # mean over all bars\n",
        "    \n",
        "    return mean_pitch_classes_per_bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10tasJPahEvQ"
      },
      "outputs": [],
      "source": [
        "def qualified_note_ratio (data) :\n",
        "    \"\"\"\n",
        "        ratio of \"qualified\" notes,\n",
        "        defined as a 3 blips/timesteps or longer. \n",
        "        In the current lpd5 dataset with 48-blip bars that is a 1/16 note.\n",
        "        ! Not like in museGAN (used 96-blip bars and thus a 1/32 note threshold)\n",
        "        \n",
        "        also called:\n",
        "            QN = \"qualified note ratio\"\n",
        "    \"\"\"\n",
        "    minimum_length = 3 # blips\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.width, lpd5.height)) # whole tracks\n",
        "    conv = np.array([-1, 1]) # used to measure note start and ends\n",
        "\n",
        "    total_notes       = 0\n",
        "    total_quali_notes = 0\n",
        "    for track in data :\n",
        "        for pitch_line in track.T :\n",
        "            note_starts = np.convolve(pitch_line, conv)\n",
        "            note_stops  = np.convolve(pitch_line, -conv)\n",
        "            start_indices = np.where(note_starts == -1)[0]\n",
        "            stop_indices  = np.where(note_stops == -1)[0]\n",
        "            \n",
        "            note_lengths     = stop_indices - start_indices\n",
        "            note_count       = note_lengths.shape[0]\n",
        "            quali_note_count = np.sum(note_lengths >= minimum_length)\n",
        "            total_notes       += note_count\n",
        "            total_quali_notes += quali_note_count\n",
        "\n",
        "    quali_note_ratio = total_quali_notes / total_notes\n",
        "\n",
        "    return quali_note_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEpHEch60X02"
      },
      "outputs": [],
      "source": [
        "def muspy_metrics (data) :\n",
        "    \"\"\"\n",
        "    computes 4 muspy metrics from a batch of pianoroll data\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    averaged_metrics : np.array, size = (4), dtype = float\n",
        "        all values taken from whole pianoroll tracks and\n",
        "        are averaged over all tracks\n",
        "        1. muspy.pitch_range()\n",
        "            pitch range from lowest to highest pitch\n",
        "        2. muspy.polyphony()\n",
        "            average number of pitches being played concurrently\n",
        "        3. muspy.scale_consistency()\n",
        "            how many of the notes are in the tracks main scale \n",
        "            (max of notes in any scale)\n",
        "        4. muspy.empty_measure_rate()\n",
        "            ratio of 1/4 note beats where no note is played\n",
        "            \"measure\" is here defined as 1/4 notes by us.\n",
        "\n",
        "        For more details, see [1]\n",
        "\n",
        "    [1] https://muspy.readthedocs.io/en/stable/metrics.html?highlight=measures#other-metrics\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if type(data) == torch.Tensor :\n",
        "        data = data.cpu().detach().numpy()\n",
        "\n",
        "    data = data.reshape((-1, lpd5.width, lpd5.height)) # whole tracks\n",
        "    data.dtype = bool\n",
        "    \n",
        "    pianorolls = np.pad(data, \n",
        "                        ((0, 0), (0, 0), \n",
        "                         (lpd5.lowest_pitch, \n",
        "                          128 - lpd5.lowest_pitch - lpd5.height))\n",
        "                 )   # complete the pitch range\n",
        "    \n",
        "    muspy_stats = np.zeros((4, data.shape[0]))\n",
        "    for i, track in enumerate(pianorolls):\n",
        "        piano_music = muspy.from_pianoroll_representation(\n",
        "                        track,\n",
        "                        resolution = lpd5.blips_per_beat, \n",
        "                        encode_velocity = False\n",
        "                    )   # convert to muspy.music_object\n",
        "                  \n",
        "        muspy_stats[0, i] = muspy.pitch_range(piano_music)\n",
        "        muspy_stats[1, i] = muspy.polyphony(piano_music)\n",
        "        muspy_stats[2, i] = muspy.scale_consistency(piano_music)\n",
        "        muspy_stats[3, i] = muspy.empty_measure_rate(piano_music, \n",
        "                                                     lpd5.blips_per_beat)\n",
        "        \n",
        "    averaged_metrics = np.nanmean(muspy_stats, axis = 1)\n",
        "    \n",
        "    return averaged_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqBIvah5RK7k"
      },
      "outputs": [],
      "source": [
        "# Calculate key metrics of dataset for evaluation\n",
        "\n",
        "lpd5_metrics_file = f\"{default_training_path}/{default_dataset}/lpd5_metrics.json\"\n",
        "\n",
        "if not os.path.exists(lpd5_metrics_file):\n",
        "    # Calculating these metrics takes several minutes for lpd5.\n",
        "    # Therefore, they are calculated once and then stored in a file.\n",
        "    metrics = {}\n",
        "    metrics[\"abs_mean_diff\"]   = 0   # difference of dataset to itself\n",
        "    metrics[\"abs_std_diff\"]    = 0\n",
        "    metrics[\"inter_bar_var\"]   = inter_bar_var(lpd5.data.view(-1, lpd5.bars, lpd5.blips_per_bar, lpd5.pitches))\n",
        "    metrics[\"inter_track_var\"] = inter_track_var(lpd5.data)\n",
        "    metrics[\"empty_bar_ratio\"]        = empty_bar_ratio(lpd5.data)\n",
        "    metrics[\"pitch_classses_per_bar\"] = pitch_classes_per_bar(lpd5.data)\n",
        "    metrics[\"qualified_note_ratio\"]   = qualified_note_ratio(lpd5.data)\n",
        "    metrics[\"muspy_metrics\"] = muspy_metrics(lpd5.data)\n",
        "    with open(lpd5_metrics_file, 'wb') as file :\n",
        "        pickle.dump(metrics, file)\n",
        "\n",
        "\n",
        "with open(lpd5_metrics_file, 'rb') as file :\n",
        "    # Loading all metrics is much quicker than recalculating them\n",
        "    metrics = pickle.load(file)\n",
        "    lpd5.abs_mean_diff   = metrics[\"abs_mean_diff\"]\n",
        "    lpd5.abs_std_diff    = metrics[\"abs_std_diff\"]\n",
        "    lpd5.inter_bar_var   = metrics[\"inter_bar_var\"]\n",
        "    lpd5.inter_track_var = metrics[\"inter_track_var\"]\n",
        "    lpd5.empty_bar_ratio        = metrics[\"empty_bar_ratio\"]\n",
        "    lpd5.pitch_classses_per_bar = metrics[\"pitch_classses_per_bar\"]\n",
        "    lpd5.qualified_note_ratio   = metrics[\"qualified_note_ratio\"]\n",
        "    lpd5.muspy_metrics = metrics[\"muspy_metrics\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyJRhxx8L_4y"
      },
      "source": [
        "#### Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VBC5mONqw-T"
      },
      "outputs": [],
      "source": [
        "def plot_training (log, dataset = lpd5, CAN = True, show_loss_terms = True) :\n",
        "    training_rounds = log.losses.shape[1]\n",
        "    rounds          = np.arange(training_rounds) + 1\n",
        "    filter_size     = math.floor(np.sqrt(training_rounds))\n",
        "    med_filter      = lambda x: ndimage.median_filter(x, size = filter_size)\n",
        "    \n",
        "    \n",
        "    # Training metrics\n",
        "\n",
        "    plt.figure(figsize = (16, 8))\n",
        "    plt.suptitle(\"Training metrics\", size=18)\n",
        "    \n",
        "    ## Losses\n",
        "    plt.title(\"Losses\")\n",
        "    dis_loss = log.losses[0]\n",
        "    gen_loss = log.losses[7]\n",
        "    plt.plot(rounds, dis_loss, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, gen_loss, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(dis_loss), label=\"Discriminator Loss\", \n",
        "             c=\"b\") #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(gen_loss), label=\"Generator Loss\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale('symlog', linthreshy = 10)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    if show_loss_terms :\n",
        "        plt.figure(figsize=(16,6))\n",
        "        plt.title(r\"Discriminator & Generator loss terms\")\n",
        "        loss_term_labels = [\"music dis. loss for real data\",\n",
        "                            \"genre dis. loss for real label\",\n",
        "                            \"music dis. loss for generated data\",\n",
        "                            \"genre gen. loss for generated data\",\n",
        "                            \"dis. regularization loss\",\n",
        "                            \"dis. music prob. normalization loss\",\n",
        "                            \"dis. genre prob. normalization losss\",\n",
        "                            ]\n",
        "        for label_idx, loss_idx in enumerate([1, 2, 3, 8, 4, 5, 6]):\n",
        "            loss_term = log.losses[loss_idx]\n",
        "            plt.plot(rounds, med_filter(loss_term), \n",
        "                     label = loss_term_labels[label_idx])\n",
        "        plt.xlabel(\"round\")\n",
        "        plt.yscale('symlog', linthreshy = 10)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    ## Probabilities\n",
        "    plt.figure(figsize=(16,4))\n",
        "    prob_real = log.music_probs[0]\n",
        "    prob_gen  = log.music_probs[1]\n",
        "    prob_diff = prob_real - prob_gen\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(r\"$p_{Dis}(data_{real} = real)$\")\n",
        "    plt.plot(rounds, np.ones_like(prob_real), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, np.zeros_like(prob_real), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, prob_real, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(prob_real), c=\"b\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(r\"$p_{Dis}(data_{real} = real) - p_{Dis}(data_{gen} = real)$\")\n",
        "    plt.plot(rounds, np.ones_like(prob_diff), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, np.zeros_like(prob_diff), \n",
        "             linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "    plt.plot(rounds, prob_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(prob_diff), c=\"b\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")  \n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "    # CAN metrics\n",
        "    if CAN :\n",
        "        plt.figure(figsize=(16,6))\n",
        "        plt.suptitle(\"CAN metrics\", size=18)\n",
        "        genre_prob_real = log.genre_probs[0]\n",
        "        genre_probs_gen = log.genre_probs[1:]\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Genre probability of true label given to real samples\")\n",
        "        plt.plot(rounds, np.ones_like(genre_prob_real), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        plt.plot(rounds, np.zeros_like(genre_prob_real), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        plt.plot(rounds, genre_prob_real, lw = 0.5, alpha=0.5)\n",
        "        plt.plot(rounds, med_filter(genre_prob_real), c='b')\n",
        "        plt.xlabel(\"round\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Genre probabilities given to generated samples\")        \n",
        "        plt.plot(rounds, np.ones_like(genre_prob_real), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        plt.plot(rounds, np.zeros_like(genre_prob_real), \n",
        "                 linestyle=\"-.\", lw=0.5, color='k', alpha=0.3)\n",
        "        for i, prob in enumerate(genre_probs_gen) :\n",
        "            plt.plot(rounds, med_filter(prob), label = dataset.genre_list[i])\n",
        "        plt.xlabel(\"round\")\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # Generator metrics\n",
        "\n",
        "    plt.figure(figsize=(16,6))\n",
        "    plt.suptitle(\"Generator metrics\", size=18)\n",
        "    abs_mean_diff   = log.abs_diff[0]\n",
        "    abs_std_diff    = log.abs_diff[1]\n",
        "    inter_bar_var   = log.gen_var[0]\n",
        "    inter_track_var = log.gen_var[1]\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Absolute mean and std difference to dataset\")\n",
        "    plt.plot(rounds, abs_mean_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, abs_std_diff, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(abs_mean_diff), label = \"abs_mean_diff\", \n",
        "             c='b') #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(abs_std_diff), label = \"abs_std_diff\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Generator variation\")\n",
        "    plt.plot(rounds, np.ones_like(inter_bar_var) * lpd5.inter_bar_var, \n",
        "             linestyle=\"--\", lw=0.5, color='b', label=\"dataset bar-wise std.\")\n",
        "    plt.plot(rounds, np.ones_like(inter_bar_var) * lpd5.inter_track_var, \n",
        "             linestyle=\"--\", lw=0.5, color='r', label=\"dataset track-wise std.\")\n",
        "    plt.plot(rounds, inter_bar_var, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, inter_track_var, lw = 0.5, alpha=0.5)\n",
        "    plt.plot(rounds, med_filter(inter_bar_var), label = \"bar-wise std dev.\", \n",
        "             c=\"b\") #, lw = 0.5)\n",
        "    plt.plot(rounds, med_filter(inter_track_var), label = \"track-wise std dev.\", \n",
        "             c=\"r\") #, lw = 0.5)\n",
        "    plt.xlabel(\"round\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWBpY1DkSfm_"
      },
      "outputs": [],
      "source": [
        "def long_test (generator, discriminator, data, test_size = 1000, \n",
        "               dataset = default_dataset): \n",
        "    \"\"\"\n",
        "        runs a detailed evaluation of generator performance\n",
        "        and compares it to the training data set in a pandas table\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    with torch.inference_mode() :  # saves gpu memory\n",
        "        device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "        \n",
        "        # Test on generated data\n",
        "        generator.eval().to(device)\n",
        "        discriminator.eval().to(device)\n",
        "        data_real, labels_real = iter(torch.utils.data.DataLoader(data.dataset,\n",
        "                        batch_size = test_size, \n",
        "                        shuffle = True, drop_last = True)\n",
        "                    ).next()  # make one batch of test_size\n",
        "        data_real   = data_real.to(device)\n",
        "        labels_real = labels_real.to(device)\n",
        "\n",
        "        ## Generator\n",
        "        data_generated = generator.forward(batch_size = test_size)\n",
        "        \n",
        "        gen_abs_mean_diff   = abs_mean_diff(data_generated, data_real)\n",
        "        gen_abs_std_diff    = abs_std_diff(data_generated, data_real)\n",
        "        gen_inter_bar_var   = inter_bar_var(data_generated)\n",
        "        gen_inter_track_var = inter_track_var(data_generated)\n",
        "\n",
        "        gen_empty_bar_ratio        = empty_bar_ratio(data_generated)\n",
        "        gen_pitch_classses_per_bar = pitch_classes_per_bar(data_generated)\n",
        "        gen_qualified_note_ratio   = qualified_note_ratio(data_generated)\n",
        "\n",
        "        gen_muspy_metrics = muspy_metrics(data_generated)\n",
        "\n",
        "    # Create comparison table: generated data vs. real data\n",
        "    \n",
        "    real_music_metrics = np.array([\n",
        "        lpd5.abs_mean_diff,\n",
        "        lpd5.abs_std_diff,\n",
        "        lpd5.inter_bar_var,\n",
        "        lpd5.inter_track_var,\n",
        "        lpd5.empty_bar_ratio,\n",
        "        lpd5.pitch_classses_per_bar,\n",
        "        lpd5.qualified_note_ratio,\n",
        "        lpd5.muspy_metrics[0],\n",
        "        lpd5.muspy_metrics[1],\n",
        "        lpd5.muspy_metrics[2],\n",
        "        lpd5.muspy_metrics[3],\n",
        "    ], dtype = float).round(2)\n",
        "    gen_music_metrics = np.array([\n",
        "        gen_abs_mean_diff,\n",
        "        gen_abs_std_diff,\n",
        "        gen_inter_bar_var,\n",
        "        gen_inter_track_var,\n",
        "        gen_empty_bar_ratio,\n",
        "        gen_pitch_classses_per_bar,\n",
        "        gen_qualified_note_ratio,\n",
        "        gen_muspy_metrics[0],\n",
        "        gen_muspy_metrics[1],\n",
        "        gen_muspy_metrics[2],\n",
        "        gen_muspy_metrics[3],\n",
        "    ], dtype = float).round(2)\n",
        "\n",
        "    table_dict = {\n",
        "        \"Metrics\":[\n",
        "            \"Absoluted mean difference\", \n",
        "            \"Absoluted standard deviation difference\", \n",
        "            \"Inter-bar standard deviation\",\n",
        "            \"Inter-track standard deviation\",\n",
        "            \"Empty bar ratio\",\n",
        "            \"Used pitch classes per bar\",\n",
        "            \"Qualified note ratio\",\n",
        "            \"Pitch range\",\n",
        "            \"Polyphony\",\n",
        "            \"Scale consistency\",\n",
        "            \"Empty 1/4 note ratio\",\n",
        "        ],\n",
        "        \"real music\":real_music_metrics,\n",
        "        \"generated music\":gen_music_metrics,\n",
        "        \"Abbreviation\":[\"AMD\", \"ASD\", \"IBS\", \"ITS\", \"EB\", \"UPC\", \"QN\", \n",
        "                        \"PR\", \"PL\", \"SC\", \"EN\",],\n",
        "        \"metric source\":[\"own\", \"own\", \"own\", \"own\", \n",
        "                        \"museGAN\", \"museGAN\", \"museGAN\",\n",
        "                        \"muspy\", \"muspy\", \"muspy\", \"muspy\",],        \n",
        "    }\n",
        "    \n",
        "    table_panda = pd.DataFrame(table_dict)\n",
        "    \n",
        "    print(\"Comparison between the real and the generated music\\n\")\n",
        "    display(table_panda)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntUausIx9L9q"
      },
      "outputs": [],
      "source": [
        "# Tweak1: Toggle to only `show_best` according to discriminator\n",
        "# Tweak2: `show_real` data instead of generated\n",
        "# Tweak3: Choose `playback_speed` (3 for 4-bar tracks, 1 for 12-bar tracks)\n",
        "# Tweak4: Save samples from a `checkpoint` in a subfolder of that name.\n",
        "def quick_test (generator, discriminator, data, test_size = 100, num_images = 1, \n",
        "                dataset = default_dataset, save_to = None, show_best = False,\n",
        "                show_real = False, playback_speed = 3, checkpoint = 0) : \n",
        "\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "\n",
        "    # Calculating Discriminator predictions\n",
        "    with torch.inference_mode() :    \n",
        "        # Loading real data and models\n",
        "        generator.eval().to(device)\n",
        "        discriminator.eval().to(device)\n",
        "        data_real, labels_real = iter(torch.utils.data.DataLoader(data.dataset,\n",
        "                        batch_size = test_size, \n",
        "                        shuffle = True, drop_last = True)\n",
        "                    ).next()  # make one batch of test_size\n",
        "        data_real   = data_real.to(device)\n",
        "        labels_real = labels_real.to(device)\n",
        "\n",
        "        # Generator\n",
        "        data_generated = generator.forward(batch_size = test_size)\n",
        "        \n",
        "\n",
        "        # Discriminator\n",
        "        music_dis_gen,  genre_dis_gen  = discriminator.forward(data_generated)\n",
        "        music_prob_gen  = torch.sigmoid(music_dis_gen)\n",
        "        std_prob_gen    = torch.std_mean(music_prob_gen, unbiased=True)\n",
        "        \n",
        "        music_dis_real, genre_dis_real = discriminator.forward(data_real)\n",
        "        music_prob_real = torch.sigmoid(music_dis_real)\n",
        "        std_prob_real   = torch.std_mean(music_prob_real, unbiased=True)\n",
        "        \n",
        "        # Converting some generated data to pianorolls\n",
        "        if show_real :\n",
        "            show_data = data_real.cpu().detach().numpy()\n",
        "            probs = music_prob_real.cpu().detach().numpy()\n",
        "        else :\n",
        "            show_data = data_generated.cpu().detach().numpy()\n",
        "            probs = music_prob_gen.cpu().detach().numpy()\n",
        "        if show_best :\n",
        "            best        = np.argpartition(-probs, num_images)\n",
        "            images      = show_data[best]\n",
        "            their_probs = probs[best]\n",
        "        else :\n",
        "            images      = show_data[:num_images]\n",
        "            their_probs = probs[:num_images]\n",
        "        images         = images.reshape(-1, data.width, data.height)\n",
        "        pianorolls     = np.pad(images, ((0, 0), (0, 0), \n",
        "                                        (data.lowest_pitch, \n",
        "                                        128 - data.lowest_pitch - data.height)))   \n",
        "                            # complete the pitch range\n",
        "\n",
        "    # Create audio save folder\n",
        "    default_path = f\"{default_training_path}/{dataset}\"\n",
        "    if save_to == None :\n",
        "        audio_folder = f\"{default_path}/temp_audio\"\n",
        "    else :\n",
        "        if checkpoint == 0 :\n",
        "            subfolder = \"\"\n",
        "        else :\n",
        "            subfolder = f\"/{checkpoint}\"\n",
        "        audio_folder = f\"{default_path}/{save_to}/audio{subfolder}\"\n",
        "    try:   # make new folder\n",
        "        os.makedirs(audio_folder)\n",
        "    except OSError:   # it already exists\n",
        "        pass\n",
        "\n",
        "\n",
        "    # Discriminator Results\n",
        "    print(f\"Discriminator p(x_real = real) = \" +\n",
        "        f\"{std_prob_real[1]*100:.0f}{std_prob_real[0]*100:.0f}%\")\n",
        "    print(f\"Discriminator p(x_gen = real)  = \" +\n",
        "        f\"{std_prob_gen[1]*100:.0f}{std_prob_gen[0]*100:.0f}%\")\n",
        "    print(\"\\n\\n\")\n",
        "    \n",
        "    # Generator examples\n",
        "    if show_real :\n",
        "        print(\"Example of the real music\")\n",
        "    else :\n",
        "        print(\"Example of the generated music\")\n",
        "    print(f\"saved under '{audio_folder}'\")\n",
        "    for i in range(num_images) :\n",
        "        beat_resolution = playback_speed * data.blips_per_beat // 3 \n",
        "        piano_music = muspy.from_pianoroll_representation(pianorolls[i] > 0,\n",
        "                        resolution = beat_resolution, \n",
        "                        encode_velocity = False)   # convert to muspy.music_object\n",
        "        \n",
        "        # save audio tracks\n",
        "        timestamp     = datetime.now()\n",
        "        audiopath     = f\"{audio_folder}/{timestamp}.wav\"\n",
        "        pianorollpath = f\"{audio_folder}/{timestamp}.npy\"\n",
        "        muspy.write_audio(path = audiopath, music = piano_music)\n",
        "        np.save(pianorollpath, pianorolls[i])\n",
        "\n",
        "        # Display example pianorolls with audio\n",
        "        kind = 'real'  if show_real else  'gen'\n",
        "        print(\"\")\n",
        "        print(f\"p(x_{kind} = real)  = {their_probs[i]*100:.2f}%\")\n",
        "        print(f\"file: {timestamp}.wav\")\n",
        "        \n",
        "        display(Audio(filename = audiopath))\n",
        "        muspy.visualization.show_pianoroll(piano_music)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "def numpify (tensor) :\n",
        "    return tensor.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def music_stats (dis_output) :\n",
        "    stats               = {}\n",
        "    stats['batch_size'] = dis_output.shape[0]\n",
        "    stats['dis_out']    = dis_output\n",
        "    stats['prob']       = torch.sigmoid(stats['dis_out'])\n",
        "    stats['std'], stats['mean'] \\\n",
        "                              = torch.std_mean(stats['prob'], unbiased=True)\n",
        "    stats['stat_err']   = stats['std'] \\\n",
        "                                / np.sqrt(stats['batch_size'])\n",
        "    stats['decision']   = torch.mean((stats['prob'] >= 0.5).float())\n",
        "    \n",
        "    for key, value in stats.items() :\n",
        "        if type(value) == torch.Tensor :\n",
        "            stats[key] = numpify(value)\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def print_music_performance (stats, mode, mode_long) :\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f\"Music discriminator performance on {mode_long} data\")\n",
        "    print(\"--------------------------------------------\")\n",
        "    print(\"\")\n",
        "    print(f\"{mode} size : {stats['dis_out'].shape[0]}\")\n",
        "    print(f\"mean[ p(x_{mode} = real) ]          : {stats['mean']*100:.2f}  {stats['stat_err']*100:.2f} %\")\n",
        "    print(f\"std[ p(x_{mode} = real) ]           : {stats['std']*100:.2f} %\")\n",
        "    print(f\"accuracy: p(x_{mode} = real) >= 50% : {stats['decision']*100:.2f} %\")\n",
        "    print(\"\")\n",
        "    \n",
        "\n",
        "def confusion_matrix (true_labels, predicted_labels, n_labels, \n",
        "                      unknown_label = False) :\n",
        "    true_labels      = numpify(true_labels)\n",
        "    predicted_labels = numpify(predicted_labels)\n",
        "    \n",
        "    n_plabels = n_labels + unknown_label\n",
        "    matrix    = np.zeros((n_labels, n_plabels))\n",
        "    for i, true_label in enumerate(range(n_labels)) :\n",
        "        this_label_mask = true_labels == true_label\n",
        "        n_true          = np.sum(this_label_mask)\n",
        "        predictions_for_this_label = predicted_labels[this_label_mask] # Selects predictions for one specific label\n",
        "        prediction_distribution    = [np.mean(predictions_for_this_label == pred_label)  \n",
        "                                      for pred_label in range(n_plabels)]\n",
        "        matrix[i]                  = np.array(prediction_distribution)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def genre_stats (dis_output, dis_labels) :\n",
        "    stats               = {}\n",
        "    stats['batch_size'] = dis_output.shape[0]\n",
        "    stats['dis_out']    = dis_output\n",
        "    stats['label']      = dis_labels\n",
        "    \n",
        "    # probabilities\n",
        "    stats['logprob']    = torch.log_softmax(stats['dis_out'], dim = 1)\n",
        "    stats['prob']       = torch.exp(stats['logprob'])\n",
        "    \n",
        "    # max probability labels\n",
        "    stats['m_prob'], stats['m_choice'] \\\n",
        "                        = torch.max(stats['prob'], dim = 1) \n",
        "    stats['m_std'], stats['m_mean'] \\\n",
        "                        = torch.std_mean(stats['m_prob'], unbiased=True) \n",
        "    stats['m_stat_err'] = stats['m_std'] \\\n",
        "                                / np.sqrt(stats['batch_size'])\n",
        "    \n",
        "    # confusion matrix\n",
        "    conf05              = stats['m_prob'] >= 0.5\n",
        "    right_choices       = stats['m_choice'] == stats['label']\n",
        "    #stats['right']      = np.logical_and(, )\n",
        "    stats['accuracy']   = torch.mean(torch.logical_and(right_choices, conf05).float())\n",
        "    #stats['decision']   = torch.mean((stats['m_prob'] > 0.5).float())\n",
        "    stats['choice']     = torch.where(conf05, \n",
        "                                    stats['m_choice'], # = most prob. label\n",
        "                                    lpd5.n_labels)  # = unknown label\n",
        "    stats['cf_matrix']  = confusion_matrix(stats['label'], \n",
        "                                            stats['choice'], \n",
        "                                            lpd5.n_labels,\n",
        "                                            unknown_label = True)\n",
        "    \n",
        "    # predictions per genre\n",
        "    labels, label_counts = torch.unique(stats['choice'], sorted = True, return_counts = True)\n",
        "    stats['classifications'] = numpify(label_counts / len(stats['choice']))\n",
        "    for i in range(lpd5.n_labels + 1) :\n",
        "        if not (i in labels) :\n",
        "            stats['classifications'] = np.insert(stats['classifications'], i, 0.0)\n",
        "    \n",
        "    # accuracy\n",
        "    stats['r_prob']     = stats['prob'][:, stats['label']]\n",
        "    stats['r_std'], stats['r_mean'] \\\n",
        "                        = torch.std_mean(stats['r_prob'], unbiased=True) \n",
        "    stats['r_stat_err'] = stats['r_std'] \\\n",
        "                                / np.sqrt(stats['batch_size'])\n",
        "    \n",
        "    # entropy\n",
        "    stats['entropy']    = torch.sum(- stats['prob'] \n",
        "                                          * stats['logprob'], dim = 1)\n",
        "    stats['e_std'], stats['e_mean'] \\\n",
        "                        = torch.std_mean(stats['entropy'], unbiased=True) \n",
        "    stats['e_stat_err'] = stats['e_std'] \\\n",
        "                                / np.sqrt(stats['batch_size'])\n",
        "    \n",
        "    # return\n",
        "    for key, value in stats.items() :\n",
        "        if type(value) == torch.Tensor :\n",
        "            stats[key] = numpify(value)\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def print_genre_performance (stats, mode, mode_long) :\n",
        "    print(f\"\"\" \n",
        "    \n",
        "    Genre discriminator performance on {mode_long} data\n",
        "    --------------------------------------------\n",
        "\n",
        "    {mode} size : {stats['dis_out'].shape[0]}\n",
        "\n",
        "    Discriminator accuracy:\n",
        "    mean[ predicted_label == real_label ] : {stats['r_mean']*100:.2f}  {stats['r_stat_err']*100:.2f} %\n",
        "    std[ predicted_label == real_label ]  : {stats['r_std']*100:.2f} %\n",
        "    accuracy: mean[ argmax[p(predicted_label)] == real_label ]   : {stats['accuracy']*100:.2f} %\n",
        "    \n",
        "    Discriminator confidence:\n",
        "    mean[ max[p(predicted_label)] ] : {stats['m_mean']*100:.2f}  {stats['m_stat_err']*100:.2f} %\n",
        "    std[ max[p(predicted_label)] ] : {stats['m_std']*100:.2f} %\n",
        "    \n",
        "    Discriminator entropy\n",
        "    mean[ entropy[p(predicted_label)] ] : {stats['e_mean']*100:.2f}  {stats['e_stat_err']*100:.2f} %\n",
        "    std[ entropy[p(predicted_label)] ] : {stats['e_std']*100:.2f} %\n",
        "    \"\"\")\n",
        "\n",
        "    ### --- code adapted from [1] --- ###\n",
        "    cf_matrix_table = pd.DataFrame(stats['cf_matrix'], \n",
        "                                   index   = [i for i in lpd5.genre_list],\n",
        "                                   columns = [i for i in lpd5.genre_list + [\"unknown\"]])\n",
        "    plt.figure(figsize = (12,7))\n",
        "    sn.heatmap(cf_matrix_table , annot=True)\n",
        "    ### --- --------------------- --- ###\n",
        "\n",
        "\n",
        "\n",
        "def CAN_test (generator, discriminator, gen_name = \"\", dis_name = \"\", notes = True) :\n",
        "    device    = get_device()\n",
        "    test_size = len(lpd5.labels_test)\n",
        "    batch_size = 100\n",
        "\n",
        "    # Calculating Discriminator predictions\n",
        "    with torch.inference_mode() :    \n",
        "        # Loading models as well as training and test data\n",
        "        generator.eval().to(device)\n",
        "        discriminator.eval().to(device)\n",
        "        \n",
        "        # Calculate Test performance\n",
        "        dataloader = iter(torch.utils.data.DataLoader(lpd5.dataset_test,\n",
        "                        batch_size = batch_size, \n",
        "                        shuffle = True, drop_last = False))\n",
        "        music_dis = torch.zeros(test_size).to(device)\n",
        "        genre_dis = torch.zeros((test_size, lpd5.n_labels)).to(device)\n",
        "        labels_dis = torch.zeros(test_size, dtype = torch.int64).to(device)\n",
        "\n",
        "        n_batches = math.ceil(test_size / batch_size)\n",
        "        for i in range(n_batches) :\n",
        "            print(f\"\\rRunning test batch {i+1} / {n_batches}.\", end = \"\")\n",
        "            data, labels = dataloader.next()\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            music_batch_dis, genre_batch_dis = discriminator.forward(data)\n",
        "            batch_slice = np.s_[i*batch_size:min((i+1)*batch_size, test_size)]\n",
        "            music_dis[batch_slice] = music_batch_dis\n",
        "            genre_dis[batch_slice] = genre_batch_dis\n",
        "            labels_dis[batch_slice] = labels\n",
        "\n",
        "        print('\\r')\n",
        "        music_test = music_stats(music_dis)\n",
        "        #print_music_performance(music_test, \"test\", \"test\")\n",
        "        genre_test = genre_stats(genre_dis, labels_dis)\n",
        "        #print_genre_performance(genre_test, \"test\", \"test\")\n",
        "        \n",
        "\n",
        "        # Calculate Training performance\n",
        "        indices = np.random.permutation(np.arange(lpd5.dataset_size))[:test_size]\n",
        "        datasubset = torch.utils.data.Subset(lpd5.dataset, indices)\n",
        "        dataloader = iter(torch.utils.data.DataLoader(datasubset,\n",
        "                          batch_size = batch_size, \n",
        "                          shuffle = True, drop_last = False))\n",
        "        music_dis = torch.zeros(test_size).to(device)\n",
        "        genre_dis = torch.zeros((test_size, lpd5.n_labels)).to(device)\n",
        "        labels_dis = torch.zeros(test_size, dtype = torch.int64).to(device)\n",
        "\n",
        "        n_batches = math.ceil(test_size / batch_size)\n",
        "        for i in range(n_batches) :\n",
        "            print(f\"\\rRunning training batch {i+1} / {n_batches}.\", end = \"\")\n",
        "            data, labels = dataloader.next()\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            music_batch_dis, genre_batch_dis = discriminator.forward(data)\n",
        "            batch_slice = np.s_[i*batch_size:min((i+1)*batch_size, test_size)]\n",
        "            music_dis[batch_slice] = music_batch_dis\n",
        "            genre_dis[batch_slice] = genre_batch_dis\n",
        "            labels_dis[batch_slice] = labels\n",
        "\n",
        "        print('\\r')\n",
        "        music_train = music_stats(music_dis)\n",
        "        #print_music_performance(music_train, \"train\", \"training\")\n",
        "        genre_train = genre_stats(genre_dis, labels_dis)\n",
        "        #print_genre_performance(genre_train, \"train\", \"training\")\n",
        "\n",
        "\n",
        "        # Calculate Generated performance\n",
        "        music_dis = torch.zeros(test_size).to(device)\n",
        "        genre_dis = torch.zeros((test_size, lpd5.n_labels)).to(device)\n",
        "        \n",
        "        for i in range(math.ceil(test_size / batch_size)) :\n",
        "            print(f\"\\rRunning generator batch {i+1} / {n_batches}.\", end = \"\")\n",
        "            gen_batch_size = min(batch_size, test_size - i*batch_size)\n",
        "            data = generator.forward(gen_batch_size)\n",
        "            \n",
        "            music_batch_dis, genre_batch_dis = discriminator.forward(data)\n",
        "            batch_slice = np.s_[i*batch_size:min((i+1)*batch_size, test_size)]\n",
        "            music_dis[batch_slice] = music_batch_dis\n",
        "            genre_dis[batch_slice] = genre_batch_dis\n",
        "    \n",
        "        print('\\r')\n",
        "        music_gen = music_stats(music_dis)\n",
        "        #print_music_performance(music_gen, \"gen\", \"generated\") \n",
        "        genre_gen = genre_stats(genre_dis, labels_dis)\n",
        "        #print_genre_performance(genre_gen, \"gen\", \"generated\") \n",
        "\n",
        "\n",
        "    # Create music comparison table\n",
        "    music_dis_table = {\n",
        "        'Data' : [\n",
        "            \"accuracy [%]\",\n",
        "            \"mean probability [%]\",\n",
        "            \"  its stat. error\",\n",
        "            \"  its std. deviation\",\n",
        "        ],\n",
        "        'test' : (np.array([\n",
        "            1*music_test['decision'],\n",
        "            music_test['mean'],\n",
        "            music_test['stat_err'],\n",
        "            music_test['std'],\n",
        "        ]) * 100).round(2),\n",
        "        'training' : (np.array([\n",
        "            1*music_train['decision'],\n",
        "            music_train['mean'],\n",
        "            music_train['stat_err'],\n",
        "            music_train['std'],\n",
        "        ]) * 100).round(2),\n",
        "        'generated' : (np.array([\n",
        "            1 - music_gen['decision'],\n",
        "            music_gen['mean'],\n",
        "            music_gen['stat_err'],\n",
        "            music_gen['std'],\n",
        "        ]) * 100).round(2),\n",
        "    }\n",
        "\n",
        "    music_panda_table = pd.DataFrame(music_dis_table)\n",
        "\n",
        "    print(\"Music Discriminator performance\")\n",
        "    display(music_panda_table)\n",
        "    if notes :\n",
        "        print(\"This table is a comparison of the Discriminator's ability to correctly\")\n",
        "        print(\"classify pianorolls as either real or generated music.\")\n",
        "        print(\"batch size        : 3517 pianorolls of each data type were classified\")\n",
        "        if (gen_name != \"\" and dis_name != \"\") :\n",
        "            print(\"Generator     :\", gen_name)\n",
        "            print(\"Discriminator :\", gen_name)\n",
        "        print(\"\")\n",
        "        print(\"Notes:\")\n",
        "        print(\"accuracy          : fraction of right classifications \")\n",
        "        print(\"                    (i.e. where p(x = real) >= 50%  or  \")\n",
        "        print(\"                     p(x = real) >= 50% for gen. data)\")\n",
        "        print(\"mean probability  : mean[ p(x = real) ] over whole batch\")\n",
        "        print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "    # Create genre comparison table\n",
        "    genre_dis_table = {\n",
        "        'Data' : [\n",
        "            \"accuracy [%]\",\n",
        "            \"mean max. probability [%]\",\n",
        "            \"  its stat. error\",\n",
        "            \"  its std. deviation\",\n",
        "            \"mean confidence [%]\",\n",
        "            \"  its stat. error\",\n",
        "            \"  its std. deviation\",\n",
        "            \"mean entropy\",\n",
        "            \"  its stat. error\",\n",
        "            \"  its std. deviation\",  \n",
        "        ],\n",
        "        'test' : (np.array([\n",
        "            genre_test['accuracy'],\n",
        "            genre_test['r_mean'],\n",
        "            genre_test['r_stat_err'],\n",
        "            genre_test['r_std'],\n",
        "            genre_test['m_mean'],\n",
        "            genre_test['m_stat_err'],\n",
        "            genre_test['m_std'],\n",
        "            genre_test['e_mean'] / 100,\n",
        "            genre_test['e_stat_err'] / 100,\n",
        "            genre_test['e_std'] / 100,\n",
        "        ]) * 100).round(2),\n",
        "        'training' : (np.array([\n",
        "            genre_train['accuracy'],\n",
        "            genre_train['r_mean'],\n",
        "            genre_train['r_stat_err'],\n",
        "            genre_train['r_std'],\n",
        "            genre_train['m_mean'],\n",
        "            genre_train['m_stat_err'],\n",
        "            genre_train['m_std'],\n",
        "            genre_train['e_mean'] / 100,\n",
        "            genre_train['e_stat_err'] / 100,\n",
        "            genre_train['e_std'] / 100,\n",
        "        ]) * 100).round(2),\n",
        "        'generated' : (np.array([\n",
        "            np.nan,\n",
        "            np.nan,\n",
        "            np.nan,\n",
        "            np.nan,\n",
        "            genre_gen['m_mean'],\n",
        "            genre_gen['m_stat_err'],\n",
        "            genre_gen['m_std'],\n",
        "            genre_gen['e_mean'] / 100,\n",
        "            genre_gen['e_stat_err'] / 100,\n",
        "            genre_gen['e_std'] / 100,\n",
        "        ]) * 100).round(2),\n",
        "    }\n",
        "\n",
        "    genre_panda_table = pd.DataFrame(genre_dis_table)\n",
        "\n",
        "    print(\"Genre Discriminator performance\")\n",
        "    display(genre_panda_table)\n",
        "    if notes :\n",
        "        print(\"This table is a comparison of the Discriminator's ability to correctly \")\n",
        "        print(\"classify pianorolls as either real or generated music.\")\n",
        "        print(\"batch size        : 3517 pianorolls of each data type were classified\")\n",
        "        if (gen_name != \"\" and dis_name != \"\") :\n",
        "            print(\"Generator     :\", gen_name)\n",
        "            print(\"Discriminator :\", gen_name)\n",
        "        print(\"\")\n",
        "        print(\"Notes:\")\n",
        "        print(\"accuracy          : fraction of right classifications \")\n",
        "        print(\"                      (i.e. where max[p(x_label = true_label)] >= 50%)\")\n",
        "        print(\"mean probability  : mean[ p(x_label = true_label) ] over whole batch\")\n",
        "        print(\"mean confidence   : mean[ max_prob(x) ] over whole batch,  \")\n",
        "        print(\"                      with 'max_prob(x) = max[p(x_label)] over labels\")\n",
        "        print(\"mean entropy      : mean[ entropy(x) ] over whole batch,\")\n",
        "        print(\"                      with 'entropy(x)' = - sum[ p(x_label) * log(p(x_label)) ] \")\n",
        "        print(\"                                            over labels\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "    dataset_genres = lpd5.genre_list\n",
        "    predicted_genres = lpd5.genre_list + [\"unknown\"]\n",
        "    # Show confusion matrices\n",
        "    ### --- code adapted from [1] --- ###\n",
        "    '''\n",
        "        test_cf_matrix = pd.DataFrame(genre_test['cf_matrix'], \n",
        "                                    index   = [i for i in lpd5.genre_list],\n",
        "                                    columns = [i for i in lpd5.genre_list + [\"unknown\"]])\n",
        "        plt.figure(figsize = (12,7))\n",
        "        plt.title(\"Genre predictions on test data\")\n",
        "        sn.heatmap(test_cf_matrix, annot=True)\n",
        "        plt.ylabel(\"true genres\")\n",
        "        plt.xlabel(\"predicted genres\")\n",
        "        plt.show()\n",
        "    '''\n",
        "\n",
        "    train_cf_matrix = pd.DataFrame(genre_train['cf_matrix'], \n",
        "                                   index   = dataset_genres,\n",
        "                                   columns = predicted_genres)\n",
        "    plt.figure(figsize = (12,7))\n",
        "    plt.title(\"Confusion matrix of training data\")\n",
        "    sn.heatmap(train_cf_matrix, annot=True)\n",
        "    plt.ylabel(\"true genres\")\n",
        "    plt.xlabel(\"predicted genres\")\n",
        "    plt.show()\n",
        "\n",
        "    ### --- --------------------- --- ###\n",
        "\n",
        "    label_freqs = np.array([np.mean(lpd5.labels.numpy() == i) \n",
        "                            for i in range(lpd5.n_labels + 1)])\n",
        "    plt.figure(figsize = (8, 4))\n",
        "    plt.title(\"True genre distribution of training data\")\n",
        "    plt.bar(predicted_genres, label_freqs*100)\n",
        "            #tick_label = predicted_genres)\n",
        "    plt.xlabel(\"genres\")\n",
        "    plt.ylabel(\"fraction of samples [%]\")\n",
        "    plt.ylim(0, 60)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.figure(figsize = (8, 4))\n",
        "    plt.title(\"Genre classifications of training data\")\n",
        "    plt.bar(predicted_genres, genre_train['classifications']*100) \n",
        "            #tick_label = predicted_genres)\n",
        "    plt.xlabel(\"predicted genres\")\n",
        "    plt.ylabel(\"fraction of samples [%]\")\n",
        "    plt.ylim(0, 60)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize = (8, 4))\n",
        "    plt.title(\"Genre classifications of generated data\")\n",
        "    plt.bar(predicted_genres, genre_gen['classifications']*100)\n",
        "            #tick_label = predicted_genres)\n",
        "    plt.xlabel(\"predicted genres\")\n",
        "    plt.ylabel(\"fraction of samples [%]\")\n",
        "    plt.ylim(0, 60)\n",
        "    plt.show()\n",
        "\n",
        "    predictions_table = {'genre':predicted_genres,\n",
        "                         'training dataset': (label_freqs*100).round(1),\n",
        "                         'prediction training': (np.array(genre_train['classifications'], \n",
        "                                                          dtype = float)*100).round(1),\n",
        "                         'prediction generated': (np.array(genre_gen['classifications'], \n",
        "                                                           dtype = float)*100).round(1),}\n",
        "    print(\"Distribution of genre predictions [%]:\")\n",
        "    display(pd.DataFrame(predictions_table))\n",
        "\n",
        "\n",
        "# Sources\n",
        "# [1] https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7\n"
      ],
      "metadata": {
        "id": "V_t88a045PDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4rFGcgSMO7j"
      },
      "source": [
        "#### Save and load trained models and logs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device () :\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "    return torch.device(device)"
      ],
      "metadata": {
        "id": "hksd094i0lBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_train_state (trainer, round) :\n",
        "\n",
        "    # dictionary to be saved\n",
        "    train_state = {\n",
        "        'info'  : trainer.info,\n",
        "        'gen'   : trainer.gen.state_dict(),\n",
        "        'dis'   : trainer.dis.state_dict(),\n",
        "        'opt_g' : trainer.optimizer_gen.state_dict(), \n",
        "        'opt_d' : trainer.optimizer_dis.state_dict(), \n",
        "        'log'   : trainer.log.__dict__,\n",
        "    }\n",
        "    \n",
        "    # creating the folder path\n",
        "    if trainer.training_folder == \"\" :\n",
        "        now       = datetime.now()\n",
        "        timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        training_folder = f\"{timestamp}_{trainer.training_name}\"\n",
        "    else :\n",
        "        training_folder = trainer.training_folder\n",
        "    folder_path = f\"{default_training_path}/{default_dataset}/\" \\\n",
        "                + f\"{training_folder}\"\n",
        "    file_path   = f\"{folder_path}/train_state{round}.pth\"\n",
        "\n",
        "    # saving the train_state\n",
        "    try :  os.makedirs(folder_path) # make new folder\n",
        "    except OSError :  pass          # it already exists        \n",
        "    torch.save(train_state, file_path)\n",
        "\n",
        "    return training_folder"
      ],
      "metadata": {
        "id": "st0Lr1LDurXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_train_state (training_folder, round) :\n",
        "    \n",
        "    load_path = f\"{default_training_path}/{default_dataset}/\" \\\n",
        "              + f\"{training_folder}/train_state{round}.pth\"\n",
        "    if os.path.isfile(load_path) :\n",
        "        train_state = torch.load(load_path, map_location = get_device())\n",
        "    else :\n",
        "        train_state = None\n",
        "        print(f\"\\nNo train state found at\\n'{load_path}'!\\n\")\n",
        "\n",
        "    return train_state"
      ],
      "metadata": {
        "id": "GKyx1K5_5GkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_CAN (training_folder, round, print_info = False,\n",
        "              gen_class = MusiGenMod2, dis_class = MusiDisMod2b) :\n",
        "\n",
        "    train_state = load_train_state(training_folder, round)\n",
        "    device      = get_device()\n",
        "\n",
        "    gen, dis = gen_class(), dis_class()\n",
        "    gen.load_state_dict(train_state['gen'])\n",
        "    dis.load_state_dict(train_state['dis'])\n",
        "    gen.to(device), dis.to(device)\n",
        "    gen.eval()    , dis.eval()\n",
        "\n",
        "    log_dict = train_state[\"log\"]\n",
        "    for key, value in log_dict.items() :\n",
        "        if type(value) == np.ndarray :\n",
        "            log_dict[key] = value[:, :round]\n",
        "    log = LogLoaded(log_dict)\n",
        "    \n",
        "\n",
        "    if print_info :\n",
        "        print(train_state[\"info\"])\n",
        "\n",
        "    return gen, dis, log"
      ],
      "metadata": {
        "id": "leKK9pCayN5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m92AAeQbMMl1"
      },
      "outputs": [],
      "source": [
        "def save_training (training_name, trainer, info_txt = None, \n",
        "                   dataset = default_dataset, new_folder = True, \n",
        "                   checkpoint = 0) :\n",
        "    assert  type(training_name) == str               \n",
        "    assert  info_txt == None  or  type(info_txt) == str\n",
        "    \n",
        "    # If trainer has already saved a checkpoint, no new folder is needed.\n",
        "    if hasattr(trainer, \"training_name\") :\n",
        "        if (training_name == trainer.training_name  and\n",
        "            trainer.training_folder != \"\") :\n",
        "            \n",
        "            training_name = trainer.training_folder\n",
        "            new_folder    = False\n",
        "    \n",
        "    # Name the save folder\n",
        "    if new_folder:\n",
        "        now             = datetime.now()\n",
        "        date            = f\"{now.year}-{now.month:02d}-{now.day:02d}\"\n",
        "        time            = f\"{now.hour:02d}-{now.minute:02d}\"\n",
        "        timestamp       = f\"{date}_{time}\"\n",
        "        training_folder = f\"{timestamp}_{training_name}\"\n",
        "    else :\n",
        "        training_folder = training_name\n",
        "    save_folder = f\"{default_training_path}/{dataset}/{training_folder}\"\n",
        "    \n",
        "    model_folder = f\"{save_folder}/model\"\n",
        "    try:   # make new folder\n",
        "        os.makedirs(model_folder)\n",
        "    except OSError:   # it already exists\n",
        "        pass\n",
        "    \n",
        "\n",
        "    # save models\n",
        "    gen = trainer.gen\n",
        "    dis = trainer.dis\n",
        "    \n",
        "    if checkpoint == 0 :\n",
        "        torch.save(gen.state_dict(), f\"{model_folder}/gen.pt\")\n",
        "        torch.save(dis.state_dict(), f\"{model_folder}/dis.pt\")\n",
        "    else : \n",
        "        # here, checkpoint is an int: the current training round number\n",
        "        torch.save(gen.state_dict(), f\"{model_folder}/gen{checkpoint}.pt\")\n",
        "        torch.save(dis.state_dict(), f\"{model_folder}/dis{checkpoint}.pt\")\n",
        "\n",
        "    # save logs\n",
        "    if checkpoint == 0 :\n",
        "        log_file = f\"{save_folder}/logs.npz\"\n",
        "        log_dict = trainer.log.__dict__\n",
        "    else :\n",
        "        # here, checkpoint is an int: the current training round number\n",
        "        total_rounds = checkpoint\n",
        "        log_file = f\"{save_folder}/logs{checkpoint}.npz\"\n",
        "        log_dict = trainer.log.__dict__.copy()\n",
        "        # shorten the log arrays to current checkpoint \n",
        "        for key, value in log_dict.items() :\n",
        "            if type(value) == np.ndarray :\n",
        "                log_dict[key] = value[:, :total_rounds]\n",
        "    \n",
        "    np.savez(log_file, **log_dict)     \n",
        "    \n",
        "\n",
        "    # save additional info about training\n",
        "    info_path  = f\"{save_folder}/info.txt\"\n",
        "    and_info   = \"\"\n",
        "    if info_txt != None :\n",
        "        with open(info_path, \"w+\") as f :\n",
        "            f.writelines(info_txt)\n",
        "        and_info = \"and info text \"  \n",
        "    \n",
        "    if checkpoint == 0:\n",
        "        print(f\"Saved models {and_info}under:\\n\",\n",
        "            f\"'{default_training_path}/{dataset}/\\n\",\n",
        "            f\" {training_folder}'\")\n",
        "        \n",
        "    return training_folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK6Iwny8CvsL"
      },
      "outputs": [],
      "source": [
        "def load_training (training_folder, model = (MusiGen, MusiDis), \n",
        "                   print_info = False, dataset = default_dataset, \n",
        "                   checkpoint = 0) :\n",
        "    save_folder = f\"{default_training_path}/{dataset}/{training_folder}\"\n",
        "    assert  os.path.exists(save_folder)\n",
        "\n",
        "    # load models\n",
        "    model_folder       = f\"{save_folder}/model\"\n",
        "    GenClass, DisClass = model\n",
        "    gen, dis           = GenClass(), DisClass()\n",
        "    cp = \"\"  if checkpoint == 0 else  f\"{checkpoint}\"   # here, checkpoint is an int: the current training round number\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    gen.load_state_dict(torch.load(f\"{model_folder}/gen{cp}.pt\", \n",
        "                                   map_location = device))\n",
        "    dis.load_state_dict(torch.load(f\"{model_folder}/dis{cp}.pt\",\n",
        "                                   map_location = device))\n",
        "    \n",
        "\n",
        "    # Prepare models for evaluation\n",
        "    device = 'cuda'  if torch.cuda.is_available() else  'cpu'\n",
        "    gen    = gen.to(device)\n",
        "    dis    = dis.to(device)\n",
        "    gen.eval()\n",
        "    dis.eval()\n",
        "\n",
        "    # load logs\n",
        "    if checkpoint == 0 :\n",
        "        log_file = f\"{save_folder}/logs.npz\"\n",
        "    else :\n",
        "        log_file = f\"{save_folder}/logs{checkpoint}.npz\"\n",
        "\n",
        "    logs = None\n",
        "    with np.load(log_file) as log_dict :\n",
        "        logs = LogLoaded(log_dict)\n",
        "    \n",
        "    # load info\n",
        "    info_path  = f\"{save_folder}/info.txt\"\n",
        "    if print_info :\n",
        "        with open(info_path, \"r\") as f :\n",
        "            print(f.read())\n",
        "    \n",
        "    return gen, dis, logs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_track (folder, number, with_image = True, checkpoint = 0, dc1 = True, save_to = \"\", best = False) :\n",
        "    if dc1 :\n",
        "        default_dataset       = \"datacombi_1\"\n",
        "    else :\n",
        "        default_dataset       = \"lpd5_full_4bars\"\n",
        "\n",
        "    default_path = f\"{default_training_path}/{default_dataset}\"\n",
        "    if checkpoint == 0 :\n",
        "        subfolder = \"\"\n",
        "    else :\n",
        "        subfolder = f\"/{checkpoint}\"\n",
        "    audio_folder = f\"{default_path}/{folder}/audio{subfolder}\"\n",
        "\n",
        "    file_names = os.listdir(audio_folder)\n",
        "    file_names.sort()\n",
        "    audio_names = [name[:-4]  for name in file_names  if \".wav\" in name]\n",
        "    \n",
        "    name = audio_names[number]\n",
        "    \n",
        "    audiopath     = f\"{audio_folder}/{name}.wav\"\n",
        "    pianorollpath = f\"{audio_folder}/{name}.npy\"\n",
        "\n",
        "    print(f\"file: {name}.wav\")\n",
        "    display(Audio(filename = audiopath))\n",
        "\n",
        "    if save_to != \"\" :\n",
        "        save_folder = f\"../audio/{save_to}\"\n",
        "        try :  os.makedirs(save_folder) # make new folder\n",
        "        except OSError :  pass          # it already exists     \n",
        "        add = \"best\"  if best else  \"\"\n",
        "        aud_idx = 0\n",
        "        aud_path = lambda idx : f\"../audio/{save_to}/{add}{idx}.wav\"\n",
        "        while os.path.exists(aud_path(aud_idx)):\n",
        "            aud_idx += 1\n",
        "        shutil.copyfile(audiopath, aud_path(aud_idx))\n",
        "        print(f\"Saved to {aud_path(aud_idx)}\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    if os.path.exists(pianorollpath) :\n",
        "        pianoroll   = np.load(pianorollpath)\n",
        "        beat_resolution = 12  if dc1 else  4 \n",
        "        piano_music = muspy.from_pianoroll_representation(pianoroll > 0.5,\n",
        "                            resolution = beat_resolution, \n",
        "                            encode_velocity = False)   # convert to muspy.music_object\n",
        "        muspy.visualization.show_pianoroll(piano_music)\n",
        "        if save_to != \"\" :\n",
        "            add = \"best\"  if best else  \"\"\n",
        "            img_idx = 0\n",
        "            img_path = lambda idx : f\"../audio/{save_to}/{add}{idx}.png\"\n",
        "            while os.path.exists(img_path(img_idx)):\n",
        "                img_idx += 1\n",
        "            plt.savefig(img_path(img_idx))\n",
        "            print(f\"Saved to {img_path(img_idx)}\")\n",
        "        plt.show() "
      ],
      "metadata": {
        "id": "ez03hvhk2Xhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7J679rwSfnC"
      },
      "source": [
        "## Network training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6uuHHR6fQOe"
      },
      "source": [
        "### Main Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eBKbfRJbh6D"
      },
      "outputs": [],
      "source": [
        "training_name = \"16k-wcan-gp-higher-lr\"\n",
        "summary       = \\\n",
        "f\"\"\"\n",
        "    Training info: {training_name}\n",
        "    =======================\n",
        "\n",
        "    models: MusiGenMod2, MusiDisMod2b\n",
        "    dataset: datacombi_1\n",
        "\n",
        "    rounds = 16000\n",
        "    batch_size = 25\n",
        "    discriminator_rounds = 5\n",
        "    loss_function = WCAN-GP\n",
        "    checkpoints   = [2000, 4000, 6000, 8000, 10_000, 12_000, 14_000]\n",
        "\n",
        "    adam_optimizer_params:\n",
        "        dis: (lr = 0.001, betas = (0.5, 0.9))\n",
        "        gen: (lr = 0.001, betas = (0.5, 0.9))\n",
        "    genre_ambiguity:\n",
        "        dis: 1\n",
        "        gen: 1\n",
        "    optional normalizations:\n",
        "        music output: ON\n",
        "        genre output: ON\n",
        "\n",
        "\n",
        "    additional comments:\n",
        "        \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gasGW3KpSfnM"
      },
      "outputs": [],
      "source": [
        "lpd5Train     = GANTraining(MusiGenMod2, MusiDisMod2b, lpd5.dataset)\n",
        "lpd5Train.setup(16000, batch_size = 25, discriminator_rounds = 5, \n",
        "                loss_function = \"WCAN-GP\", info_text = summary, \n",
        "                norm_dis_probs = True, norm_dis_genre = True, \n",
        "                genre_ambiguity = (1, 1), learning_rates = (1e-3, 1e-3))\n",
        "#lpd5Train.resume(\"2022-09-18_20-43-31_16k-long-loss-test\", 12000)\n",
        "lpd5Train.set_backups(training_name, checkpoints = list(range(0, 16_000, 1000)),\n",
        "                      save_folder = \"\")\n",
        "lpd5Train.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47lzP9FrGRaI"
      },
      "outputs": [],
      "source": [
        "training_folder_name = save_training(training_name, lpd5Train, info_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXwYSkWt3oRg"
      },
      "outputs": [],
      "source": [
        "plot_training(lpd5Train.log, CAN = True, show_loss_terms = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd-LOltQSfnN"
      },
      "outputs": [],
      "source": [
        "long_test(lpd5Train.gen, lpd5Train.dis, lpd5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlOBH55thlJQ"
      },
      "outputs": [],
      "source": [
        "quick_test(lpd5Train.gen, lpd5Train.dis, lpd5, num_images = 5, \n",
        "           save_to = training_folder_name, playback_speed = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb-NJGprfWGK"
      },
      "source": [
        "### Loading and investigating saved trained models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_folder = \"2022-09-17_16-50-04_4k-does-norm-work\"\n",
        "gen, dis, log = load_CAN(training_folder, 1000)"
      ],
      "metadata": {
        "id": "PJGlxDXep2P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(log, CAN = True, show_loss_terms = True)"
      ],
      "metadata": {
        "id": "jS9mUvPA3fD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading saved tracks"
      ],
      "metadata": {
        "id": "k3mPviru2bqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5) :\n",
        "    load_track(\"2022-09-15_06-01_8k-mod2-datacombi-1\", (i+5))"
      ],
      "metadata": {
        "id": "qLcap0F-2f4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5) :\n",
        "    load_track(\"2022-09-14_19-39_8k-test-mod2\", -(i+5), dc1 = False)"
      ],
      "metadata": {
        "id": "JmK22G-B2oHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"2022-09-12_02-14_10k-vanilla-musegan\"\n",
        "\"2022-09-14_19-39_8k-test-mod2\"\n",
        "\"2022-09-15_00-35_10k-v-musegan-datacombi-1\"\n",
        "\"2022-09-15_06-01_8k-mod2-datacombi-1\"\n",
        "\"2022-09-18_20-43-31_16k-long-loss-test\"\n",
        "\"2022-09-20_11-25-44_16k-wcan-gp-higher-lr\""
      ],
      "metadata": {
        "id": "q5G5NiVSTh0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5) :\n",
        "    load_track(\"2022-09-20_11-25-44_16k-wcan-gp-higher-lr\", -(i+1), save_to = \"can\")"
      ],
      "metadata": {
        "id": "V8fqfx0fUE6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8ATBcagDnDpM",
        "trwcbyaoSfm2",
        "E5pWodkypE8U",
        "KdGO-KlvSfm5",
        "kU1bKYfdSfm6",
        "swF_vr8kSfm9",
        "vG8eX-HGSfm-",
        "L_lrgdWMSrIk",
        "xS0kN-LMSzV0",
        "6ejVE_JNS2VE",
        "WstRQBd2SfnA",
        "9UjwRjHiook6",
        "ixzSjpbOS64f",
        "LyJRhxx8L_4y",
        "k4rFGcgSMO7j",
        "i7J679rwSfnC",
        "Q6uuHHR6fQOe"
      ],
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('aml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e9db57278ae8397bc5fe3bec6b9ba53c33a2aa76e79d386f678fc754e34f9547"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}